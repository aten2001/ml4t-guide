[
 {
   "index": 0,
   "question": "Which of the following metric is most suitable in determining whether prediction quality linearly matches up with actual data?",
   "A": "Pearson Correlation",
   "B": "RMSE",
   "C": "Spearman Correlation",
   "D": "MSE",
   "answer": "A",
   "why": "Pearson correlation determines linear relationship among two variables."
 },
 {
   "index": 1,
   "question": "What is the fundamental feature of supervised learning that  distinguishes it from unsupervised learning? For all following choices (A through D), X is an mxn matrix where each column corresponds to one feature/factor and each row represent one data instance. Y is vector of size m in which all prediction values are stored. For supervised learning:",
   "A": "Both X and Y are provided when building the predictive model using the ML algorithms. Each row of X and each value of Y are given as data pair. In this data pair, the Y value is associated with the row in X.  ",
   "B": "Both X and Y are provided when building the predictive model using the ML algorithms. Each row of X and each value of Y are given as data pair. In this data pair, Y value is not necessarily associated with the row in X.  ",
   "C": "Only one of X and Y is provided when building the predictive model using ML algorithms.  ",
   "D": "Neither X or Y is necessary when building the predictive model using ML algorithms.",
   "answer": "A",
   "why": "For supervised learning, the feature/factor data X and its corresponding prediction value Y are both provided. Each Y value must be associated with each row in X."
 },
 {
   "index": 2,
   "question": "Consider a regression decision tree modeled with 1000 data points.  Which combination of leaf size and bootstrap aggregating will result in the LEAST amount of overfitting?",
   "A": "leaf size = 1, bootstrap aggregating = false",
   "B": "leaf size = 50, bootstrap aggregating = false",
   "C": "leaf size = 1, bootstrap aggregating = true",
   "D": "leaf size = 50, bootstrap aggregating = true",
   "answer": "D",
   "why": "Larger leaf sizes rather than small ones prevent over fitting.  Bagging reduces overfitting.  "
 },
 {
   "index": 3,
   "question": "Your company is investigating two different models for use in their company, kNN and linear regression. Which of the below is a false claim made by the investigator?",
   "A": "Linear regression is better for lots of queries because it’s faster than kNN.",
   "B": "kNN is better when you frequently want to add more training data.",
   "C": "Linear regression is better at predicting the future.",
   "D": "Linear regression is faster at training since it’s just a linear equation.",
   "answer": "D",
   "why": "Linear regression is not faster at training than kNN."
 },
 {
   "index": 4,
   "question": "What is the purpose of using entropy?",
   "A": "To assess errors.",
   "B": "To determine relevant features.",
   "C": "To reduce variance.",
   "D": "To reduce bias.",
   "answer": "B",
   "why": "Information gain is the amount of information acquired that increases certainty in prediction. The more information we get the more certain we are going to be about the outcome. Predictors or features serve as sources of such information and each usually provides different levels of this information. We can measure the usefulness of predictors by how much information they provide. For this we can calculate entropy or value of uncertainty - the higher is entropy the more uncertain we are in prediction. Conversely, the lowest entropy indicates the highest information gain. Entropy can change when different features are included or excluded from the model. The difference between the former and current entropies is the information gain that helps to determine the most relevant features."
 },
 {
   "index": 5,
   "question": "What is the purpose of using entropy?",
   "A": "To assess errors.",
   "B": "To determine relevant features.",
   "C": "To reduce variance.",
   "D": "To reduce bias.",
   "answer": "B",
   "why": "Information gain is the amount of information acquired that increases certainty in prediction. The more information we get the more certain we are going to be about the outcome. Predictors or features serve as sources of such information and each usually provides different levels of this information. We can measure the usefulness of predictors by how much information they provide. For this we can calculate entropy or value of uncertainty - the higher is entropy the more uncertain we are in prediction. Entropy can change when different features are included or excluded from the model. The difference between the former and current entropies is the information gain that helps to determine the most relevant features."
 },
 {
   "index": 6,
   "question": "What is the purpose of using entropy?",
   "A": "To assess errors.",
   "B": "To determine relevant features.",
   "C": "To reduce variance.",
   "D": "To reduce bias.",
   "answer": "B",
   "why": "Information gain is the amount of information acquired that increases certainty in prediction. The more information we get the more certain we are going to be about the outcome. Predictors or features serve as sources of such information and each usually provides different levels of this information. We can measure the usefulness of predictors by how much information they provide. For this we can calculate entropy or value of uncertainty - the higher is entropy the more uncertain we are in prediction. Entropy can change when different features are included or excluded from the model. The difference between the former and current entries is the information gain that helps to determine the most relevant features."
 },
 {
   "index": 7,
   "question": "Which statement below is true about Random Forests and overfitting? (Random Forests = bagging Random Trees.)",
   "A": "Random Forests cannot reduce overfitting associated with the base learners, in this case Random Trees.",
   "B": "Increasing the number of bags in a Random Forests improves predictive performance, but also increases overfitting.",
   "C": "Random Forests do not overfit data.",
   "D": "Answers A and B are both true.",
   "answer": "C",
   "why": "Correct answer is (C). Overfitting refers to high variance between training and test data. Bagging reduces variance by averaging across base learners. While  Random Tree base learners might overfit, e.g., leaf size=1 (one unique sample per branch), bagging across trees with randomly selected features will average out the variance. So (A) is false. (B) is false since the performance gains from increasing the number of bags imply lower variance, which cannot cause increased overfitting. Averaging smoothes; it cannot inject external variance and so only (C) is true."
 },
 {
   "index": 8,
   "question": "Which option among the following best describes features of an instance based learning model such as KNN?",
   "A": "Faster learning, slower recall, less memory required compared to parameterized models.",
   "B": "Slower learning, faster recall, more memory required compared to parameterized models.",
   "C": "Faster learning, slower recall, more memory required compared to parameterized models.",
   "D": "Slower learning, slower query compared, less memory required compared to parameterized models.",
   "answer": "C",
   "why": "Instance based models need to mostly store the training data which they refer during the query process. This makes learning faster(as it is just a storage operation), slower query (as the model will process the training data to ans the query), and more memory required to store the training data sets. "
 },
 {
   "index": 9,
   "question": "Which option among the following best describes features of an instance based learning model such as KNN?",
   "A": "Faster learning, slower recall, less memory required compared to parameterized models.",
   "B": "Slower learning, faster recall, more memory required compared to parameterized models.",
   "C": "Faster learning, slower recall, more memory required compared to parameterized models.",
   "D": "Slower learning, slower query compared, less memory required to parameterized models.",
   "answer": "C",
   "why": "Instance based models need to mostly store the training data which they refer during the query process. This makes learning faster(as it is just a storage operation), slower query (as the model will process the training data to ans the query), and more memory required to store the training data sets. "
 },
 {
   "index": 10,
   "question": "Accuracy alone is a sufficient metric for classification model evaluation.",
   "A": "True, because the higher the accuracy the better the model performance, and an indicator of how useful the model is.",
   "B": "False, because it is also important to know what the recall is, that is how well it identifies all from a class - how complete the results are.  ",
   "C": "False, because if the number of samples of two classes being identified is not balanced, and one class being a lot less frequent than another (for e.g. 3%), then a 97% accuracy is not saying much about the model as that is achievable by always predicting the more frequent class.",
   "D": "Both B and C",
   "answer": "D",
   "why": "Both accuracy and recall are important metrics to evaluate a classification machine learning model."
 },
 {
   "index": 11,
   "question": "What value would indicate the strongest direct correlation coefficient?",
   "A": "-1",
   "B": "0",
   "C": "1",
   "D": "100",
   "answer": "C",
   "why": "Correlation coefficients range from -1 to 1 continuously. -1 is the strongest inverse correlation; 0 indicates no correlation; 1 indicates the strongest direct correlation."
 },
 {
   "index": 12,
   "question": "Pearson correlation coefficient, r, can take a range of values from +1 to -1. While deciding the feature to split on, what value of correlation coefficient of a feature with respect to target would serve as the best split?",
   "A": "Values close to 1",
   "B": "Values close to 0",
   "C": "Values close to -1 ",
   "D": "Absolute values close to 1",
   "answer": "D",
   "why": "Information gain is maximum when there is maximum correlation. Positive or negative values indicate if they are positively or negatively correlated, either of which would serve as a good split as long as they are strongly correlated."
 },
 {
   "index": 13,
   "question": "knn algorithm belongs to which families (y) of algorithm (Choose one)",
   "A": "Instance Based Model/Algorithms",
   "B": "Parameterized Model/Algorithms",
   "C": "Both of the above",
   "D": "None of the Above",
   "answer": "A",
   "why": "knn algorithm depends on instances seen in the training set. Hence an instance based algorithm."
 },
 {
   "index": 14,
   "question": "Cutler's random tree building method differs from JR Quinlan's  tree building method mainly in two aspects. which of the following answers is one of the two main differences between the two tree building methods?",
   "A": "The split value for each node is determined by: computing correlation of each X value with Y column, selecting highest correlation X and computing it's mean.",
   "B": "The split value for each node is determined by: Randomly selecting two samples of data and taking the mean of their Xi values.",
   "C": "Choosing a Y value randomly and putting it in the training set",
   "D": "Using boosting to improve performance",
   "answer": "B",
   "why": "There are two primary differences between the two decision tree building methods:  1. The feature i to split on at each level is determined randomly. It is not determined using information gain or correlation, etc.  2. The split value for each node is determined by: Randomly selecting two samples of data and taking the mean of their Xi values.    Choice B happens to be one of them"
 },
 {
   "index": 15,
   "question": "Which of the following is true in regards to overfitting:",
   "A": "Describes the noise in the data as opposed to the fundamental relationship in the data",
   "B": "Is indicted by a low coefficient correlation in the training dataset.",
   "C": "Performs very well on testing and training datasets",
   "D": "It is hard to correct your model once it has overfit the data.",
   "answer": "A",
   "why": "Overfitting describes noise, not data, which is answer A.  B is false, as overfitting has a high correlation coefficient in the training dataset.  C is false, as overfitting does not perform well on testing datasets. D is false, as you simply retrain your model using different parameters."
 },
 {
   "index": 16,
   "question": "When applying adaptive boosting algorithm to random decision trees, what type of observation do we care most after equally assigning weight to each tree? ",
   "A": "tree size",
   "B": "error in predicting out sample",
   "C": "error in predicting in sample",
   "D": "if the tree is balanced",
   "answer": "B",
   "why": "The out sample generality and its accuracy should be the focus over others"
 },
 {
   "index": 17,
   "question": "When build a random tree for an array of data. Which group of statements randomly select a feature and randomly choose a split value from the feature? ",
   "A": "# randomly choose feature to split on  i = numpy.random.random_integers(0, data.shape[1])  # randomly choose an index of a value  id = numpy.random.random_integers(0,data.shape[1]-1)  SplitVal = (data[:, i][id] + numpy.delete(data[:,i], id)[random_integers(0,data[:,i].shape[0]-1)])/2",
   "B": "# randomly choose feature to split on  i = numpy.random.random_integers(0, data.shape[0]-1)  # randomly choose an index of a value  id = numpy.random.random_integers(0,data.shape[0])  SplitVal = (data[:, i][id] + numpy.delete(data[:,i], id)[random_integers(0,data[:,i].shape[1]-2)])/2",
   "C": "# randomly choose feature to split on  i = numpy.random.random_integers(0, data.shape[1]-1)  # randomly choose an index of a value  id = numpy.random.random_integers(0,data.shape[1]-1)  SplitVal = (data[:, i][id] + numpy.delete(data[:,i], id)[random_integers(0,data[:,i].shape[1])])/2",
   "D": "# randomly choose feature to split on  i = numpy.random.random_integers(0, data.shape[1]-1)  # randomly choose an index of a value  id = numpy.random.random_integers(0,data.shape[0]-1)  SplitVal = (data[:, i][id] + numpy.delete(data[:,i], id)[random_integers(0,data[:,i].shape[0]-2)])/2",
   "answer": "D",
   "why": "First statement randomly choose feature by using data.shape[1]-1 as high value.  Second and third statement randomly choose two different value from rows. "
 },
 {
   "index": 18,
   "question": "Supervised learning differs from unsupervised clustering in that supervised learning requires    ",
   "A": "A) at least one input attribute.  ",
   "B": "B) input attributes to be unqualified   ",
   "C": "C) at least one output attribute.  ",
   "D": "D) output attributes to be unqualified .",
   "answer": "C",
   "why": "In supervised learning, the output datasets are provided which are used to train the machine and get the desired outputs whereas in unsupervised learning no datasets are provided  instead the data is clustered into different classes"
 },
 {
   "index": 19,
   "question": "what is an example of a parametric based learning algorithm",
   "A": "kernel machines",
   "B": "KNN",
   "C": "RBF networks",
   "D": "Linear regression",
   "answer": "D",
   "why": "A parametric model captures all its information about the data within its parameters. All you need to know for predicting a future data value from the current state of the model is just its parameters"
 },
 {
   "index": 20,
   "question": "Which of the following statements is true with respect to parameterized and instance-based models?",
   "A": "Parameterized models train quicker but take up more memory as compared to instance-based models.",
   "B": "Parameterized models take more time to train as well as take up more memory as compared to instance-based models.",
   "C": "Instance-based models make stronger assumptions about the underlying functions mapping the data as compared to parameterized models.",
   "D": "Instance-based models have a higher memory footprint as compared to parameterized models.",
   "answer": "D",
   "why": "Parameterized models usually have a bounded number of parameters and hence a lower memory footprint."
 },
 {
   "index": 21,
   "question": "Which of the following statements is true about boosting (as implemented by Adaboost) and overfitting?",
   "A": "Boosting frequently overfits because it gives more weight to weak learners that correctly classify certain points.",
   "B": "Boosting is likely to overfit because the combination of a large number of weak learners with varied weights results in high model complexity.",
   "C": "Boosting typically doesn't overfit because it combines a large number of weak learners with different errors.",
   "D": "Boosting typically doesn't overfit because the model isn't complex enough for overfitting to occur.",
   "answer": "C",
   "why": "Adaboost is designed to give high weight to weak learners that have different errors from the rest of the weak learners. This results in lower average error."
 },
 {
   "index": 22,
   "question": "what is an example of a parametric based learning algorithm",
   "A": "kernel machines",
   "B": "KNN",
   "C": "RBF networks",
   "D": "Linear regression",
   "answer": "D",
   "why": "A parametric model captures all its information about the data within its parameters. All you need to know for predicting a future data value from the current state of the model is just its parameters."
 },
 {
   "index": 23,
   "question": "Which of the following statements is true with respect to parameterized and instance-based models?",
   "A": "Parameterized models train quicker but take up more memory as compared to instance-based models.",
   "B": "Instance-based models have a higher memory footprint as compared to parameterized models.  ",
   "C": "Instance-based models make stronger assumptions about the underlying functions mapping the data as compared to parameterized models.",
   "D": "None of the above",
   "answer": "B",
   "why": "Parameterized models usually have a bounded number of parameters and hence a lower memory footprint."
 },
 {
   "index": 24,
   "question": "Which one is correct?  ",
   "A": "The overfitting is more likely happens when the training set is small.",
   "B": "Boosting have an advantage that it will not be overfitting.",
   "C": "There are two regression function A and B, if A is simple than B, A must have a better performance than B on testing set.",
   "D": "Given a data set of n nodes, half for training and half for testing, the error between the training result and the testing result will become smaller when n becomes smaller",
   "answer": "A",
   "why": "The overfitting is more likely happens when the training set is small."
 },
 {
   "index": 25,
   "question": "When creating a model for predicting an outcome should overfitting be considered as a rubic for determining the quality of the model, and why? ",
   "A": "No. Overfitting doesn't predict the quality of a model, because it's just 1 aspect of creating a good model.",
   "B": "Yes. Changing aspects of the model to minimize overfitting would result in a model that has a higher predictive quality.",
   "C": "Yes. Overfitting a model would produce the best results, as the more a model is overfit the higher the predictive quality.",
   "D": "No. Overfitting should only be considered for KNN models.",
   "answer": "B",
   "why": "Reducing overfitting improves predictive quality as the less rigid, and more general a model becomes, the higher the quality of its prediction."
 },
 {
   "index": 26,
   "question": "Which of the following is true about KNN?",
   "A": "We weight the contributions of each of the data points stored based on how distant they are.",
   "B": "KNN is parametric model.",
   "C": "KNN uses much more memory than linear regression.",
   "D": "KNN is much more effective than Linear regression for all types of problems.",
   "answer": "C",
   "why": "Because you store all individual data points vs a parametric representation of the model."
 },
 {
   "index": 27,
   "question": "Your coworker, who has limited knowledge of modeling, is tasked with developing a model to predict the temperature from a number of different observation stations, each of which report weather-related metrics such as atmospheric pressure.  However, many of the stations are randomly missing various pieces of equipment, and so many of your observations are missing a few random measurements.  Assume your coworker doesn't understand imputation. Between a linear regression learner, kNN, and a decision tree, which model would you expect to offer the best performance out of the box?",
   "A": "A linear regression learner, because linear regression easily lets you predict the missing data.",
   "B": "A linear regression learner, because linear regression is better at forecasting trends outside of the range of previous observations (i.e. tomorrow's weather).",
   "C": "A KNN algorithm, because you can use neighboring weather stations to fill in missing data.  The other methods cannot easily do this.",
   "D": "A decision tree learner, because decision trees aren't dependent on having non-missing data in each observation.",
   "answer": "D",
   "why": "Decision trees easily handle missing data, and do not algorithmically depend on complete observations.  This is in contrast to KNN (many distance measures need each feature to be non-missing) and linear regression (need all features to be non-missing or encode missingness as an indicator - can't just ignore it)."
 },
 {
   "index": 28,
   "question": "What is the main advantage of using Ensemble learning methods, such as bagging?",
   "A": "It runs faster than any one specific model (such as KNN). ",
   "B": "It reduces variance by computing average predictions over several models trained using several sets of the training set. ",
   "C": "It produces a model that can easily be interpreted (such as a decision tree). ",
   "D": "It produces good models with very few explanatory variables.",
   "answer": "B",
   "why": "Bagging involves drawing several subsamples, or bags, selected randomly with replacement from the training data set.Each of these bags are then used to train a different model.Each model is then used to do predictions that are then averaged to do the final prediction. The inherent feature of selecting bootstrap samples and then averaging the output reduces the variance as compared to using outputs from one specific model, such as KNN or Decision Tree. "
 },
 {
   "index": 29,
   "question": "While measuring the performance of a learner model, which of the below options is the best:  1. RMSE must be preferred over correlation because correlation gives scaled results but RMSE gives absolute results  2. RMSE must be preferred over correlation because through correlation performance can't be measured  3. Correlation must be preferred over RMSE because correlation provides the information about scaled similarity between datapoints  4. Neither correlation or RMSE can measure learner's performance  4.   ",
   "A": "1",
   "B": "2",
   "C": "3",
   "D": "4",
   "answer": "A",
   "why": "RMSE must be preferred because correlation gives scaled results but RMSE gives absolute results."
 },
 {
   "index": 30,
   "question": "You are given a black box estimator that can accept three genes at a time, and outputs an estimate of the person's height. The procedure of bagging will be most useful in what scenario?",
   "A": "Specific genes bias the estimator in consistent ways, and other genes have no effect on the predicted height.",
   "B": "It is unknown which genes are most useful in predicting height, but many triplets of genes return predictions with normal variance around a consistent value with some small bias.",
   "C": "A specific three genes together accurately predict a person's height, but it has random variance around the true value.",
   "D": "A combination of six genes together accurately predicts the height.",
   "answer": "B",
   "why": "Bagging will work to average out the small biases and random variance around the true prediction, and produce a better estimator than an individual prediction using any three genes alone."
 },
 {
   "index": 31,
   "question": "Which one of the following choices is most accurate for boosting?",
   "A": "Boosting is generally used to decrease variance error (overfitting)",
   "B": "Boosting allows replacement in the bootstrapped sample",
   "C": "Boosting is primarily used to reduce bias by performing selective sampling",
   "D": "Boosting can only be used in homogeneous populations",
   "answer": "C",
   "why": "All the other answers describe more accurately bagging. The only answer that strictly applies to boosting is C"
 },
 {
   "index": 32,
   "question": "Which is NOT a primary implementation component of Cutler's Random Tree that is not present in the methodology originally proposed by JR Quinlan?",
   "A": "The feature to split on at each level is determined randomly.",
   "B": "The leaf_size for each branch is determined randomly prior to the creation of that branch.",
   "C": "The split value for each node is determined by randomly selecting two samples of data and taking the mean of their feature values.",
   "D": "B and C.",
   "answer": "B",
   "why": "This is taken from the RTLearner component of the MC3-P1 assignment and clearly documented here:    http://quantsoftware.gatech.edu/MC3-Project-1#Part_1:_Implement_RTLearner_.2830.25.29    Leaf_size does not vary within the tree, it is fixed at the time the tree is created."
 },
 {
   "index": 33,
   "question": "Which of the following is used as a measure for calculating the best fit for samples predicted by a model?",
   "A": "Average absolute deviation(AAD)  ",
   "B": "Mean signed deviation(MSD)  ",
   "C": "Root-mean-square deviation(RMSD)",
   "D": "Squared deviations from the mean(SDM)",
   "answer": "C",
   "why": "Root mean square deviation is the method which is used for measuring the difference between the values predicted by a model or an estimator and the values actually observed."
 },
 {
   "index": 34,
   "question": "For which of the following problems will a regression learning algorithm be applicable as opposed to a classification algorithm?",
   "A": "Predicting percentage of breast cancer occurences according to age-groups.",
   "B": "Determining if a stock price will go up or down the next trading day using previous day stock prices.",
   "C": "Identifying the breed of dog in a picture.",
   "D": "Predicting the outcome of 2016 US presidential elections - who will be the next US president?",
   "answer": "A",
   "why": "A - requires you to predict the percentage of breast-cancer occurence which is a continuous output while all the other answers are classification problems."
 },
 {
   "index": 35,
   "question": "Which of the following statements about the prediction performance of bagging is true",
   "A": "Bagging usually helps the prediction performance of stable learners (such as kNN)",
   "B": "Bagging usually improves the variance if the underlying learners have high variance",
   "C": "Bagging usually improves the bias of the learned model if the underlying learner is very biased",
   "D": "If the underlying learner used by Bagging is such that a small change to the training causes a large change the hypothesis, than bagging is expected to increase variance",
   "answer": "B",
   "why": "A,C,D and really saying the same thing: That bagging will usually  not improve on a stable learner (kNN). Rather bagging is expected to improve the variance of an unstable (decision tree) learner. The reason the former wont benefit is that (say in kNNs) case removing a couple of the points won't significantly change the hypothesis. In a sense is what bagging is doing for every bag and than performing a vote between them (for A,C,D most bags is expected to give similar answers)."
 },
 {
   "index": 36,
   "question": "Which of the following statements is TRUE in regards to overfitting of linear regression models?  Note: D = number of dimensions",
   "A": "For linear regression models, overfitting is more likely to occur as D INCREASES",
   "B": "For linear regression models, overfitting is more likely to occur as D DECREASES",
   "C": "For linear regression models, overfitting does not occur",
   "D": "For linear regression models, overfitting occurs, but is equally as likely for any value of D",
   "answer": "A",
   "why": "Overfitting is more likely as D increases because the resulting polynomial will produce a line that more closely matches the training data, which most likely contains either some random error or some noise."
 },
 {
   "index": 37,
   "question": "What is the difference between knn and kernel regression?",
   "A": "Kernel regression uses a single kernel function to determine a regression while knn uses k neighbors.",
   "B": "Kernel regression weights the contribution of the nearest k data points in terms of their distance, while knn considers the nearest k data points weighted equally.",
   "C": "Kernel regression is able to fit data that is polynomial in nature while knn regression is only able to fit data described by lines.",
   "D": "Kernel regression only considers a single data point while knn considers k data points.",
   "answer": "B",
   "why": "This answer is correct because kernel regression takes distance away from an answer into consideration and weights data points accordingly, while knn does not take weights into consideration."
 },
 {
   "index": 38,
   "question": "Which of the following statements is TRUE in regards to overfitting of linear regression models?  Note: D = number of dimensions",
   "A": " For linear regression models, overfitting is more likely to occur as D INCREASES",
   "B": "For linear regression models, overfitting is more likely to occur as D DECREASES",
   "C": " For linear regression models, overfitting does not occur",
   "D": "For linear regression models, overfitting occurs, but is equally as likely for any value of D",
   "answer": "A",
   "why": "Overfitting is more likely as D increases because the resulting polynomial will produce a line that more closely matches the test data, which most likely contains either some random error or some noise."
 },
 {
   "index": 39,
   "question": "In which scenario are you most likely to encounter overfitting?",
   "A": "A polynomial with degree of 1 in a parametric model",
   "B": "A value of k =1  in a KNN model",
   "C": "A value of k = size of data in a KNN",
   "D": "Overfitting can not occur in any of the above scenarios",
   "answer": "B",
   "why": "Because when K = 1 the model will retrieve the exact value for y in the model. It is taking the average of k =1 data point."
 },
 {
   "index": 40,
   "question": "I need to predict the price of gold by the end of the year. Which algorithms should I use?",
   "A": "Classification Tree",
   "B": "Linear Regression",
   "C": "Support Vector Machines",
   "D": "Neural Network",
   "answer": "B",
   "why": "Gold prices are represented by a continuous numerical value, which makes linear regression the answer for this question. The other solutions are classification-based algorithms, which are best suited for discrete variables"
 },
 {
   "index": 41,
   "question": "Assuming all three models are equally accurate in their predictions, which model - kNN, decision tree, or linear regression - is favorable, and why?",
   "A": "kNN is the best model because it has the lowest query cost.",
   "B": "Linear regression is favorable because it is the fastest to query.",
   "C": "Decision tree should be used because it has the fastest training time.",
   "D": "All models are equally favorable because they produce the same level of accuracy.",
   "answer": "B",
   "why": "If the accuracy is the same for all three models, then linear regression is the best method, as it has the the fastest query time and, in general, lower training times."
 },
 {
   "index": 42,
   "question": "Which of the following is better to be judged as a unsupervised learning algorithm instead of a supervised learning algorithm? Suppose all the algorithms are either supervised learning algorithm or unsupervised learning algorithm.",
   "A": "Given a database which contains the feature of many credit card applicants and wether they should be granted a credit card, create a algorithm which can help the bank decide wether a new applicant should be granted a credit card.",
   "B": "Given a large database which contains the behavior pattern of many clients, together with their names, create a algorithm which can divide them into different client groups, so that in each group the clients may have similar hobbies.",
   "C": "Given the data set which have the information about many houses, and how much they are finally bought by someone. Create a algorithm which can give advice to people about how much they should sell the house if they want.",
   "D": "Given a data set of many vectors, each of them represents a image which contains only one hand-written digit between 0 and 9, and labels telling you what the digit in each vector actually is. Build a algorithm which can identify what digit does a new image contains when giving its equivalent vector (assume it only contains one digit).",
   "answer": "B",
   "why": "This problem is a unsupervised one where we only use the X and no Y is provided and used, actually it is more like a clustering algorithm. For A, it is a classification algorithm, Y is wether they are granted credit card. For C, it is regression algorithm, Y is the price. For D, it is a classification algorithm, Y is the digit which the vector/image represents."
 },
 {
   "index": 43,
   "question": "Which of the following methods will benefit least from bagging. (Assume perfectly uniform random sampling and sufficiently high number of bags)",
   "A": "kNN",
   "B": "Decision trees",
   "C": "Linear regression",
   "D": "Depends on nature of data",
   "answer": "C",
   "why": "Linear fitting is a convex problem for which the best possible fit will be almost the same when trained on different random samples of data (because we are assuming a perfectly uniform random sampling). Thus bagging will create the same model over and over whose combination will again be the same model as the one trained on the original data.  Option d is incorrect because even if the data has a very well linear fit, it only means that the linear regression can be better than the other two methods but the question asks for an improvement of performance relative to non bagging version of the same model."
 },
 {
   "index": 44,
   "question": "Which of the following statement about boosting is TRUE?",
   "A": "Boosting is suitable for low variance/high bias models.",
   "B": "The training error of boosting classifier monotonically decreases as the number of iterations in the boosting algorithm increases.",
   "C": "Classifier x is a boosting classifier of weak learners y. If y is a linear classifier, then x is also a linear classifier.",
   "D": "In boosting, each model is built independently.",
   "answer": "A",
   "why": "A. Boosting aims to decrease bias, and sequentially extract information from the \"residual\" of previous model. Starting with high variance models is likely to cause overfitting.  B. the training error decrease but not necessarily monotonically   C. The boosted classifier is linear combinations of weak classifiers and can form a more complex decision boundary   D. Each model is added sequentially based on the information from previous models, not independent"
 },
 {
   "index": 45,
   "question": "Which of the following is better to be judged as a unsupervised learning algorithm instead of a supervised learning algorithm? Suppose all the algorithms are either supervised learning algorithm or unsupervised learning algorithm.",
   "A": "Given a database which contains the feature of many credit card applicants and wether they should be granted a credit card, create a algorithm which can help the bank decide wether a new applicant should be granted a credit card.",
   "B": "Given a large database which contains the behavior pattern of many clients, together with their names, create a algorithm which can divide them into different client groups, so that in each group the clients may have similar hobbies.",
   "C": "Given the data set which have the information about many houses, and how much they are finally bought by someone. Create a algorithm which can give advice to people about how much they should sell the house if they want.",
   "D": "Given a data set of many vectors, each of them represents a image which contains only one hand-written digit between 0 and 9, and labels telling you what the digit in each vector actually is. Build a algorithm which can identify what digit does a new image contains when giving its equivalent vector (assume it only contains one digit).",
   "answer": "B",
   "why": "Even we have the information about their names, that shouldn’t be judged as Ys, this problem is still a unsupervised one where we only use the X, actually it is more like a clustering algorithm. For A, it is a classification algorithm, Y is wether they are granted credit card. For C, it is regression algorithm, Y is the price. For D, it is a classification algorithm, Y is the digit which the vector/image represents."
 },
 {
   "index": 46,
   "question": "In which of the following models is overfitting most likely to occur?",
   "A": "Non-Parametric",
   "B": "Linear",
   "C": "Parametric",
   "D": "Non-Parametric and Non-Linear",
   "answer": "D",
   "why": "Non-parametric and non-linear models are more likely cause overfitting because of the flexibility in terms of learning the target function. The learner needs to adjust to random features of the training data. Therefore, the performance on unseen data is worse compared to training data."
 },
 {
   "index": 47,
   "question": "Which of the following improvements in C4.5 algorithm over ID3 for building tree makes it less sensitive to features with large number of values(example feature: SSN)",
   "A": "Pruning",
   "B": "Ability of handle continuous attributes",
   "C": "Splitting criteria focused on difference on entropy rather than entropy itself",
   "D": "They both will be equally sensitive",
   "answer": "C",
   "why": "C4.5 allows to measure a gain ratio in addition to ID3 which will avoid choosing features such as SSN that are prone to low entropy but are un-useful."
 },
 {
   "index": 48,
   "question": "When comparing the performance of a bootstrap aggregating from bags=[1toN] of Random Decision Trees leaf size=[1toK] on a training  set of shape [500,4] and a testing set of 700 elements, why is using RMSE unreliable?",
   "A": "The testing set is larger than the training set.",
   "B": "RMSE is scale-dependent.",
   "C": "RMSE is only a measure of deviation between sample and population values.",
   "D": "RMSE is not an effective metric to evaluate bootstrap aggregation.",
   "answer": "B",
   "why": "RMSE is a scale-dependent function, so when comparing against multiple variables (in this case against the number of bags and number of leaves), then it is not an effective measure. RMSE is only effective in evaluating single-variable distributed models."
 },
 {
   "index": 49,
   "question": "In which of the following models is overfitting most likely to occur?",
   "A": "Non-Parametric",
   "B": "Linear",
   "C": "Parametric",
   "D": "Non-Parametric and Non-Linear",
   "answer": "D",
   "why": "Non-parametric and non-linear models have lot of flexibility in terms of target function. The learner needs to adjust to random features of the training data. Therefore, the performance on unseen data becomes worse compared to training data."
 },
 {
   "index": 50,
   "question": "Which of the following statements is wrong on the comparison between decision tree algorithms CART and C4.5?",
   "A": "CART and C4.5 both use entropy info-gain to to select input variables",
   "B": "C4.5 can have multiple splits at each node while CART is binary",
   "C": "C4.5 tends to build smaller tree compared to CART",
   "D": "Input variables of CART and C4.5 can be categorical/continuous",
   "answer": "A",
   "why": "CART uses Gini impurity as a measure of the frequency of incorrect labeling. C4.5 utilizes information entropy concept, similar to ID3. "
 },
 {
   "index": 51,
   "question": "Which model has the lowest query cost out of KNN, decision tree and linear regression)?  ",
   "A": "a) KNN, because a query amounts to an efficient database loookup.",
   "B": "b) Decision tree, since it has the clear advantage of going only through a few branches  ",
   "C": "c) Linear Regression, because calculating a function value can't be beat  ",
   "D": "d) Decision trees and linear regression have in all cases the same lookup speed - since the few tree branch traversals and calculating a function value amount to about the same number of operations    ",
   "answer": "C",
   "why": "Correct answer is c) - since calculating a value is scales best towards infinity (always cost of 1)  "
 },
 {
   "index": 52,
   "question": "What is an not an example of how to determine the best feature in a decision tree that was discussed in class?",
   "A": "Gini Index",
   "B": "Correlation",
   "C": "Entropy",
   "D": "Variance Reduction",
   "answer": "D",
   "why": "In the course, we specifically discussed using entropy, correlation, and the gini index in order to determine the best feature in a tree. When discussing code, we looked at using correlation, but both correlation and entropy \"should\" return similar results. According to research, variance reduction is another common way to determine the best feature. We did not, however, discuss this in class."
 },
 {
   "index": 53,
   "question": "Let the error of hypothesis h over training data be error_train (h) and the error of h over the entire distribution be error_distribution (h). Then a hypothesis h over-fits the training data if there is an alternative hypothesis h' such that:",
   "A": "error_train (h) < error_train (h'); error_distribution (h) < error_distribution (h')",
   "B": "error_train (h) < error_train (h'); error_distribution (h) > error_distribution (h')",
   "C": "error_train (h) > error_train (h'); error_distribution (h) < error_distribution (h')",
   "D": "error_train (h) > error_train (h'); error_distribution (h) > error_distribution (h')",
   "answer": "B",
   "why": "This question is to test the definition of over-fitting. When hypothesis h has smaller error on training data but larger error on actual data than an alternative hypothesis h', it over-fits the training data."
 },
 {
   "index": 54,
   "question": "Which of the following is true of the Random Tree Algorithm proposed in A Cutler's paper, as compared to JR Quinlan's Decision Tree Algorithm proposal?",
   "A": "The split factor X is chosen randomly, and the split value is determined by finding the median row value.",
   "B": "Computational cost is minimized by removing information gain calculations necessary in JR Quinlan's proposal. ",
   "C": "The split factor X is chosen randomly, and the split value is determined by finding the mean of two random row values.",
   "D": "Both B and C.",
   "answer": "D",
   "why": "(D) is correct, because (B) and (C) are True. (B) is true, since A Cutler's proposal removes the need to compute information gain (e.g. Correlation, Entropy) and calculate median row value. (C) is true, as it accurately described both the randomness in the Split Factor AND the split value. (A) is not true, because the median row value determination is an aspect of JR Quinlan's Decision Tree Algorithm proposal, not A Cutler's paper. "
 },
 {
   "index": 55,
   "question": "Which choices can make the prediction more accurate?",
   "A": "Make the leaf-size as small as possible",
   "B": "Less data",
   "C": "More baglearner",
   "D": "More factors",
   "answer": "C",
   "why": "  C. More baglearner will give the decision tree better training"
 },
 {
   "index": 56,
   "question": "in bagging, where M stands for the number of different bags and N' is the number of samples used to train each bag, N' is",
   "A": "chosen from 60% of N selected randomly without replalcement",
   "B": "chosen randomly based on each learner's preference",
   "C": "chosen randomly from the 40% allocated to the test set in a 60/40 train/test set data split",
   "D": "chosen from 100% of N selected randomly with replacement",
   "answer": "D",
   "why": "Because in 03-04 Ensemble Learners, Bagging And Boosting video, the instructor notes state that in most implementations N' = N because the training data is sampled with replacement (making about 60% of the instances in each bag unique)  "
 },
 {
   "index": 57,
   "question": "Which statement is true regarding boosting using weak learner?",
   "A": "Boosting suffers from overfitting because it doesn’t generate additional boostrap samples for training.",
   "B": "The predictive force of boosting is less since individual weak learner is not trained on the entire training set.",
   "C": "The individual weak learner must be slightly better than random guessing. ",
   "D": "The primary aim of boosting is to reduce or eliminate overfitting.",
   "answer": "C",
   "why": "Because random guessing can't be improved into strong leaner through boosting."
 },
 {
   "index": 58,
   "question": "Which statement is true regarding boosting using weak learner?",
   "A": "Boosting suffers from overfitting because it doesn’t generate additional boostrap samples for training.",
   "B": "The predictive force of boosting is less since individual weak learner is not trained on the entire training set.",
   "C": "The individual weak learner must be slightly better than random guessing. ",
   "D": "The primary aim of boosting is to reduce or eliminate overfitting.",
   "answer": "C",
   "why": "Because boosting can’t improve upon total random guessing."
 },
 {
   "index": 59,
   "question": "Which of the following would be a reason NOT to use a parametric regression learning model?  ",
   "A": "It is a well-defined problem for which we can reasonably extrapolate data points outside of the sample range",
   "B": "The model will be fed millions of rows of sample data which are applied to the model in a nightly batch process",
   "C": "Parametric models use complex equations which are not intuitive to the end user",
   "D": "The behavior of the data is hard to model mathematically and is better suited to an instance-based approach",
   "answer": "D",
   "why": "Parametric regression is a biased learning model, meaning that we will provide an initial guess as to its shape. A non-parametric approach would be more suitable for problems that are hard to model mathematically."
 },
 {
   "index": 60,
   "question": "You are designing a cat face detection mobile app. Even though most of the app will be driven by server side API's most of the face detection logic will reside on the mobile app itself. What type of machine learning algorithm would you choose to use and why?",
   "A": "An instance based approach like kNN because it's really quick and requires less processing overhead.",
   "B": "A parameterized approach like an SVM or Neural Net since the trained model is needs is fairly lightweight function and requires a small footprint.",
   "C": "An instance based approach like kNN because of it's small storage requirements.",
   "D": "A parameterized approach since the mobile app will help collect the parameters while it learns to recognize cats' faces.",
   "answer": "B",
   "why": "A parameterized approach is preferable in this scenario as  the resulting model can be fairly lightweight and fast to execute on the storage constraints and offline requirements of a mobile device. An instance based method, while quick to execute also, will require as much storage of all the input data required to accurately predict what's being classified (which may be significant)."
 },
 {
   "index": 61,
   "question": "KNN overfitting typically occurs when …",
   "A": "The out of sample error is high and the in sample error is low when K decreases.",
   "B": "The in sample error is high and the out of sample error is low when K decreases.",
   "C": "The out of sample error is high and the in sample error is low as K increases.",
   "D": "The in sample error is high and the out of sample error is low when K increases.",
   "answer": "A",
   "why": "The relationship is different than polynomial degrees of error. And overfitting appears to occur between in and out of samples when K is decreasing with in sample error decreasing and out of sample error increasing."
 },
 {
   "index": 62,
   "question": "What is one noticeable difference between JR Quinlan and A Cutler's implementation of the their respective decision tree?",
   "A": "JR Quinlan's implementation uses correlation to determine the best feature to split on and A Cutler's implementation uses enthropy to determine the best feature to split on.",
   "B": "In JR Quinlan's implementation, the mode of the data in the selected feature is computed as the split value and in A Cutler's implementation, the average of the data in the selected feature is chosen as the split value.",
   "C": "In JR Quinlan's Decision Tree Algorithm, if all the Y values are the same, the algorithm returns a leaf node with the average of all the Y values and in A Cutler's Decision Tree Algorithm, if all the Y values are the same, the algorithm returns a random Y value.",
   "D": "In JR Quinlan's implementation, the median of the data in the selected feature is computed as the split value and in A Cutler's implementation, two random data points are chosen from their selected feature data and their average is computed as the split value.",
   "answer": "D",
   "why": "Page 7 of the How to Learn a decision tree slides - JR Quinlan's algorithm -> SplitVal\t= data[:,i].median().     Page 8 - A Cutler's algorithm  -> SplitVal=\t(data[random,i]+data[random,i])/2\t."
 },
 {
   "index": 63,
   "question": "When building a decision tree, what is the benefit of finding the correlation of each feature (X1, X2, X3...) with the labels (Y) ",
   "A": "Features with LARGEST ABSOLUTE correlation to the labels (Y) are the best features to split on ",
   "B": "Features with SMALLEST ABSOLUTE correlation to the labels (Y) are the best features to split on ",
   "C": "The correlation of the features (X1, X2...) to the labels (Y) is directly used to calculate the trees prediction accuracy",
   "D": "The correlation of the features (X1, X2...) to the labels (Y)  is used to calculate query time of decision tree",
   "answer": "A",
   "why": "Correct answer is a) because higher absolute correlation between the features (X1, X2...) and labels (Y) indicates that it will best feature to split on to build the best performing decision tree"
 },
 {
   "index": 64,
   "question": "Per Cutler's algorithm, how is randomization used by a random decision tree learner?",
   "A": "In querying the tree.",
   "B": "In calculating entropy needed to choose the best factor.",
   "C": "In selecting a factor and its split value.",
   "D": "Only in selecting a factor to split on.",
   "answer": "C",
   "why": "Cutler's paper defines how to build a single random tree learner, introducing the method of using randomization to both select a factor and its split value."
 },
 {
   "index": 65,
   "question": "In the decision tree algorithm by J R Quinlan, if the split value is calculated to be the second largest   element among the data rows of the best feature column, instead of the median of the data rows of the best   feature column. What will be the worst case height (i.e. maximum) of the decision tree? Assume n to be the number of rows of data.",
   "A": "Log2(n)",
   "B": "n^2",
   "C": "n-1",
   "D": "n",
   "answer": "C",
   "why": "For a data set of n rows, if we build the decision tree as per the question,   then there will be (n-1) edges from the root of the tree till the deepest leaf in the worst case scenario.   Hence worst case height of the decision tree will be (n-1). If second largest data element repeats in more than one row,   then height of tree will be less than n-1. Hence only the worst case scenario i.e. maximum height of tree possible is asked."
 },
 {
   "index": 66,
   "question": "Hand written digit recognition (0-9) is a good application of supervised machine learning. Examples of handwritten digits and their corresponding mapping to integers [0,9] are provided as input for training. Which of the following statements are true about it?",
   "A": "It is a form of regression because the output is a numerical",
   "B": "It is NOT a form of classification because there are more than two types of possible answers ",
   "C": "It is a form of classification since the output is in the form of labelled classes [0-9]",
   "D": "It is a form of regression since training data with examples is provided",
   "answer": "C",
   "why": "Classification is a form of supervised machine learning where given an input, the expected response is a label from a known set of discrete classes"
 },
 {
   "index": 67,
   "question": "Assume we have a model and we have tested in sample data and out of sample data over a specific parameter (leaf size, degrees of freedom etc).  What should we observe on the in sample and out of sample errors to identify overfit: ",
   "A": "In sample error is increasing and out of sample error is increasing",
   "B": "In sample error is decreasing and out of sample error is increasing",
   "C": "In sample error is decreasing and out of sample error is decreasing",
   "D": "In sample error is increasing and out of sample error is decreasing",
   "answer": "B",
   "why": "According to lecture, 03-03 Assessing a Learning Algorithm, it shows a a graph where in sample (test) error increases and the out of sample error (train) is increasing.  This shows the model is overfit to training data."
 },
 {
   "index": 68,
   "question": "Which of the following option can be the characteristic of both Quinlan and Cutler decision trees, in other words, which of the following feature can be used by both of them?",
   "A": "Using Correlation to determine the best feature to split.",
   "B": "Feeding random data for the decision tree creation and using ensemble of learners.",
   "C": "None of these.",
   "D": "Randomly selecting the feature to split on.",
   "answer": "B",
   "why": "Correct answer: Feeding random data for the decision tree creation and using ensemble of learners.    Explanation:  Quinlan's decision tree algorithm uses correlation to pick the feature to split and then recommends taking mean of the values of that feature. These are the unique properties of Quinlan decision tree algorithm. On the other hand Cutler's decision tree makes use of random selection of the feature to split on.    However it is always possible to feed random data to both Quinlan's and Cutler's decision tree algorithms and then use ensemble of learners to improve the prediction."
 },
 {
   "index": 69,
   "question": "While building a decision tree, which of the following methods can would most likely cause in taking longer to query the tree?",
   "A": "Choosing a smaller leaf size.",
   "B": "Using Information gain to select the feature to split on, instead of randomly selecting one.",
   "C": "Limiting the depth of the tree (how many levels the tree can have).",
   "D": "Choosing the median of all points from the split feature, instead of taking the mean of two random points. ",
   "answer": "A",
   "why": "A - Choosing a smaller leaf size would cause the tree to branch out more from a node, increasing it's depth and complexity. This would in turn result in larger trees and more time to query them.   B- Using information gain would actually shorten the query time as it would find the best feature to split on.  C- Limiting the depth would result in a shorter tree, which would be easier to query.  D- Choosing the median of all points from the split feature would mean that we are dividing our data into the following left and right nodes equally. This would also shorten query time.      It may be easy to get confused between what causes the tree to take longer to build vs what causes it to take longer to query. "
 },
 {
   "index": 70,
   "question": "In following algorithm , which one doesn't belong to supervised learning  ",
   "A": "Nearest Neighbor",
   "B": "Decision Trees",
   "C": "k-means clustering",
   "D": "Linear Regression",
   "answer": "C",
   "why": "The difference between supervised and unsupervised learning is whether the data come with the  answer. A B D , all of them has be given answers while C has not. Crusting need associate rules because no answers would be provided. "
 },
 {
   "index": 71,
   "question": "Which of the following techniques DO NOT help a Decision Tree generalize better (i.e. avoid overfitting)?",
   "A": "Pruning",
   "B": "Setting an appropriate leaf size",
   "C": "Limiting the number of training samples",
   "D": "Limiting the depth of a Tree",
   "answer": "C",
   "why": "Limiting the number of training samples increases the bias of the tree. Although it may make the tree simple, but does not necessarily make the tree generalize better.    The other options all may help make the tree generalize better."
 },
 {
   "index": 72,
   "question": "Which of the following correctly describe unsupervised learning?",
   "A": "Scheme is being provided with actual outcome",
   "B": "Algorithms that try to find correlations without any external inputs other than the raw data.",
   "C": "It doesn't have training data",
   "D": "Decision tree algorithm is one of unsupervised learning",
   "answer": "B",
   "why": "In unsupervised learning the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data"
 },
 {
   "index": 73,
   "question": "Considering Parametric and Instance based learners - which of the following is FALSE ",
   "A": "In Kernel regression we weight the contributions of the each of the nearest data points based on how distant they are",
   "B": "In Parametric regression as the model complexity increases the number of parameters increase",
   "C": "KNN usually requires a sort across all the data",
   "D": "In Linear regression as we add more data, time to query will increase.",
   "answer": "D",
   "why": "Linear regression is a parametric model based learner and hence the addition of new data will only increase the training time but the time to query will not change. "
 },
 {
   "index": 74,
   "question": "You have been tasked with building a learning model that must be trained with incoming sample data in real-time. Which model would be the most suitable and why?  ",
   "A": "Regression, since we only need to plug the sample data into an equation to train it.",
   "B": "KNN, since there is no training needed, we just add the new data point.",
   "C": "Decision tree, since we can just add a leaf to the tree for each new sample.",
   "D": "It depends on how well-defined the problem is, and if we can make a guess as to the shape of the model.",
   "answer": "B",
   "why": "A K-Nearest Neighbor model does not need to be trained when we add new data points. Query times can be slower, but since there are no requirements with regard to query performance, KNN is the most suitable."
 },
 {
   "index": 75,
   "question": "Which one of the following statement of Bagging and Boosting is correct?",
   "A": "Bagging are more likely to overfit the data than Boosting as the number of bags increases.",
   "B": "The chance of selecting each data points is the same for all bags when using Boosting to do sampling.",
   "C": "Data points in bags are sampled from training data without replacement when using Boosting.",
   "D": "When sampling using Boosting, the chance of selecting each data points into bags is different except for the first bag. ",
   "answer": "D",
   "why": "The chance of selecting each data point is weighted according to the error of that data point in the previous bag."
 },
 {
   "index": 76,
   "question": "Which decision tree algorithm is better in terms of performance for use in a ensemble learner",
   "A": "JR Quinlann's decision tree algorithm",
   "B": "Adele Cutler's Random tree algorithm",
   "C": "KNN algorithm",
   "D": "Both A and B combined",
   "answer": "B",
   "why": "Adele Culter's algorithm performs better because randomly selecting features and values to split on significantly speeds up the algorithm. Also, since randomness and averaging in involved, the outcome of the ensemble learner is more accurate in terms of predicting correct Y values.  It is not 'C' because KNN is not even a decision tree algorithm."
 },
 {
   "index": 77,
   "question": "Suppose you have a constant stream of new data and you can summarize your data points accurately using some algorithm like Reservoir Sampling (https://en.wikipedia.org/wiki/Reservoir_sampling). Your requirement is to be able to query (i.e classify a test point) based on previous data at any time. Which classification algorithm will you choose?",
   "A": "Decision Trees with Bagging",
   "B": "Decision Trees with Boosting",
   "C": "KNN",
   "D": "Linear Regression",
   "answer": "C",
   "why": "The primary requirement is to add new data. Although it takes longer (O( n log n) to query KNN, by limiting N using sampling, it will be faster than building a decision tree or linear regression at query time.   "
 },
 {
   "index": 78,
   "question": "Which method is not included in supervised learning?",
   "A": "linear regression",
   "B": "K nearest neighbor",
   "C": "clustering",
   "D": "random forest",
   "answer": "C",
   "why": "Clustering is a method of unsupervised learning, which is based on relationships among the variables in the data without feedback based on the prediction results."
 },
 {
   "index": 79,
   "question": "Which of the following statements is true about Regression versus Classification Learning.",
   "A": "Regression Learners are usually faster to build their models but slower to query than classification learners",
   "B": "Classification learners usually are faster to build their models than query them.",
   "C": "Regression Learners are usually faster to query but slower to build their models than Classification Learners.",
   "D": "All of the above is true.",
   "answer": "C",
   "why": "Because the regression model usually do expensive processing before hand to build the model, and they produce an equation that is easily used for querying."
 },
 {
   "index": 80,
   "question": "When does overfitting most likely to occur?",
   "A": "Overfitting occurs when a model learns from both training and test dataset",
   "B": "Overfitting occurs when a model is optimized well for prediction",
   "C": "Overfitting occurs when a model has validation dataset",
   "D": "Overfitting occurs when a model learns noises through training data",
   "answer": "D",
   "why": "D) Overfitting occurs when a model learns noises from training data. It is more likely to overfit on actual test dataset since the model \"over-learned\" from the training dataset."
 },
 {
   "index": 81,
   "question": "Which statement below is correct related to AdaBoost?",
   "A": "In the AdaBoost procedure, we choose each of the bags randomly with replacement.",
   "B": "Comparing to simple bagging, AdaBoost is less likely to result in over-fitting when we are using both of them to build models.",
   "C": "AdaBoost choose each of the bags by choosing only data points that did not fit well in the previous model generated by the previous learners.",
   "D": "AdaBoost is sensitive to noisy data and outliers, for it concentrates on those instances that are misclassified by previous learners. ",
   "answer": "D",
   "why": "A and C is wrong. In AdaBoost, we choose both randomly from the training data and from instances that did not fit well in previous learners.  B is wrong. AdaBoost is more likely to result in over-fitting because it concentrate on the data points that did not fit well.   D is correct. AdaBoost is sensitive to noisy data and outliers, for it concentrates on those instances that are misclassified by previous learners. If there is a major outlier, AdaBoost will be more likely trying to fit it."
 },
 {
   "index": 82,
   "question": "Supervised learning differs from unsupervised clustering in that supervised learning requires",
   "A": "at least one input attribute.  ",
   "B": "input attributes to be categorical.  ",
   "C": "at least one output attribute.  ",
   "D": "output attributes to be categorical.  ",
   "answer": "C",
   "why": "Supervised learning and unsupervised clustering both require at least one input attribute. But in supervised learning, the output datasets are provided which are used to train the  machine and get the desired outputs whereas in unsupervised learning no output datasets are provided, instead the data is clustered into different classes . "
 },
 {
   "index": 83,
   "question": "What is the best way of improving an overfitting model, and why?",
   "A": "Train it with the same parameters on a larger dataset because the additional data will increase the variance.",
   "B": "Train it with the same parameters on a smaller dataset because fewer examples will decrease the variance.",
   "C": "Train it with different parameters that capture the training data less well because  that would reduce the generalization error.",
   "D": "Train it with different parameters that capture the training data better because that would reduce the generalization error.",
   "answer": "C",
   "why": "The correct answer is c) because by fitting the training data less well, the model can focus on the most important features of the training data, which are the ones most likely to remain true for additional unseen data, while avoiding basing its predictions on unimportant, erroneous, or coincidental details present in the data used to build the model."
 },
 {
   "index": 84,
   "question": "What is the main difference between the approach of boosting and bagging when creating each weak learner?",
   "A": "Bagging creates the weak learners from a sample of the input space without replacement whereas boosting creates the weak learners from a sample of the input space with replacement.",
   "B": "Boosting creates its weak learners by testing the previous weak learner and weighting the input values that were incorrectly modeled by the preceding weak learners higher so that they are more likely to appear in the next weak learner. Bagging just uniformly selects random input values. ",
   "C": "Boosting uses multithreading so it is faster than bagging at creating the weak learners.",
   "D": "Boosting uses the same weak learner multiple times, so it is much faster and more accurate that bagging, which only uses a specific weak learner once.",
   "answer": "B",
   "why": "Bagging creates weak learners from uniform samples with replacement from the input space. Boosting checks the error rate of each weak learner and its preceding weak learners for values that are not well predicted. Those values are weighted higher when selecting input values for a weak learner so that they are better predicted in subsequent weak learners."
 },
 {
   "index": 85,
   "question": "Which of the following comparisons between Bagging and Boosting is true?  ",
   "A": "Bagging increases RMSE while boosting reduces it  ",
   "B": "Bagging is more likely than Boosting to cause overfitting as the number of bags approaches infinity",
   "C": "Boosting performs bootstrap sampling (with replacement) while Bagging does not  ",
   "D": "Boosting is more likely than Bagging to cause overfitting as the number of bags approaches infinity  ",
   "answer": "D",
   "why": "Boosting is more likely to cause overfitting as the number of bags approaches infinity because it takes into consideration past model prediction error when resampling, causing the RMSE in sample to reduce at the cost of overfitting.  The rest of the answers are blatantly false: both Bagging and Boosting reduce RMSE, and Bagging does performs bootstrap sampling. "
 },
 {
   "index": 86,
   "question": "Supervised learning is?",
   "A": "The machine learning task of inferring a function to describe hidden structure from unlabeled data.",
   "B": "The machine learning task of providing many examples of x and the correct answer y so that when it sees x again, it knows the answer y.",
   "C": "The machine learning task of providing multiple datasets as input and monitoring and selecting the ones that are optimal for to create a machine learning model.",
   "D": "Is the machine learning tasks of dynamically building a model using recursion that is provided a training and test data set.",
   "answer": "B",
   "why": "B is the true definition per the class content that correctly describe supervised learning. A is unsupervised learning. C is an answer that \"sounds reasonable\" and D is an answer that incorrectly extrapolates from MC3-P1"
 },
 {
   "index": 87,
   "question": "Which of the following statements about k-NN and linear regression is true?",
   "A": "(a) While both k-NN and linear regression are parametric models, only k-NN’s predicted values can be outside the range of y values in the training data.",
   "B": "(b) While both k-NN and linear regression are parametric models, only linear regression’s predicted values can be outside the range of y values in the training data.",
   "C": "(c) Both k-NN and linear regression are non-parametric models whose predictions cannot be outside the range of y values in the training data.",
   "D": "(d) Linear regression is a parametric model whose predicted values can be outside the range of y values in the training data while k-NN is a non-parametric model whose predicted values cannot be outside the range of y values in the training data.",
   "answer": "D",
   "why": "k-NN is a non-parametric model that takes the average of the y values of the k nearest neighbors to output a prediction. Therefore, it cannot extrapolate to values outside the observations’ range. On the other hand, linear regression is a parametric model whose prediction is based on a linear combination of parameters. Once a function is found, the model can evaluate any point in its domain and output a prediction that may be outside the range seen in the training data."
 },
 {
   "index": 88,
   "question": "Which of the following is true about Bagging (without boosting)?",
   "A": "Bagging reduces overfitting but introduces underfitting as the number of bags increases.",
   "B": "Bagging can only be applied to regression problems and not to classification problems because taking the mean of predicted values from each bag doesn't make sense for classification problems.",
   "C": "Bagging performs better if the elements are picked without replacement as compared to when they are picked with replacement. This is because 'without replacement' we avoid repetitions of elements in the bag and hence a better representation of the training set. ",
   "D": "None of the above.",
   "answer": "D",
   "why": "A is wrong because Bagging doesn't cause underfitting, it improves the performance. If anything, with the increase in number of bags to a very large number, it might lead to some overfitting.  B is wrong because in case of classification problems, we can always take the most frequent value instead of the mean.  C is wrong because 'without replacement' it'd be Running the Learner on the same set of data multiple times which defeats the whole concept of Bagging (Ensemble learners).  Hence D."
 },
 {
   "index": 89,
   "question": "Which one (select only one) of the following is NOT true about boosting?",
   "A": "Boosting is especially susceptible to noise",
   "B": "Boosting will eliminate overfitting ",
   "C": "Boosting has no parameters to tune",
   "D": "AdaBoost has ability to identify outliers",
   "answer": "B",
   "why": "A is true because boosting applies more weight on hardest examples, but those can be noise. Boosting is more likely to overfit when the number of bags increases than simple bagging. So B is not true. Boosting can be applied on any weak learner by applying more weight on the harder examples with larger errors, and has no parameter to tune. So C is true. D is true, because boosting applies more weight on hardest examples, and the examples with the highest weight often turn out to be outliers."
 },
 {
   "index": 90,
   "question": "Comparing a decision tree model using information gain to a decision tree model using randomized feature selection and splits with the same leaf size, the decision tree using information gain will likely require",
   "A": "less time to train and more time to predict than the random tree model.",
   "B": "more time to train and less time to predict than the random tree model.",
   "C": "more time to train and more time to predict than the random tree model.",
   "D": "equal time to train and predict as the random tree model.",
   "answer": "B",
   "why": "Correct answer is b) because the computational complexity of using information gain is greater than choosing random splits, resulting in greater training time. Trees using information gain tend to be shorter than random trees, resulting in reduced prediction time."
 },
 {
   "index": 91,
   "question": "Which one is true with respect regression problem  in machine learning?",
   "A": "We can use both unsupervised and supervised learning for regression problem.",
   "B": "We can use only supervised learning for regression problem.",
   "C": "Regression is used to predict continues values as well as discrete values",
   "D": "Regression is used to predict only discrete values",
   "answer": "B",
   "why": "Regression is always supervised learning, since we use set of input and output data to predict unseen data."
 },
 {
   "index": 92,
   "question": "Which is most likely to overfit?",
   "A": "Increment the leaf size in Random Decision Tree ",
   "B": "Increment the number of bags in simple Bagging",
   "C": "Increment the k value in KNN",
   "D": "Increment the number of bags in AdaBoost",
   "answer": "D",
   "why": "AdaBoost subsequently attempts to fit specific dataset poorly performed in previous bags over and over, which makes the prediction to be accurate for training dataset and causes overfitting. Therefore, incrementing the number of bags in AdaBoost is most likely to overfit."
 },
 {
   "index": 93,
   "question": "Between KNN, linear regression, and decision trees, which would be the best method in terms of prediction accuracy to use for extrapolating beyond the domain of a given data set?",
   "A": "KNN",
   "B": "Linear Regression",
   "C": "Decision Trees",
   "D": "All three would be equally effective.",
   "answer": "B",
   "why": "Of all the methods mentioned, only linear regression is a parametric model capable of extrapolation. KNN and decision trees simply use the edge values of the data set to predict the values beyond them."
 },
 {
   "index": 94,
   "question": "Task of inferring a model from labelled training data is called ",
   "A": "Unsupervised learning ",
   "B": "Supervised learning",
   "C": "Reinforcement learning ",
   "D": "Random optimization",
   "answer": "B",
   "why": "Labelled datasets are provided in supervised learning for training and testing the data to infer a predictive model."
 },
 {
   "index": 95,
   "question": "When building decision trees, instead of randomly choosing a feature index to split on, we can select the one with lowest node impurity. Given a split, let R1 and R2 be its left and right regions. For a region Ri (i=1,2), let Ni be the number of samples of this region, and let pi be the proportion of samples with label 1, then we compute Gini index of this region as Qi = 2*pi*(1-pi). When choosing a feature to split, we are minimizing the weighted impurity measure f = N1*Q1 + N2*Q2.    For an example dataset in the format of {(Label, F1, F2):(1,1,0),(1,0,1),(1,1,1),(1,1,0),(0,0,0),(0,0,1)}, the weighted impurity measure f of F1 is _______, and we should choose feature ____ as the root split.",
   "A": "4/3, F1",
   "B": "8/3, F1",
   "C": "4/3, F2",
   "D": "8/3, F2",
   "answer": "A",
   "why": "The weighted impurity measures of feature F1 and F2 are 4/3, 8/3 respectively. So we should choose F1 to split."
 },
 {
   "index": 96,
   "question": "In this class, we learn about overfitting. Which of the examples below do you think will likely contribute to/describe overfitting?",
   "A": "setting n=1 when using a K-Nearest neighbor model to perform classification",
   "B": "Fitting a 1 degree polynomial model to a dataset which data is sampled from the equation y=ax^3 +bx^2+cx+d, plus some gaussian noise.",
   "C": "Fitting a 5 degree polynomial model to a dataset which data is sampled from the equation y=mx+c, plus some gaussian noise . ",
   "D": "A and C ",
   "answer": "D",
   "why": "A is correct. Setting N=1 for KNN learners will generally lead to overfitting, because training error will always be 0 as existing samples will always be matched to themselves, but will not scale well to unseen data.     B is wrong. The model used in the learner  is too simple to represent the underlying data, and will generally lead to high error, even if more data is used to learn the model. This phenomenon is called underfitting, instead of overfitting.    C is correct.  Fitting a complicated (5 degree polynomial) model to data generated from a simple 1 degree polynomial will lead to, like A, good in-sample error but poor out-sample error. n-folds Cross validation can be used with existing data to verify this.     D is the correct answer, since both A and C are correct."
 },
 {
   "index": 97,
   "question": "Which of the following ways, when used to select feature to split on in creating a decision tree, is going to create the biggest tree, provided leaf size is fixed?",
   "A": "Use entropy",
   "B": "Use correlation and prefer the feature with its absolute value closest to 1",
   "C": "Randomly pick a feature",
   "D": "Use Gini index",
   "answer": "C",
   "why": "The answer is C because randomly selecting a feature to split on has the least information gain. Thus, each split at random won't be as effective as other methods, causing the produced tree to be bigger."
 },
 {
   "index": 98,
   "question": "Bagging leads to improvements in machine learning procedures by reducing the variance in data. This is least true for : ",
   "A": "Linear Regression",
   "B": "Regression in Random Forest",
   "C": "K-Nearest Neighbors",
   "D": "Decision Tree Classification",
   "answer": "C",
   "why": "Bagging can mildly degrade the performance of stable methods such as K-nearest neighbors"
 },
 {
   "index": 99,
   "question": "You are working for an online video service. Your boss wants you to recommend a simple algorithm for generating video recommendations. New videos are added all the time so training time needs to be kept to a minimum. Which of the following would be the best choice, with the correct reasoning?",
   "A": "Decision Tree. When new videos are added only relevant parts of the tree are updated.",
   "B": "K-Nearest-Neighbor. The training time is zero, as all calculation is done upon query.",
   "C": "Decision Tree. Re-training time is low due to back propagating new data points.",
   "D": "K-Nearest-Neighbor. Old nearest neighbors are stored for later when new videos are added.",
   "answer": "B",
   "why": "The question asks for the best algorithm with regards to training time, with the correct reasoning. The answer is B) KNN, as there is no 'training' required."
 },
 {
   "index": 100,
   "question": "What is the one of the main benefits/goals of utilizing an ensemble learner made by bagging with multiple KNN learners over using a single KNN learner with the same k value?",
   "A": "Reduce the query time by using a combination of faster learning algorithms",
   "B": "Reduce the error for predictions on out-of-sample data by reducing the bias inherent to a single learner",
   "C": "Reduce the training time by using a combination of less complex learning algorithms ",
   "D": "Reduce overfitting on out-of-sample data by reducing the bias inherent to a single learner",
   "answer": "B",
   "why": "Answer B is correct as the bias in each of the individual learners will be smoothed out when aggregated. Reduced testing error is one of the main reasons bagging is utilized. Answer A is incorrect as the query time for the KNN ensemble learner will be greater due to the need to query multiple learners. Answer C is incorrect as the training time for KNN algorithms is negligible.  Answer D is incorrect because the goal is to reduce overfitting of the in-sample, or training data. "
 },
 {
   "index": 101,
   "question": "In the bagging, how to combine the results from querying different learners to generate the output value?",
   "A": "Take medium",
   "B": "Take mean",
   "C": "Use additional learner",
   "D": "Randomly choose",
   "answer": "B",
   "why": "according to our lecture, we should take the mean of the output from all the learners to generate the predicted value"
 },
 {
   "index": 102,
   "question": "Typically classification problems involve sorting elements into categories. How could a regression problem be turned into a classification problem? ",
   "A": "Regression problems cannot be turned into classification problems. ",
   "B": "Use ranges of values to setup buckets that the values can be sorted into. ",
   "C": "Create a category for every possible answer. ",
   "D": "Sort all values into a single category. ",
   "answer": "B",
   "why": "Creating a category for every single answer is impossible for a regression problem, there is an uncountably infinite number of possibilities. However values can be sorted into ranges. "
 },
 {
   "index": 103,
   "question": "Unsupervised learning is different from supervised learning because unsupervised learning is good at...",
   "A": "It provides good generalization.",
   "B": "Utilizing the training set to determine good predictions",
   "C": "Clustering data points into categories.",
   "D": "Unsupervised learning is faster and more accurate at predicting than supervised learning",
   "answer": "C",
   "why": "With little to no training sets available, unsupervised learning does not require perfect training set. Unsupervised learning is good at clustering significance and labeling data points even if the clusters are small."
 },
 {
   "index": 104,
   "question": "For which of the following applications would an instance-based model have the greatest benefits over a parameterized model? An application whose priorities are…",
   "A": "Query speed & Extrapolation",
   "B": "Speed of training model & Ease of Adding new data",
   "C": "Low space requirements for model & Ease of Adding new data",
   "D": "Biased model & Speed of training model",
   "answer": "B",
   "why": "Instance based models such as KNN do not take very long to train a model and adding new data just requires the addition of new data points to the existing model. Querying speed is an advantage of parameterized models, so answer A is incorrect. Instance-based models have much larger space requirements than parameterized models so Answer C is incorrect and they are unbiased so answer D is not a good choice either."
 },
 {
   "index": 105,
   "question": "In which case would a parametric model be preferable to an instance based model?",
   "A": "Number of car crashes as speed increases",
   "B": "skyscraper weight as floors increase",
   "C": "number of times people eat at restaurants as income increases",
   "D": "Number of hours of daylight for a given day as latitude increases ",
   "answer": "D",
   "why": "D is really the only answer that can be represented mathematically and therefore lends itself to a parameterized model. We don't know that relationships exist between any of the other comparisons. A car can certainly drive fast and not get in a car crash and vice versa. A skyscraper will definitely increase in weight as floors increase but there is no mathematical formula as a lot depends on construction and size and use of the building. Restaurant visits by income definitely doe not have a mathematical relationship and should be used with an instance based method. "
 },
 {
   "index": 106,
   "question": "Supervised learning differs from unsupervised clustering in that supervised learning requires",
   "A": "at least one input attribute.",
   "B": "input attributes to be categorical.  ",
   "C": "at least one output attribute.",
   "D": "output attributes to be categorical.",
   "answer": "C",
   "why": "Supervised learning and unsupervised clustering both require at least one input attribute. But in supervised learning, the output datasets are provided which are used to train the machine and get the desired outputs whereas in unsupervised learning no output datasets are provided, instead the data is clustered into different classes."
 },
 {
   "index": 107,
   "question": "Which of the following statement is false?  ",
   "A": "A strong learner is defined to be a classifier which is only slightly correlated with the true classification  ",
   "B": "Boosting convert strong learner to weak learner  ",
   "C": "Both A and B are correct  ",
   "D": "Both A and B are wrong  ",
   "answer": "D",
   "why": "A. A weak learner is defined to be a classifier which is only slightly correlated with the true classification  B.Boosting convert weak learner to strong learner  "
 },
 {
   "index": 108,
   "question": "We can identify overfitting by plotting in sample and out of sample model error against model complexity. Overfitting is characterized by portions of the graph with:",
   "A": "In sample error increasing and out of sample error decreasing",
   "B": "In sample error decreasing and out of sample error decreasing",
   "C": "In sample error increasing and out of sample error increasing",
   "D": "In sample error decreasing and out of sample error increasing",
   "answer": "D",
   "why": "This is the definition of overfitting given in the lecture. The test taker should either be able to remember this or reason it from the ML project. If in sample error is decreasing while out of sample is increasing it indicates that the model is fitting the training set well, but not generalizing to the test data."
 },
 {
   "index": 109,
   "question": "Which of the following problem can be addressed by using proper classification algorithm?",
   "A": "You want to fit a line between the size of a house and the price of the house in order to predict house price once the size of the house is known. ",
   "B": "You want to recognize human written digits, which is from 0 to 9, based on training examples of human written digits. ",
   "C": "You are asked to construct an optimal portfolio by maximizing its sharp ratio given specific stock tickers. ",
   "D": "You are asked to evaluate the quantitative influence that body weight has on the blood cholesterol level",
   "answer": "B",
   "why": "Because this problem is a multi classification problem with 10 classes, which is 0 to 9. One can train the classification model and predict the written digits to be one of {0,1, ... 9}. If people doesn't know this, he can correctly get the answer by simply observing that only the predicted value of B takes discrete value."
 },
 {
   "index": 110,
   "question": "How is a random tree built using A. Cutler's approach different than a standard decision tree using JR Quinlan's approach?",
   "A": "Building a random tree entails randomizing the association between each input test point and the label associated with it in the training set whereas for a standard tree, each input point and its label remains intact.",
   "B": "It is much more difficult to build a random tree that learns quickly than to build a deterministic tree that learns quickly using JR Quinlan's approach.",
   "C": "When determining where to split, in the random tree approach of Cutler a factor is randomly chosen whereas in Quinlan's approach a method  using a measure of information gain such as entropy is used to determine the best factors to split on.",
   "D": "In a decision tree built by Quinlan's approach factors can be repeated but in the random tree generated by Cutler's method a factor can only appear once.",
   "answer": "C",
   "why": "A.  Randomization occurs in selecting factors and split values, not in randomly scrambling the input data, so this is false.  B.  It is much easier to build a random tree because meaures of information gain do not have to be studied and implemented.  So this is false.  C.  Correct answer.  Random trees built by Cutler's method are fast learners due selecting the factors randomly and their split values randomly as well, whereas the standard tree relies on measures of information gain to select the best factor.  These measures are more complex to build, analyze and are more computationally demanding.  D.  Factors can appear multiple times in either algorithm and so this is false."
 },
 {
   "index": 111,
   "question": "Which of the following statements is FALSE with regards to supervised and unsupervised learning algorithms?  ",
   "A": "Supervised learning algorithms require a labeled dataset.",
   "B": "Unsupervised learning algorithms cannot be used with a labeled dataset.   ",
   "C": "Decision Trees and KNN are examples of supervised learning algorithms.    ",
   "D": "Both supervised and unsupervised learning algorithms require a labeled dataset.      ",
   "answer": "D",
   "why": "D.) Correct, because the whole idea behind unsupervised learning is that labeled data is not required.  They attempt to cluster the data into a structure to make sense of it.    A.) is wrong because that is the idea behind a supervised learner, you give it labeled data and attempt to perform classification or regression    B.) is wrong because you can just not pass in the label portion of the dataset into the algorithm or you can pass it in and allow it to use that as a feature to cluster on.    C.) is wrong because lesson \"03-01 how machine learning is used at a hedge fund\" slide 4 lists these as supervised learning algorithms."
 },
 {
   "index": 112,
   "question": "Which of the followings regarding supervised learning and unsupervised learning is correct?",
   "A": "In unsupervised learning, you know the desired class or value of each input.",
   "B": "Linear regression is a type of unsupervised learning algorithms.",
   "C": "In supervised learning, the output datasets are provided which are used to train the machine and get the desired outputs",
   "D": "K-means clustering is a type of supervised learning algorithms. ",
   "answer": "C",
   "why": "In supervised learning we know what the output should look like and there is a relationship between the input and the output"
 },
 {
   "index": 113,
   "question": "Which of the following machine learning algorithms is faster than the rest?",
   "A": "K-Nearest Neighbours",
   "B": "Linear Regression",
   "C": "Decision Trees",
   "D": "Not enough data to decide",
   "answer": "D",
   "why": "Deciding which one is the fastest depends on whether we are looking at training time or testing time. "
 },
 {
   "index": 114,
   "question": "What best describes a key benefit of parameterized models?  ",
   "A": "Parameterized models are fast at performing learning.  ",
   "B": "Parameterized models are suited for simpler problems.  ",
   "C": "Parameterized models overfit the training data.  ",
   "D": "Parameterized models are a lot slower to train.  ",
   "answer": "A",
   "why": "Learning is fast because parameterized models only are required to estimate the parameters of the function.  "
 },
 {
   "index": 115,
   "question": "When building a decision tree, which method will result in the GREATEST impact on the results produced from querying the final tree?",
   "A": "Using Gini index as the information gain metric.",
   "B": "Using entropy as the information gain metric.",
   "C": "Using correlation as the information gain metric.",
   "D": "Employing pruning after the decision tree is constructed.",
   "answer": "D",
   "why": "Most measures of information gain, including the three mentioned in A, B, and C, are very consistent with one another. As such, the metric used for information gain will have very little effect relative to pruning the tree after constructing it."
 },
 {
   "index": 116,
   "question": "What technique is often used to avoid overfitting when working with a machine learning algorithm?",
   "A": "Cross validation",
   "B": "Use a lot of data",
   "C": "Regularization",
   "D": "All of the above",
   "answer": "D",
   "why": "Regularization introduces additional information to the program that can prevent overfitting when successful. Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. Obtaining more training data also helps solve overfitting."
 },
 {
   "index": 117,
   "question": "Out of K-Nearest Neighbor, linear regression and decision tree, which model has the least query time?",
   "A": "K-Nearest Neighbor ",
   "B": "Linear Regression",
   "C": "Decision Tree",
   "D": "All 3 have same query time",
   "answer": "B",
   "why": "Linear Regression model has least query time since it only calculates output from known parameters and equation. Decision tree takes longer to query since it must traverse the full tree until it reaches a leaf. KNN must calculate the distance from each data point and takes longer at query time than linear regression."
 },
 {
   "index": 118,
   "question": "The Root Mean Square Error equation compute the sum of square difference of which data points?",
   "A": "Xtest – Xtest",
   "B": "Ytest – Ypredict",
   "C": "Xtest - Ypredict ",
   "D": "Ytest  - Xpredict ",
   "answer": "B",
   "why": "RMSE equation is the square root of the sum different of Y test value and Y predicted value square divide by n.  RMSE = sqrt(mean((y-y_pred)**2));"
 },
 {
   "index": 119,
   "question": "Which of the following statements is correct with regards to overfitting?",
   "A": "For a regression learner, overfitting occurs as the number of degree polynomial decreases",
   "B": "Overfitting is said to happen when the in-sample error increases and the out-of-sample error decreases. ",
   "C": "In a KNN learner, overfitting can be reduced by increasing the value of k",
   "D": "In a perfect random tree learner, overfitting can be eliminated by setting the leaf_size = 1",
   "answer": "C",
   "why": "the closer the value of k gets to 1, the in-sample error decreases as the model fits the data more or less perfectly. Where as the out of sample error increases by not being able to fit the data as well."
 },
 {
   "index": 120,
   "question": "Which of the following statements is true about decision tree?",
   "A": "Decreasing the leaf size may make the model more resistant to overfitting.",
   "B": "Implementing boost aggregation over multiple trees does not increase the size of the model.",
   "C": "Decision trees can be used to solve classification problems.",
   "D": "Decision trees with bigger heights are better than a decision trees with smaller heights.",
   "answer": "C",
   "why": "C is correct, decision trees can be used for either regression or classification problems, with small differences in training and aggregating result.    A is plainly wrong; for B, the model with bagging contains multiple trees and would take more memory space; D is a bit tricky, but we want the tree to be as balanced as possible."
 },
 {
   "index": 121,
   "question": "We are training a parametric regression model and ending up with many learners with different degrees of freedom (d). We then test these learners and plot the in sample errors (training data) and out of sample error (testing data) vs degrees of freedom (0~d) to detect overfitting, which of the following statements indicates overfitting?",
   "A": "Both the in sample error and out of sample error are decreasing while d is increasing",
   "B": "Both the in sample error and out of sample error are increasing while d is increasing ",
   "C": "When the in sample error is decreasing, the out of sample error is increasing",
   "D": "When the out of sample is decreasing, the in sample error is increasing",
   "answer": "C",
   "why": "When the d increases, the model fits the training sample better and better and thus the in sample error always goes down. When the in sample error is decreasing and the out of sample error is increasing, the overfitting occurs."
 },
 {
   "index": 122,
   "question": "The most likely reason for overfitting in a kNN classifier can be",
   "A": "Using low value of k .(It means considering less neighboring points)",
   "B": "Using high value of k .(It means considering more neighboring points)",
   "C": "Using Euclidean distance as a distance metric to measure distance between two points",
   "D": "Using Manhattan distance as a distance metric to measure distance between two points.",
   "answer": "A",
   "why": "The most likely reason for overfitting in a kNN classifier is choosing small values of K . For instance when k = 1 , the model might fit training data quite perfectly and in sample error will be quite low , but it might not generalize well , leading to high out of sample error . As we consider more values of k , it helps us to build a more generalized model ,reducing overfitting."
 },
 {
   "index": 123,
   "question": "You're part timing it for a Canadian investment firm.  An algorithm that you developed in the states can predict tomorrows stock prices with an RMSE of 5 US dollars.  The Canadians have a similar algorithm, but it works in Canadian dollars.  What RMSE value would indicate the Canadians algorithm is equivalent to your own, given the exchange rate is one US dollar to 'X' Canadian dollars?",
   "A": "5 / X",
   "B": "5 * (X^2)",
   "C": "5 * X",
   "D": "5 / (X^2)",
   "answer": "C",
   "why": "My topic was: measuring the quality of predictions using RMSE, correlation, other?    (X^2) indicates X squared, or X * X.    This question is supposed to illustrate that RMSE scales linearly with the magnitude of the input.  MSE squares quadratically, but RMSE scales linearly."
 },
 {
   "index": 124,
   "question": "We train a model with low and high temperature data for every day from 2010-2015 in Neverland. The temperatures in Neverland vary smoothly for the most part, increasing steadily from January to August and then falling till December. But every year, from a period of 15th to 20th of March, Neverland experiences a cold streak that results in temperatures being around 20 degrees lower than normal. Choose the option that is correct for a kNN and a Parametric Regression learner from the following. (Assume k<10 for the kNN.)",
   "A": "kNN will not predict the cold streak, Parametric Regression will predict the cold streak.",
   "B": "kNN will predict the cold streak, Parametric Regression will not predict the cold streak.",
   "C": "Both kNN and Parametric Regression fail to predict the cold streak.",
   "D": "Both kNN and Parametric Regression will predict the cold streak.",
   "answer": "B",
   "why": " kNN is more sensitive to local changes in the data. Parametric Regression will not be affected a lot by this dip for this small sample (5:365), but will fit try to a curve to the overall data. kNN when queried with this date, will use the outlier data for the cold streak found in the training data rather than bothering about the entire training data. So kNN will be able to predict the cold streak, while Parametric Regression will not be predict it."
 },
 {
   "index": 125,
   "question": "We train a model with low and high temperature data for every day from 2010-2015 in Neverland. The temperatures in Neverland vary smoothly for the most part, increasing steadily from January to August and then falling till December. But every year, from a period of 15th to 20th of March, Neverland experiences a cold streak that results in temperatures being around 20 degrees lower than normal. Choose the option that is correct for a kNN and a Parametric Regression learner from the following. (Assume k<10)",
   "A": " kNN will not predict the cold streak, Parametric Regression will predict the cold streak",
   "B": "kNN will predict the cold streak, Parametric Regression will not predict the cold streak",
   "C": "Both kNN and Parametric Regression fail to predict the cold streak.",
   "D": "Both kNN and Parametric Regression will predict the cold streak.",
   "answer": "B",
   "why": "kNN is more sensitive to local changes in the data. Parametric Regression will not be affected a lot by this dip for this small sample (5:365), but will fit try to a curve to the overall data. kNN when queried with this date, will use the outlier data for the cold streak found in the training data rather than bothering about the entire training data. So kNN will be able to predict the cold streak, while Parametric Regression will not be predict it."
 },
 {
   "index": 126,
   "question": "Under what circumstances would you prefer decision trees over K-NN?",
   "A": "When the query time should be very less",
   "B": "When the training time should be very less",
   "C": "When you need a model that can be updated easily",
   "D": "When the problem uses classification as opposed to regression  ",
   "answer": "A",
   "why": "K-NN takes no time to train, but very long to query. K-NN can be updated easily as it can learn incrementally due to its instance-based learning paradigm. Both algorithms are used for classification so that is not a deciding factor."
 },
 {
   "index": 127,
   "question": "Which strategy has a chance protect a supervised learning algorithm from damage due to some rows being contaminated by a deliberately hostile supervisory signal?",
   "A": "Increase the number of available features to the decision tree.",
   "B": "Do a clustering analysis and remove outliers in the training data.",
   "C": "Ensamble many different classifier systems and average their result, correct each in proportion to their contribution.  ",
   "D": "Introduce noise and random training rows to the training data to exercise the supervised learning system from damage.",
   "answer": "C",
   "why": "Adding features to the decision tree does not protect against damage because the target data is still wrong.    Hostile supervisory signal would not be identified in a clustering analysis, perfectly good outlier data would be removed.    Training a supervised learning algorithm on random noise rows would not decrease damage, it would increase damage.      Ensambling many supervised learning classifiers to work together like a democracy and vote on where the exact target is and averaging their response, correcting all algorithms in proportion to their stake in the vote would be an effective resistance to a hostile supervisory signal.  An attacker may be able to stage an effective attack, heavily damaging one of the algorithms, but that the vote of the damaged model be washed out by the ineffectiveness of the attack on all the others.  "
 },
 {
   "index": 128,
   "question": "Which of the following methods would you choose in order to minimize the running time for a single decision tree? ",
   "A": "Choose the split feature base on entropy",
   "B": "Choose the split feature base on correlation",
   "C": "Choose the split feature base on random draw",
   "D": "Choose the split feature base on Gini Index",
   "answer": "C",
   "why": "because the other three methods all need to calculate information gain before being able to decide which feature to split."
 },
 {
   "index": 129,
   "question": "Why is using cross-validation more reliable in determining overfitting than doing a simple dataset split (test and train)?",
   "A": "You could be unlucky in selecting your test set, and see a bias on the true accuracy. Cross-validation will smooth this bias out.",
   "B": "Cross-validation is slower because you have to train repeatedly on your splits. Dataset splits are faster.",
   "C": "For quantitative finance, the structure of the data can change over time. Cross-validation will help you to avoid fitting new predictions when the structure has changed.",
   "D": "Dataset split uses the data more efficiently, so you don’t lose too much data for modeling.",
   "answer": "A",
   "why": "A) Because you do multiple complementary sets, you are going to reduce variability. Thus, the results that you see from cross-validation are going to be truer to the real accuracy of your model. Cross-validation gets around the problem of being unlucky and choosing a biased data set. B is true but have nothing to do with reliability. For C, the structure can change, but cross-validation will not help you with this. D is false—cross-validation actually uses the data more efficiently because you have multiple splits for your training and test sets."
 },
 {
   "index": 130,
   "question": "Given a large set of data for training and knowing you will have missing and malformed data that is not in your training data sets what is the best option to implement a learning algorithm and why?",
   "A": "kNN, the low cost of learning will compensate for number and distance of data points.",
   "B": "Decision tree, it will not have to normalize data and can easily handle missing data over a large data set.",
   "C": "Linear Regression, the low query cost will compensate for the training over a large data set.",
   "D": "All of the above. All of the above methods can handle the data equally well.",
   "answer": "B",
   "why": "Trees don't have to normalize data and can easily handle missing data. kNN and Linear Regression do not. kNN will slow down considerable depending on distance and number of neighbors."
 },
 {
   "index": 131,
   "question": "Can learning algorithms produce different predictions on a given test set but with the same RMSE of the predictions (up to 4 decimal points)?",
   "A": "No, different predictions will always have different values of RMSE",
   "B": "Yes, but only if RMSE = 0",
   "C": "Yes, but only if the underlying process has predictable nature",
   "D": "Yes, there can be different predictions generated by learning algorithms that have the same RMSE",
   "answer": "D",
   "why": "RMSE is a measure of error. There can be more than one set of predictions that have the same value of RMSE (up to certain decimal point), given non-zero RMSE. Randomized learning algorithms can potentially produce such results given adequate number of training."
 },
 {
   "index": 132,
   "question": "Which of the following statements are true:   1. Quinlan Trees select feature which has lowest correlation with output values  2. Cutler Trees uses correlation/information gain to select feature to split on",
   "A": "Statement 1",
   "B": "Statement 2",
   "C": "Both",
   "D": "None",
   "answer": "D",
   "why": "Cutler trees are random trees and choose features to split on randomly, whereas Quinlan trees select feature with highest correlation/information gain."
 },
 {
   "index": 133,
   "question": "For a number N of training samples, which of the following is faster to train?",
   "A": "KNN where K = 1 trained over all the data",
   "B": "linear regression where d = 3",
   "C": "decision tree with leaf size 1",
   "D": "decision tree with leaf size = N",
   "answer": "A",
   "why": "Doesn't really matter what K is, KNN is the fastest in training because all it does is store the samples in memory. No computation, nothing else going on. The slowest would probably be decision trees of leaf size 1 because it takes more recursive steps to get to 1 observation."
 },
 {
   "index": 134,
   "question": "Which is not an acceptable way to alleviate overfitting?",
   "A": "Create an ensemble learner consisting of multiple different types of learners(knn, decision tree, lin  regression) and average their results.  ",
   "B": "  Create an ensemble learner of a single type using a technique called bagging and then averaging their results.",
   "C": "Both of these are acceptable ways to deal with overfitting  ",
   "D": "Neither, to avoid overfitting we must adjust the degrees of freedom within our model",
   "answer": "C",
   "why": "A is correct because different types of models can develop bias in different ways, these will be less pronounced if we average many different types of learners. B is also correct because bagging eliminates the severity of the influence of outliers or faulty data. Therefore C is correct."
 },
 {
   "index": 135,
   "question": "In decision tree, linear regression and KNN, which is the least costing at learning and which is the least costing at query?",
   "A": "decision tree, decision tree",
   "B": "KNN, linear regression",
   "C": "KNN, decision tree",
   "D": "linear regression, linear regression",
   "answer": "B",
   "why": "The cost at learning is O(n) for linear regression and O(nlogn) for decision tree and O(1) for KNN, so the least cost at learning is KNN.  The cost at query is O(1) for linear regression and O(logn) for decision tree and O(n) for KNN, so the least cost at query is linear regression."
 },
 {
   "index": 136,
   "question": "Given a data set containing information about oranges with the following header: size, date of production, color and quality, you are asked to train a machine learning model so that you could estimate how good an orange is given its size, date and production. What kind of machine learning approach should you choose?",
   "A": "Supervised Learning",
   "B": "Unsupervised Learning",
   "C": "Reinforcement Learning",
   "D": "Orange Learning",
   "answer": "A",
   "why": "This is a typical classification problem that Supervised Learning is great for."
 },
 {
   "index": 137,
   "question": "Select the correct answer with regard to the concept of bagging:",
   "A": "In order to avoid bias, bagging generates m new datasets of n' observations from the original data set without replacement, where n' < n, and n is the number of examples in the original dataset.  ",
   "B": "Bagging helps to avoid overfitting, but increases variance since bags are filled using replacement.  ",
   "C": "Bagging reduces variances but increments the chances of overfitting.  ",
   "D": "One of the disadvantages of bagging is its computational complexity, since it multiplies the work of generating one tree by m, where m is the number of bags to use.",
   "answer": "D",
   "why": "b and c are incorrect. Bagging helps to reduce overfitting and decreases variance. a is incorrect since bagging uses replacement to select the samples that go in each bag, and n' is usually equals to n.   The correct answer is D, if we increment m, then we will be increasing the number of models to be built during the bagging process. "
 },
 {
   "index": 138,
   "question": "For KNN algorithm, when does overfitting take place?",
   "A": "both in sample and out of sample error increases",
   "B": "both in sample and out of sample error decreases",
   "C": "in sample error decreases while out of sample error increases",
   "D": "in sample error increases while out of sample error decreases",
   "answer": "C",
   "why": "Professors's video lectures on learning provides the following definition on overfitting: overfitting occurs when in sample error decreases while out of sample error increases"
 },
 {
   "index": 139,
   "question": "Which of the following is an incorrect statement about bagging?",
   "A": "Bagging reduces the variance and helps avoid overfitting.",
   "B": "Bagging requires the sampling uniformly with replacement.",
   "C": "Bootstrapping is an approach to ensemble learning that is based on bagging.",
   "D": "Bagging can be used with any type of model for classification or regression.",
   "answer": "C",
   "why": "It's the other way around, \"Bagging is an approach to ensemble learning that is based on bootstrapping.\" ."
 },
 {
   "index": 140,
   "question": "Which of these models would be a simple yet effective predictor of the time it would take for a car in a controlled test to come to a complete stop given different speeds when the brakes were initially applied?",
   "A": "3-NN",
   "B": "quadratic",
   "C": "1-NN",
   "D": "linear",
   "answer": "C",
   "why": "This is a simple scenario with extremely predictable behavior based on only a few factors. There would be a high correlation between how fast the car is moving and how long it would take to stop and a very low correlation with other factors."
 },
 {
   "index": 141,
   "question": "Bagging works especially well for unstable predictive algorithms - algorithms whose output undergoes major changes in response to small changes in the training data. For which of these predictive algorithms Bagging may not give substantial gains in accuracy?",
   "A": "Classification Trees",
   "B": "Regression Trees",
   "C": "k-Nearest Neighbors",
   "D": "Nearest Neighbor",
   "answer": "C",
   "why": "Bagging methods rely on the instability of the classifiers to improve their performance, as k-NN is  fairly stable(for large enough k) with respect to resampling, bagging may fail in its attempt to improve the performance significantly for k-NN classifier."
 },
 {
   "index": 142,
   "question": "Which of the following can techniques can minimize overfitting when you have a small dataset?",
   "A": "Overfitting is not common in small datasets",
   "B": "Use cross validation with slicing",
   "C": "Increase the dataset by adding the result of the test data to the pool of data",
   "D": "Continue to train the model so that the model matches the train data as much as possible to produce minimal error",
   "answer": "B",
   "why": "Overfitting happens when a model contains errors and noise from the training data. Because of the small nature of the dataset, there is not enough data to effectively analyze the data and thus errors and noise will be highly evident. By using roll cross validation with slicing, more data can be effectively created."
 },
 {
   "index": 143,
   "question": "Which of the following three models, kNN, decision trees, and linear regression, can be guaranteed to achieve perfect in-sample prediction accuracy (assuming there are no one-to-many relationships between the X to Y values)?",
   "A": "    kNN, if we set k (the number of nearest neighbors to use), to equal the number of training observations.  ",
   "B": "Decision Trees, if we set the leaf size (the maximum number of observations to be used at a leaf) to equal the number of training observations",
   "C": "    Linear Regression, if we use a second order model to capture any curvature of the data  ",
   "D": "    Both Knn and Decision Trees if we set k =1 and leaf_size = 1  ",
   "answer": "D",
   "why": "For Knn with k=1, the closest observation to every in-sample query point will be the point itself, so it will output the same value. For decision trees with leaf_size =1, the tree will be expanded until it perfectly fits the in-sample data and every in-sample query point will be represented in the tree. Choice A is wrong because k=n (n= training observation) will always output the mean, however if the student is not familiar with the algorithm this answer may be appealing.  Choice B is wrong since increasing the leaf size usually increases in-sample error. Choice C is wrong because a second order linear regression is only guaranteed to fit the data perfectly if there are three observations. It is possible for a linear regression to achieve perfect in-sample accuracy, but only if we include a degree n-1 polynomial so students who do not understand linear regressions may choose choice C."
 },
 {
   "index": 144,
   "question": "For the Random Tree Algorithm in A Cutler's paper, comparing to the traditional Decision Tree Algorithm proposed by JR Quinlan, which of the following best describes the keyword \"random\":",
   "A": "At each level, the feature i to split on is chosen randomly, and the split value is calculated by computing the median value of randomly selected feature i, in order to divide the dataset equally.",
   "B": "At each level, the feature i to split on is chosen based on the correlation of each feature and Y values, and the split value is determined randomly by picking two row randomly and computing the mean for Xi.",
   "C": "At each level, the feature i to split on is chosen randomly, and the split value is also determined randomly by picking two row randomly and computing the mean for selected two Xi.",
   "D": "At each level, the feature i to split on is chosen randomly, and the split value is calculated by randomly selecting half of rows and computing the mean of selected Xi.",
   "answer": "C",
   "why": "C is the right answer, because in A cutler's random tree algorithm, the randomness contains two part: randomly selecting feature i and randomly picking up two Xi and take the mean. A is wrong, because calculating the median is a little bit slower, which is different from A Cutler's purpose. (but I think A is the most attractive answer other than correct one since it provide the reason of using median value). B is wrong because compute correlation for all feature is expensive. D is wrong because in this algorithm only two values of Xi are selected."
 },
 {
   "index": 145,
   "question": "Professor Balch has highlighted that the decision tree learner you have submitted is overfitting.  This is MOST likely because:",
   "A": "root mean squared error and correlation for the training set is high while the testing set is low",
   "B": "root mean squared error is high and correlation is low for the training set, while the testing set has low RMSE and high correlation",
   "C": "root mean squared error is low and correlation is high for the training set, while the testing set has high RMSE and low correlation",
   "D": "root mean squared error and correlation for the training set is low while the testing set is high",
   "answer": "C",
   "why": "Typically once the training set continues decreasing RMSE and the testing set begins having higher RMSE then your model is overfitting.  The exact opposite is the case for Correlation, increasing correlation for training set mixed with decreasing correlation for the testing set typically also signals overfitting."
 },
 {
   "index": 146,
   "question": "Order the algorithm starting from fastest to slowest for querying",
   "A": "knn algorithm, linear regression, decision tree",
   "B": "linear regression, knn algorithm, decision tree",
   "C": "linear regression, decision tree, knn algorithm",
   "D": "decision tree, linear regression, knn algorithm",
   "answer": "C",
   "why": "Linear regression you just need to do multipllication  Decision tree you only need to traverse the tree (log(n))  KNN you need to compute every distance of dataset to query"
 },
 {
   "index": 147,
   "question": "We want to apply bagging method to improve the performance of a decision tree classifier. Please select the statement that bagging fits:",
   "A": "The algorithm can be parallelized directly. ",
   "B": "There will be over-fitting when the data is extremely noisy. ",
   "C": "The algorithm is guaranteed to reduce the upper error bound to fit the training data. ",
   "D": "It is faster than training a single decision tree classifier.",
   "answer": "A",
   "why": "Because each model is built independently. "
 },
 {
   "index": 148,
   "question": "For the Random Tree Algorithm in A Cutler's paper, comparing to the traditional Decision Tree Algorithm proposed by JR Quinlan, which of the following best describes the keyword \"random\":",
   "A": "At each level, the feature i to split on is chosen randomly, and the split value is calculated by computing the median value of randomly selected feature i, in order to divide the dataset equally.",
   "B": "At each level, the feature i to split on is chosen based on the correlation of each feature and Y values, and the split value is determined randomly by picking two row randomly and computing the mean for Xi.",
   "C": "At each level, the feature i to split on is chosen randomly, and the split value is also determined randomly by picking two row randomly and computing the mean for Xi. ",
   "D": "At each level, the feature i to split on is chosen randomly, and the split value is calculated by randomly selecting half of rows and computing the mean of selected Xi.",
   "answer": "C",
   "why": "C is the right answer, because in A cutler's random tree algorithm, the randomness contains two part: randomly selecting feature i and randomly picking up two Xi and take the mean. A is wrong, because calculating the median is a little bit slower, which is different from A Cutler's purpose. (but I think A is the most attractive answer other than correct one since it provide the reason of using median value). B is wrong because compute correlation for all feature is expensive. D is wrong because in this algorithm only two values of Xi are selected. "
 },
 {
   "index": 149,
   "question": "You have a data set with 1000s of features (X) but the end result for each row (y) can be determined by the first 3 features and the rest of the features are useless.    Which technique is ideal to learn from such a data set?",
   "A": "Linear regression because it is not susceptible to noise and it can fit a line to any kind of data.",
   "B": "Regression with higher order polynomials. Features which are less useful get assigned lower coefficients and hence get filtered out while doing the prediction.",
   "C": "kNN because the useless features get filtered out when calculating distance between the points.",
   "D": "Decision trees because it considers the information gain of each feature when selecting the node.",
   "answer": "D",
   "why": "Decision trees start with calculating information gain of each column. The feature with highest info gain will get selected as the root node, the feature with second highest gain will get selected as the next node and so on. The least contributing features will have very less or 0 information gain. With an appropriate threshold value for the information gain, we can avoid selection of less useful columns when constructing the tree."
 },
 {
   "index": 150,
   "question": "Which of the following CANNOT reduce or eliminate overfitting?",
   "A": "Increase the number of bags in bagging",
   "B": "Increase leaf size",
   "C": "Decrease the amount of training data examples",
   "D": "Implement Bootstrap Aggregating",
   "answer": "C",
   "why": "Learned from MC3P1, A, B and D can improve the model’s performance. Increasing the amount of training data examples can also improve while decreasing cannot. So the answer is C."
 },
 {
   "index": 151,
   "question": "Suppose you have a dataset with features based on characteristics of animals. Each animal has a classification assigned to it, but you don't know what the classification means or what animal is represented by each data row. Your task is to classify new animal rows by finding other animals similar to it. Could this best be formulated as a supervised or unsupervised learning model?",
   "A": "Unsupervised, because you don't know what the classifications are and the goal is to find similarities between animals.",
   "B": "Unsupervised, because you don't have time to manually label all of the data.",
   "C": "Supervised, because supervised learning is easier.",
   "D": "Supervised, because you have labeled data (classification assignment) despite not knowing what the label means.",
   "answer": "D",
   "why": "Supervised learning is the machine learning task of inferring a function from labeled training data. It does not matter if you don't know what the label means as long as it is labeled."
 },
 {
   "index": 152,
   "question": "In KNN based models, which of the following scenarios involving values of K is likely to produce an overfit model?",
   "A": "As K increases, we are more likely to produce an overfit model.",
   "B": "As K decreases, we are more likely to produce an overfit model.",
   "C": "Depending on the data set, increasing or decreasing K values may produce an overfit model.",
   "D": "K = N/2, where N is the size of the data set, will produce an overfit model.",
   "answer": "B",
   "why": "As K decreases, KNN model will produce values that are closer and closer to the actual data points. Therefore, when KNN model is trained using a training set and lower value of K, the model becomes too specific for that set and produces larger errors for any testing sets. This is why overfitting occurs. When K=1, KNN will produce the most overfit model."
 },
 {
   "index": 153,
   "question": "When plotting a validation curve, what is an example of overfitting?  MSE = mean squared error.  MSE is always positive.  Values closer to 0 are better.",
   "A": "The training MSE is high and the validation MSE is high.",
   "B": "The training MSE is low and the validation MSE is high.",
   "C": "The training MSE is low and the validation MSE is low.",
   "D": "None of the above.",
   "answer": "B",
   "why": "A higher mean square error (MSE) on the validation set is less likely to generalize the model correctly from the training set.  A is an example of underfitting.  B is an example of overfitting.  C is an example that generalizes well.  "
 },
 {
   "index": 154,
   "question": "Which of the following is not true about boosting ?",
   "A": "Boosting helps generate high-accuracy predictions by combining many weak learners to form a strong learner.",
   "B": "Boosting reduces bias in the resulting model.",
   "C": "Boosting weights misclassified data points so that future learners handle more of such misclassified points than past learners.",
   "D": "The classifiers used in Boosting ensemble learners can only be weak.   ",
   "answer": "D",
   "why": "The question asks for a false statement about boosting. While boosting usually uses weak learners to generate an ensemble strong learner, it is because weak learners are cheaper. Strong learners can also be used in Boosting to produce an ensemble boosted model."
 },
 {
   "index": 155,
   "question": "Which of the following statements about parameterized models is TRUE?",
   "A": "A parameterized model has less chance of overfitting on the training data, versus an instance-based model.",
   "B": "For large sets of data, parameterized models are generally much slower to train than instance-based models.",
   "C": "Examples of parameterized machine learning algorithms might include k-Nearest Neighbor and Decision Trees.",
   "D": "Parameterized models perform better than instance-based models in cases where you have an idea of what features you wish to train on.",
   "answer": "D",
   "why": "Parameterized models are called that because they expect some parameters/features identified to train on.  Instance-based models do not need features to be identified beforehand as they train best over large sets of training data where the user may not have a good idea of what the makeup of the data looks like."
 },
 {
   "index": 156,
   "question": "Consider a decision tree, with leaf size greater than 1 i.e. we aggregate several nodes into a leaf.     As you know, leaves of decision trees contain the output Y (prediction/classification/association) values, and since in our example tree, leaves can aggregate multiple nodes (up to leaf size), we need a way to calculate that aggregated leaf value.     If our aim is regression, then the average of aggregated nodes' values will give us the correct Y value. What if our aim is classification?",
   "A": "We need to take the smallest value among the aggregated nodes.",
   "B": "We need to take mode of values of aggregated nodes.",
   "C": "We need to take the average of smallest and highest values.",
   "D": "Decision trees can be used only for regression, not classification.",
   "answer": "B",
   "why": "For classification we need to vote, to select the value that most prominently classifies the instance. And mode function will give us the most prominent value, as this will be the most frequent value among aggregated nodes."
 },
 {
   "index": 157,
   "question": "Compared to the others, which of the below Decision Tree construction processes would save us computational cost for feature selection most?",
   "A": "C4.5 algorithm (J.R. Quinlan) using Information Entropy to select the feature that splits the training data most",
   "B": "C4.5 algorithm (J.R. Quinlan) using Correlation to select the feature that splits the training data most",
   "C": "C4.5 algorithm (J.R. Quinlan) using Gini Index to select the feature that splits the training data most",
   "D": "Random Tree algorithm (A. Cutler) using random number to arbitrarily select any feature to split the training data",
   "answer": "D",
   "why": "Quilan's algorithm in options A, B, C requires some optimization to determine the \"best\" feature that splits data. Hence they are more time-consuming. Cutler's methodology simply picks a random feature to split the data and forms a random decision tree, so it is much faster to grow a tree in this way. Such random trees can then be ensembled with algorithms like Bagging and Boosting, so that model accuracy won't be affected, surprisingly."
 },
 {
   "index": 158,
   "question": "Which of the following techniques can be used to reduce overfitting:",
   "A": "Using a simpler predictor.  ",
   "B": "Using more training examples.  ",
   "C": "Integrating over many predictors.  ",
   "D": "All of the above",
   "answer": "D",
   "why": "All three options are standard techniques used to reduce overfitting. "
 },
 {
   "index": 159,
   "question": "As increase leaf size in Random Tree, or k in KNN algorithm, or degree d of polynomial in Linear Regression, is it more likely to overfit? The order is Random Tree, KNN, Leaner Regression.",
   "A": "True, False, True",
   "B": "True, True, False",
   "C": "False, True, True",
   "D": "False, False, True",
   "answer": "D",
   "why": "When increasing the leaf size in Random Tree, it shouldn't be more overfit. The first one should be false.    When increasing k in KNN algorithm, it shouldn't be more overfit. because it will calculate the average for larger size. It should be false.    When increasing degree of polynomial in Leaner Regression, it should be more overfit. Because the more degree of polynomial means the model is more accurate for the training data. The last one should be turtle    So the answer is False, False, True"
 },
 {
   "index": 160,
   "question": "What should we use to determine the quality of predictions for any regression model?",
   "A": "The accuracy i.e. (PredictedY ActualY)/numberOfPredictions",
   "B": "RMSE",
   "C": "Mean of all errors",
   "D": "Sum of all errors",
   "answer": "B",
   "why": "A. Regression is used for continuous values, so this metric won’t take into account how close the predicted value is to the actual one. For example, it would return 0 accuracy if the true value was 5 and the predicted value in one case was 5.2 and the other case was 10.    B. RMSE is a good measure of how close the predicted value is to the true value.    C and D. They won’t take into account the absolute values of error, so if there is a prediction of +10 and -10 than the actual values, the error would average out to zero according to these metrics."
 },
 {
   "index": 161,
   "question": "Which of the following is false about supervised and unsupervised learning?",
   "A": "Unsupervised learning is typically used to group data based on similar relationships.",
   "B": "Supervised problems are often categorized as regression and classification problems.",
   "C": "Both supervised and unsupervised learning require training data with corresponding result values.",
   "D": "A decision tree is an example of supervised learning.",
   "answer": "C",
   "why": "Only supervised learning requires training data with corresponding result values, unsupervised learning only requires training data. "
 },
 {
   "index": 162,
   "question": "Which of these is typically the greatest advantage to using a parameterized model rather than an instance-based model?",
   "A": "Parameterized models tend to be faster to query  ",
   "B": "Parameterized models tend to be faster to train",
   "C": "Parameterized models tend to be less susceptible to overfitting",
   "D": "Parameterized models tend to be easier to understand  ",
   "answer": "A",
   "why": "Parameterized models are usually more expensive to train than instance-based models, which are more storage intensive but often do little processing during training. kNN is an example of an instance-based model, while a linear regression model is an example of a parameterized model. The advantage of of a parameterized model is that the hard work in training the model often pays of in query performance. A linear model can be computed against new data very quickly once it has been fit to data. Depending on the expressiveness of the model, many parameterized models can easily overfit data, such as a high-order polynomial regression model, so c) is not a good choice, and parameterized models such as a neural network or support vector machine are at a minimum not any easier to understand than an instance based model like kNN, so d) is not a good choice."
 },
 {
   "index": 163,
   "question": "Which algorithm would you use for a classification problem where all its features are binary i.e 0/1?",
   "A": "K-NN",
   "B": "Decision Trees",
   "C": "It depends on the output value",
   "D": "both A and B",
   "answer": "D",
   "why": "For binary attributes, it makes sense intuitively to opt for decision trees. However, we can use K-NN as well because the data points that have similar binary attribute values will be closer to each other, so K-NN would fit the model well. Since it is given that it is a classification problem, the output value doesn’t matter."
 },
 {
   "index": 164,
   "question": "What are some reasons that could explain a high out-of sample error in your model?",
   "A": "Overfitting and high RMSE on your in-sample data. ",
   "B": "Low RMSE and high correlation between your Ypredict and Ytest on your test set.",
   "C": "High correlation and low RMSE on your training set. Overfitting on your out-of sample data. ",
   "D": "Overfitting on your training set and low correlation on your test set. ",
   "answer": "D",
   "why": "D is the answer because high out-of sample error (i.e., the RMSE for the test set) is indicative of low correlation between the factors and the label in your test set. By knowing that your training set is overfitting, it can also provide some insights into why your model may have high out-of sample error.     A could potentially be an answer, but without any assessment of how the test data is also performing, it's difficult to assess why RMSE on the test set may be happening.     B is incorrect because, generally speaking, if the correlation between Ypredict and Ytest is high, you'll see RMSE to be lower. The answer choice also states \"Low RMSE\", but the question refers to high out-of sample error (i.e., high RMSE) so this is incorrect.    C runs into the same issue as A where, even though it seems that the model is fitting the training data well, it's difficult to draw conclusions without more details of the model with the test data. The second part of the answer also tells you that the model is overfitting on the test data, which means the out-of sample error is expected to be low. Thus, this answer is also incorrect. "
 },
 {
   "index": 165,
   "question": "Which of the following learning algorithms is the fastest to query (assuming a non-trivial model)?",
   "A": "k-Nearest-Neighbor with a low value for k",
   "B": "k-Nearest-Neighbor with a high value for k",
   "C": "Linear regression",
   "D": "Decision Tree",
   "answer": "C",
   "why": "Linear regression is the fastest to query as simply involves substituting the input into a linear equation. Decision trees would be next fastest, but involves a number of comparisons for a non-trivial model. kNN for any value of k is the slowest as all training data points must be considered."
 },
 {
   "index": 166,
   "question": "What is a key difference between Decision Tree Algorithm (JR Quinlan) and Random Tree Algorithm (A Cutler)?",
   "A": " Decision Tree Algorithm is a Classification method whereas Random Tree Algorithm is a Regression method.",
   "B": "Decision Tree Algorithm uses Median to split data whereas Random Tree Algorithm uses Mean value to split data.",
   "C": "We need to determine Random feature for Decision Tree Algorithm whereas Random feature is already provided for Random Tree Algorithm",
   "D": "All of the above",
   "answer": "B",
   "why": "‘A’ is not true as both of them could be used as a Regression method. ‘C’ not true as for Random Tree Algorithm a random feature has to be determined, ‘D’ is obviously not true and ‘B ‘is definitely a key difference between these algorithms and how it generates the decision tree."
 },
 {
   "index": 167,
   "question": "Which answer defines Overfitting ?",
   "A": "In sample error decreasing while out of sample data increasing",
   "B": "In sample error decreasing while out of sample data decreasing",
   "C": "In sample error increasing while out of sample data increasing",
   "D": "In sample error increasing while out of sample data decreasing",
   "answer": "A",
   "why": " In sample error is decreasing, out of sample error is increasing. When we train data in sample, we can expect RMSE would be decrease. In contrast, when we test data out of sample, also we can expect RMSE would be increase."
 },
 {
   "index": 168,
   "question": "Problem 1: A dataset for the types of fruits, with columns for color, length, diameter, density and taste. Predict the type of a particular fruit based on available data.    Problem 2: A dataset for the price of houses with columns for number of rooms, number of bathrooms, square-footage, location, and price of the house. Predict the price of other houses based on the data available.    What would you use for the above two problems?",
   "A": "Regression for both problems",
   "B": "Classification for both problems",
   "C": "Regression for problem 1 and classification for problem 2",
   "D": "Regression for problem 2 and classification for problem 1",
   "answer": "D",
   "why": "Regression is used to predict a continuous value based on the available data (when data is not labeled), therefore it can predict the price of houses. Classification is used to predict the category based on the given labeled data, therefore the type of fruit can be predicted."
 },
 {
   "index": 169,
   "question": "Which of the following best describes the part of an error functions graph (ie RSME) where overfitting is occurring between the test data set and its training data set?",
   "A": "The test data set error (out of sample) rate is increasing while the training’s data set error  (in sample) is increasing",
   "B": "The test error (out of sample) rate is increasing while the training set’s error (in sample) is decreasing",
   "C": "The test error (out of sample) rate is decreasing while the training set’s error (in sample) is decreasing",
   "D": "The test error (out of sample) rate is increasing while the training set’s error (in sample) is increasing",
   "answer": "B",
   "why": "In overfitting a model is produced that will follow the training data too closely to the detriment of the models usefulness on the training data.  This will be most seen in the error graph when there is a down tick in the error function for the in-sample tests, showing a better result for that data set, while at the same time increasing the error in the testing data set (out of sample)."
 },
 {
   "index": 170,
   "question": "RMSE is more sensitive to ______ than other measures of error. ",
   "A": "the occasional large error",
   "B": "the occasional small error",
   "C": "frequent small errors",
   "D": "all of the above",
   "answer": "A",
   "why": "The answer is (A) because the squaring process in the RMSE calculation gives disproportionate weight to very large errors. This can affect the quality of your prediction results if a large error is a problem in your choice situation.  "
 },
 {
   "index": 171,
   "question": "What's the most fundamental difference between k-means clustering and random decision tree and why? ",
   "A": "The are both machine learning algorithms that can be used to do classification given a certain amount of training has been done. But k-means requires training data to be purely numerical, whereas random decision tree can have categorical data. ",
   "B": "They are both machine learning algorithms that can be used to do classification given a certain amount of training has been done. But k-means takes more training data than random decision tree does. ",
   "C": "The are both machine learning algorithms that can be used to do classification given a certain amount of training has been done. But k-means requires more numerical computations, such as computing mean position of data points and evaluating different distance metrics (Euclidean distance, etc). ",
   "D": "The are both machine learning algorithms that can be used to do classification given a certain amount of training has been done. But K-means is an unsupervised learning technique whereas random decision tree is a supervised learning technique. ",
   "answer": "D",
   "why": "We are looking for fundamental difference between k-means clustering and random decision tree. K-means assumes no prior knowledge of label/class in the training data, ie. unsupervised. On the contrary, random decision tree requires ground truth labels/classes in order to train, i.e, a supervised learning process. "
 },
 {
   "index": 172,
   "question": "Out of the given choices, in a decision tree model for which value of leaf size would you expect your training set RMSE to be the highest?",
   "A": "1",
   "B": "10",
   "C": "20",
   "D": "30",
   "answer": "D",
   "why": "Training set RMSE would increase as we increase our decision tree leaf size as we get less precise in predicting the response variable. RMSE would be the highest for the highest value of leaf size, so in this case -> 30 i.e answer (d)"
 },
 {
   "index": 173,
   "question": "Which of the following are generally true of instance-based models?",
   "A": "They are able to interpolate beyond the original data",
   "B": "They have faster query times than parameterized models",
   "C": "They are more spatially efficient than parameterized models",
   "D": "They are easier to update than parameterized models.",
   "answer": "D",
   "why": "The correct answer is (D). (A) is incorrect because instance-based models directly use the sample data, and therefore typically cannot make any guesses outside the bounds of the original data. (B) is incorrect because querying is slow as the data is being consulted and the solution is being calculated as opposed to consulting a minimal, pre-built model. (C) is incorrect because they have to store all the data presented to them. (D) is correct, because in most case the data only need be stored to update the \"model,\" whereas in parameterized learning the whole model must be retrained. Any initial calculations that may be performed on the data to update the model tend to be minimal, and significantly faster than retraining an entire parameterized model."
 },
 {
   "index": 174,
   "question": "The major advantage of an \"Instance-Based model\" as compared to a \"Parameterized model\" is",
   "A": "Instance-Based models are easier to understand and results are easily interpretable",
   "B": "Instance-Based models are more flexible and capable of fitting a large number of functional forms.",
   "C": "Instance-Based models are very fast to learn from data compared to Parameterized models",
   "D": "Instance-Based models require less training data and work well even if the fit to the data is not perfect.",
   "answer": "B",
   "why": "Instance-Based models seek to best fit the training data in constructing the mapping function, whilst maintaining some ability to generalize to unseen data. All other options are benefits of Parameterized models over Instance-Based models"
 },
 {
   "index": 175,
   "question": "Which statement is False?",
   "A": "KNN has a large training cost",
   "B": "Decision trees have a large training cost",
   "C": "Linear regression has a small query cost",
   "D": "All are true",
   "answer": "A",
   "why": "Training time for KNN is not dependent on size of training set "
 },
 {
   "index": 176,
   "question": "In bagging, assume that the training set D is of size n and there are m bagging training sets each n'=n. i.e. n'=n=size(D). In those new bagging training sets, samples are randomly chosen from D. Then roughly how many unique samples are there in each bagging training set, others are duplicates?",
   "A": "63%",
   "B": "81%",
   "C": "52%",
   "D": "90%",
   "answer": "A",
   "why": "1-1/e ~63.2%"
 },
 {
   "index": 177,
   "question": "When building a decision tree, choosing the factor to split a node on can be done using entropy or the factor can be chosen randomly. Which is an advantage of choosing the factor to split on randomly?",
   "A": "Entropy is a complex measure, and though it leads to accurate trees, it is difficult to interpret how the predictions are made. Randomness leads to less complex trees with predictions that are easy to interpret.",
   "B": "Calculating entropy is more computationally expensive. Randomly choosing a factor to split on makes the algorithm faster.",
   "C": "Choosing the factor randomly increases the likelihood that all factors will be used, leading to trees that are more robust to overfitting.",
   "D": "Choosing the factor randomly does not work and does not offer any advantages over using entropy.",
   "answer": "B",
   "why": "Calculating entropy for the factors is the most expensive step of the algorithm. Choosing the factor randomly speeds up the algorithm."
 },
 {
   "index": 178,
   "question": "List one advantage and one disadvantage of Instance Based Models over Parameterized Models:",
   "A": "Advantage: Relatively small dataset is good enough to build a precise predictive model.   Disadvantage :  Is more error prone with test data set.  ",
   "B": "Advantage : Makes fewer assumptions about the data and so is less constrained to it.  Disadvantage : Computational complexity grows with the size of the data.  ",
   "C": "Advantage :  Relatively small dataset is good enough to build a precise predictive model.   Disadvantage : Computational complexity grows with the size of the data.  ",
   "D": "Advantage : Makes fewer assumptions about the data and so is less constrained to the data.  Disadvantage :  Is more error prone with test data set.  ",
   "answer": "B",
   "why": "Instance based models for example, KNN or Decision tree do not build the form of a function to summarize the data. That is why they assume less about the data. Instead they learn from the instances of the data points. Since they learn from each instance of the data point, as data set grows, the computational complexity of the model also grows linearly. Also, as instance based models are dependent on number of instances of the data, so a big data set allows better precision of the model. But since they do not assume anything about the data and do not form a function to summarize it they form more precise predictive model on unseen data."
 },
 {
   "index": 179,
   "question": "Which type of model is best-suited to adding new data?",
   "A": "Parametric",
   "B": "Testing",
   "C": "Instance-based",
   "D": "Training",
   "answer": "C",
   "why": "Instance-based is best to add new data because you don't need to change an existing underlying model. Even though it is easy to add testing and/or training data, those aren't models."
 },
 {
   "index": 180,
   "question": "A strength of Perfect Random Trees Ensembles is that they are not:                Answer C",
   "A": "Fast to fit",
   "B": "Accurate ",
   "C": "High Correlated",
   "D": "Randomly split",
   "answer": "C",
   "why": "The answer is the opposite of a correct answer. It's a did you read the paper we were supposed to read type of question."
 },
 {
   "index": 181,
   "question": "Which model is most likely to overfit?  Let n = the number of training instances.",
   "A": "k Nearest Neighbors with k = n, taking the unweighted mean of all neighbors.",
   "B": "Adaptive boosting with decision trees where leaf size = 1",
   "C": "Linear Regression",
   "D": "Decision Stump (Decision tree with one level)",
   "answer": "B",
   "why": "Options A and D would be very likely to underfit the data and Linear regression is less likely to overfit than using a \"not-so-weak\" learner  with adaptive boosting."
 },
 {
   "index": 182,
   "question": "Which option is true for instance based models?",
   "A": "It has fixed number of parameters",
   "B": "It is very fast to learn the data pattern",
   "C": "Makes fewer assumptions about the data",
   "D": "Makes more assumptions about the data",
   "answer": "C",
   "why": "Instance based models deals with real train data so fewer assumptions."
 },
 {
   "index": 183,
   "question": "You train a random forest decision tree with a leaf size of 1 then query it on the in-sample data set, it outputs a result with an RMSE of 0 and a correlation of 1. This result could mean:",
   "A": "The model is likely under fit to the in sample data set.",
   "B": "The model is likely overfit to the in sample data set and will likely produce inaccurate results out of sample.",
   "C": "The model will produce perfectly accurate results out-of sample.",
   "D": "The model has generalized and will be able to provide accurate predictions out-of sample.",
   "answer": "B",
   "why": "The model is likely overfit. RMSE of zero means there is no error and the the correlation of 1 means the prediction labels are the same as the in-sample actual labels. This likely means it has failed to generalize and won't product accurate results out-of-sample."
 },
 {
   "index": 184,
   "question": "One of the main goals with decision trees is to split the data as quickly and efficiently as possible into similar groups. In the Decision Trees lecture, we were provided with three potential measures for determining the best feature to split on. Which of the following is NOT one of those measures?",
   "A": "Entropy",
   "B": "Correlation",
   "C": "Sharpe Ratio",
   "D": "Gini Index",
   "answer": "C",
   "why": "The Sharpe Ratio is a measure used to calculate risk-adjusted returns, not similarity of a group. "
 },
 {
   "index": 185,
   "question": "Which of the following statement is incorrect?",
   "A": "Kernel regression is a nonparametric, instance-based learning algorithm.",
   "B": "A decision tree is a parametric learning algorithm because the decision split points may be determined from some distribution of feature values",
   "C": "KNN is an instance based learning algorithm as it does not make any assumption about the distribution.",
   "D": "Linear regression is a parametric learning algorithm as it can summarize the data with a set of fixed parameters. ",
   "answer": "B",
   "why": "Decision tree is an instance-based learning algorithm and it does not assume any assumption about the distribution. "
 },
 {
   "index": 186,
   "question": "What method, by itself, is not used for reducing variance error in individual decision trees?",
   "A": "Imposing limits on tree depth. ",
   "B": "Stopping training at a larger leaf size. ",
   "C": "Randomly selecting split decisions when training. ",
   "D": "Pruning splits based on their impact on validation error. ",
   "answer": "C",
   "why": "A, B, and D are common techniques for reducing overfitting. By itself, randomly selecting splitting decisions does not reduce overfitting, it will fit one leaf for each distinct data point. "
 },
 {
   "index": 187,
   "question": "Which of the following quality metric is robust to outliers?",
   "A": "Root Mean Squared Error",
   "B": "Mean absolute error",
   "C": "r2 square",
   "D": "explained variance score",
   "answer": "B",
   "why": "Since mean absolute error gives equal weight to the data, hence it is robust to outliers."
 },
 {
   "index": 188,
   "question": "Which of the following statements if TRUE in regards to overfitting of linear regression models?  Note: D = number of dimensions, N = number of test inputs",
   "A": "For linear regression models, overfitting is more likely to occur as D INCREASES",
   "B": "For linear regression models, overfitting is more likely to occur as D DECREASES",
   "C": "For linear regression models, overfitting does not occur",
   "D": " For linear regression models, overfitting occurs, but is equally as likely for any value of D",
   "answer": "A",
   "why": "Overfitting is more likely as D increases because the resulting polynomial will produce a line that more closely matches the training data, which most likely contains either some random error or some noise."
 },
 {
   "index": 189,
   "question": "Which of the following statements is TRUE about overfitting?",
   "A": "An overfitted model has a poor predictive performance on new, unseen data.",
   "B": "Overfitting cannot occur on nonparametric instance-based models.",
   "C": "For a polynomial regression model with degree d, we are more likely to overfit as d decreases.",
   "D": "A model may be overfitting when its in-sample error and out-of-sample error both start to decrease.",
   "answer": "A",
   "why": "Correct answer is (A) because an overfitted model tries so hard to fit to the training data perfectly (\"memorize\" training data rather than learning it) that the resulting model does not generalize to new, unseen data well."
 },
 {
   "index": 190,
   "question": "Suppose you used four different regression models (A, B, C, D) and achieved the following values :     A . Low correlation, Low RMSE  B.  Low correlation, High RMSE  C. High correlation, High RMSE  D. High correlation, Low RMSE    Which answer choice depicts the arrangement of the regression models in terms of prediction quality, worst quality first.",
   "A": "ABCD",
   "B": "ABDC",
   "C": "BCAD",
   "D": "BACD",
   "answer": "C",
   "why": "RMSE is a much more strong factor than Correlation for measuring the predictor quality.  There may be cases when prediction is highly correlated but RMSE is also high, thus reducing the importance of the regression model."
 },
 {
   "index": 191,
   "question": "If you want to normalize your data, which of the following commands in Python is correct? (df is short for data frame)",
   "A": "df/df.ix[0,:] ",
   "B": "df/df.ix[:,0]",
   "C": "df*df.ix[0,:]",
   "D": "df*df.ix[:,0]",
   "answer": "A",
   "why": "If you want to normalize your data, you should take all the data to divide the first row, and the command df.ix[0,:] slices exactly the first row out, therefore the correct answer is A. "
 },
 {
   "index": 192,
   "question": "What is the difference between classification tree and regression tree?",
   "A": "Classification tree is used when the target variable is categorical or discrete. Regression tree is used when the target variable is numeric or continuous. ",
   "B": "Classification tree is used to predict and regression is used to group. ",
   "C": "Classification tree is used to classify and regression is used to predict. ",
   "D": "A, B",
   "answer": "D",
   "why": "The difference between classification tree and regression tree depends on the data and what you want to do with the data. Classification tree have discrete target variable that is used for classifying. Regression tree is build on continuous numeric values that is used for predicting"
 },
 {
   "index": 193,
   "question": "which one is more likely to overfit as the number of bags increase?",
   "A": "Simple bagging",
   "B": "AdaBoost",
   "C": "It depends on the size of the leaf in the decision tree",
   "D": "None of the above",
   "answer": "B",
   "why": "AdaBoosting is trying hard to match those data points that are off or outliers and thus is more susceptible to overfitting"
 },
 {
   "index": 194,
   "question": "Which of the following is TRUE about overfitting?  ",
   "A": "In a KNN model, it is more likely to occur overfitting if we increase K.    ",
   "B": "In a polynomial model, it is more likely to occur overfitting if we increase degree d.  ",
   "C": "In a random tree learner, it is more likely to occur overfitting if we decrease leaf size.  ",
   "D": "In a bagging learner, it is more likely to occur overfitting if we decrease the number of bags.",
   "answer": "B",
   "why": "In a KNN model, if we increased k, like k=N, then all neighbors would be considered and hence no overfitting. It occurs when K is small.  In a polynomial model, if d is small, like d=1, then it is a linear model and cannot over fit. When d is larger, it is more likely to overfit.  In the project, we know that if fix bags, the larger leaf size, the better performance of the model. Also, if fix leaf size, the larger the number of bags, the better performance of the model. So the correct answer is B.  "
 },
 {
   "index": 195,
   "question": "Which of the following is a true statement regarding supervised and unsupervised learning?",
   "A": "In supervised learning the machine is shown many examples of x(the predictors) and y(the correct answer), whereas in unsupervised learning the machine does not know the correct answer.",
   "B": "Decision trees are an example of unsupervised learning whereas linear regression is an example of supervised learning",
   "C": "Unsupervised learning models will always be more accurate than supervised learning models.",
   "D": "All of the above statements are true.",
   "answer": "A",
   "why": "A is the definition of supervised learning. In unsupervised learning the correct output is not known. B is false as both are examples of supervised learning. C is not always true. A is the only correct answer"
 },
 {
   "index": 196,
   "question": "For simple bagging, how can we choose the number of learners that will help reduce overfitting?",
   "A": "m=0",
   "B": "m=1",
   "C": "large m",
   "D": "It depends",
   "answer": "C",
   "why": "For simple bagging, increase the number m will help reduce overfitting as while m increasing, it resulted in more generalized predictions as the predictions for each learner results are averaged."
 },
 {
   "index": 197,
   "question": "You use bagging with decision trees such that each tree learns from a random subset of your training data. You might want to:",
   "A": "avoid traditional decision trees (Quinlan) because they require normalization, which can be costly.",
   "B": "build each tree on random splits because it is much faster than building trees with splits based on information gain.",
   "C": "avoid bagging with decision trees built using information gain, because the trees would all be identical.",
   "D": "both a and c.",
   "answer": "B",
   "why": "Correct answer is b)  because a great advantage of random decision trees is the speed of learning in comparison to traditional decision trees. a) is wrong because decision trees don't require normalization. c) is wrong because, while each tree would use information gain, they would not be identical because each was trained on a different subset of the data."
 },
 {
   "index": 198,
   "question": "How about the effect of random tree built using A. Cutler's approach and a standard decision tree using JR Quinlan's approach?  ",
   "A": "A single random tree’s effect is almost equal to a standard tree.  ",
   "B": "A single random tree’s effect surpasses a standard tree.",
   "C": "Multiple Random tree with bagging’s effect can surpass a standard tree  ",
   "D": "Not all of above.  ",
   "answer": "C",
   "why": "due to radom choice of feature and split value, the effect of random tree is worst than a standard tree. However if we build multiple tree with bagging, the effect can surpass a standard tree."
 },
 {
   "index": 199,
   "question": "Given a dataset with X features and the Y values are given as  [2,3,4,8,2,5,4,8,7,6,9,4,1,3,7,5,6,9] for a sample, what kind of supervised learning would you use?",
   "A": "Classification since there are clearly 9 distinct classes.",
   "B": "Regression because the values are consecutive numbers and therefore can be considered continuous.",
   "C": "Both classification and regression because either can be used to fit a model.",
   "D": "The given sample size is not big enough to determine which type of learning algorithm to apply.",
   "answer": "C",
   "why": "A is true because we can see 9 classes and so is B as the values are continuous. This is similar to the wine dataset problem where either regression or classification can be used."
 },
 {
   "index": 200,
   "question": "After training with k-nearest neighbor (pattern) classification algorithm, your model is 100% accurate in fitting the training data. But when applying it on real data, it generates big error and only 25% percent accurate on real data. Which method below might work during optimize your model?",
   "A": "Use more features  ",
   "B": "Use more training samples",
   "C": "Use more testing samples",
   "D": "Use smaller k  ",
   "answer": "B",
   "why": "Because to solve overfitting, we usually use less features, more training data, and increase k in knn algorithm, so (a) and (d) are wrong. (c) is no help for overfitting."
 },
 {
   "index": 201,
   "question": "How about the training time of a random tree built using A. Cutler's approach and a standard decision tree using JR Quinlan's approach?  ",
   "A": "The training time of random tree is faster than a standard decision tree  ",
   "B": "The training time of random tree is slower than a standard decision tree  ",
   "C": "The training time of random tree is equal to a classical standard tree  ",
   "D": "We cannot decide which one is faster.  ",
   "answer": "A",
   "why": "the process of choosing feature and split value randomly can speed up the training process and make building random tree faster."
 },
 {
   "index": 202,
   "question": "Which of the following is NOT true when comparing between kNN, decision trees, and linear regression?",
   "A": "kNN does not require an explicit learning prior to testing (or is very minimal), thus resulting in the lowest training cost of the three",
   "B": "Linear regression takes the most time to query due to using complex mathematical formula",
   "C": "A perfect prediction accuracy can be achieved with the decision tree if a leaf size of 1 is used for in-sample",
   "D": "Linear regression is likely to result in a very low prediction accuracy when there are many outliers in the data",
   "answer": "B",
   "why": "B is not true as linear regression is most likely to take the least time to query among the three. It uses mathematic functions to query, unlike the other two, which also need to traverse and/or perform iterations in order to get results.    (A is true because kNN is a lazy learning algorithm, which defers most of the computations until testing (i.e. classification); C is true because using a leaf size of 1 and the same test dataset as the training dataset (in-sample) would always return the known, accurate result; D is true because linear regression is not reliable when the data is highly non-linear)"
 },
 {
   "index": 203,
   "question": "Suppose we are measuring correlation between price and sales of a commodity. Which of the following number will we most likely to get?",
   "A": "-2.4",
   "B": "-0.8",
   "C": "0.8",
   "D": "1.5",
   "answer": "B",
   "why": "The correlation should be range from -1 to 1. And minus means they are inversely correlated, since drop in price will increase sales."
 },
 {
   "index": 204,
   "question": "Which of the following is NOT true when comparing between kNN, decision trees, and linear regression?",
   "A": "kNN does not require an explicit learning prior to testing (or is very minimal), thus resulting in the lowest training cost",
   "B": "Linear regression takes the most time to query due to using complex mathematical formula",
   "C": "A perfect prediction accuracy can be achieved with the decision tree if you use a leaf size of 1 for in-sample",
   "D": "Linear regression is likely to result in a very low prediction accuracy when there are many outliers in the data",
   "answer": "B",
   "why": "B is not true as linear regression is most likely to take the least time to query among the three. It uses mathematic functions to query, unlike the other two, which need to traverse and/or perform iterations as well in order to get the result.    (A is true because kNN is a lazy learning algorithm, which defers most of the computations until testing (classification); C is true because using a leaf size of 1 and the same test dataset with the training dataset (in-sample) would always return the known, accurate result; D is true because linear regression is not reliable when the data is highly non-linear)"
 },
 {
   "index": 205,
   "question": "Which of the following statement is correct regarding unsupervised learning and supervised learning?",
   "A": "In unsupervised learning, all data is labeled and the algorithms learn to predict the output from the input data.",
   "B": "In supervised learning, all data is unlabeled and the algorithms learn to inherent structure from the input data.",
   "C": "Supervised learning includes linear regression, decision trees, random forest, and classification. ",
   "D": "Unsupervised learning includes K-means clustering, self-organizing maps, association, and support vector machines for classification. ",
   "answer": "C",
   "why": "The correct answer is C. Because in supervised learning, all data is labeled and the algorithms learn to predict the output from the input data. In unsupervised learning, all data is unlabeled and the algorithms learn to inherent structure from the input data. Supervised learning includes linear regression, decision trees, random forest, classification and support vector machines for classification. Unsupervised learning includes K-means clustering, self-organizing maps, association, but does not include support vector machines for classification. "
 },
 {
   "index": 206,
   "question": "In Random tree learner if a leaf size is provided we use that as a limit to put as many as leaf_size values in a leaf. What is the ideal value(SplitVal) that should be used to identify this leaf with multiple entries?",
   "A": "Value of the first element that was pushed to this leaf",
   "B": "A random value picked from the set of values in the leaf",
   "C": "Mean of the values that belong to this leaf",
   "D": "Pick the maximum value of all the values in the leaf",
   "answer": "C",
   "why": "Ans: Mean of the values that belong to this leaf.    In Random tree learner if a leaf size is provided we use that as a limit to as many as leaf_size values in a leaf. We need to use a correct value to make sure the decision tree functions.    Of the given choices, (A) First value and (D) Maximum value are clearly not correct answers. We also cannot pick a (B) random value as that will hurt the accuracy.    Out of the given choices C mean is the closest. If the student had worked thru MC3 P1 this should be easy to answer."
 },
 {
   "index": 207,
   "question": "Consider the options below and select the appropriate choice for the kind of trees that are preferred and why:",
   "A": "Shorter trees are preferred; Attribute selection based on Entropy builds Longer Trees",
   "B": "Longer trees are preferred; Attribute selection based on Entropy builds Longer Trees",
   "C": "Shorter trees are preferred; Attribute selection based on random feature selection builds shorter trees",
   "D": "All the above choices are equally ok ",
   "answer": "A",
   "why": "Correct Answer is a) Shorter Trees are preferred because they fit the data with the simplest hypothesis and avoid overfitting.   Attribute selection measures based on Information gain like Entropy, Gini Index  place high information gain features close to the root of the tree resulting in shorter trees"
 },
 {
   "index": 208,
   "question": "Which of the following does not require training?",
   "A": "Decision trees",
   "B": "Linear regression",
   "C": "K nearest neighbour",
   "D": "Decision forests",
   "answer": "C",
   "why": "KNN just stores each data, so it spends 0 time on training"
 },
 {
   "index": 209,
   "question": "Q1. Which of the following measures of building a decision tree will NOT lead to successfully classifying atleast 50% of training data?",
   "A": "a) Attribute selection based on Entropy",
   "B": "b) Attribute selection based on Gini Index",
   "C": "c) Random Feature selection and splitting based on that feature",
   "D": "d) None of the above",
   "answer": "D",
   "why": "Correct Answer is D) None of the above. Note that the questions ask you to compare based on performance on Training data and tests the   fundamental concepts on how trees are built. Easier variant will be performance on test data, in that case, C will be our answer"
 },
 {
   "index": 210,
   "question": "Which of the following statements is false?",
   "A": "Decision trees can represent functions that linear regression cannot represent.",
   "B": "Linear regression can represent functions that decision trees cannot represent.",
   "C": "Decision trees can always be trained to get a lower in-sample error than linear regression",
   "D": "Linear regression can always be trained to get a lower in-sample error than decision trees",
   "answer": "D",
   "why": "A is true as functions represented by decision trees are nonlinear.  B is true as decision trees applied to regression functions always learn step functions.  C is true as decision trees can always have decision rules that model any train set perfectly.  D is false because C is true."
 },
 {
   "index": 211,
   "question": "Which of the following is not true about boosting ?",
   "A": "Boosting helps generate high-accuracy predictions by combining many weak learners.",
   "B": "The classifiers used in Boosting ensemble learners can be weak or strong.",
   "C": "The classifiers used in boosting ensemble learners should be weak alone, as long as they are better than a random guess.",
   "D": "Boosting of ensemble learners reduces bias of many of small models with low variance.",
   "answer": "C",
   "why": "The question is to pick the false statement about Boosting. While an ensemble of many weak learners can be combined to perform better than a single strong learner, an ensemble of strong learners can also be used. The reason it is not usually used is that it's cheaper to construct weak learners than strong ones."
 },
 {
   "index": 212,
   "question": "In terms of the cost of query, which of the following algorithms is considered the be the worst or slowest?",
   "A": "Decision Tree",
   "B": "KNN",
   "C": "Linear Regression",
   "D": "They are all equal for terms of cost of query",
   "answer": "B",
   "why": "The system would have to compute the distance from the query to all the individual data points, sort them, and find the nearest K data point"
 },
 {
   "index": 213,
   "question": "what of the following statements about Boosting and Bagging is true?",
   "A": "The key goal of both algorithms is to decrease variance.",
   "B": "Both of algorithms produce a distribution of simple ML models on subsets of the original data and then combining the distribution into one \"aggregated\" model.",
   "C": "Both of algorithms create next classifier/ML model based on results of previously trained classifier.",
   "D": "Random Forest modifies the grown procedure to reduce the correlation between trees, so Random Forest is a boosting method",
   "answer": "B",
   "why": "A: Wrong. the main goal of boosting is to increase predictive force.  B. right  C: Wrong. Bagging builds classifier/model independent from the previous one to decrease variance. (Each model is independent)  D. wrong. It is designed to be no correlation between trees. So it is a Bagging method"
 },
 {
   "index": 214,
   "question": "Which one is true about overfitting?",
   "A": "Overfitting is beneficial when learning a classifier ",
   "B": "Cross-validation can reveal overfitting",
   "C": "Overfitting often ignores minor fluctuations in the training data",
   "D": "Overfitting is abosolutely unavoidable",
   "answer": "B",
   "why": "When using cross-validation, if the accuracy on the training folds is higher than on the test folds, the overfitting occurs. So cross-validation can reveal overfitting."
 },
 {
   "index": 215,
   "question": "Pick the choice where the statements regarding KNN, Decision Tree and Linear regression are all true",
   "A": "KNN is a lazy learner, Decision tree accepts only nominal features, Linear regression stores all of the training data",
   "B": "Decision tree is used for both regression and classification,  Linear regression takes nominal features, KNN is unsupervised learning",
   "C": "KNN is computationally expensive, Decision tree is an eager learner, Linear regression takes numeric features",
   "D": "KNN is lazy learner, Decision tree is unsupervised learner, Linear regression does not store any data",
   "answer": "C",
   "why": "Decision tree is a supervised learner. It is an eager learner because it builds a classification model based on the training data and then uses it for classifying test data. It takes both numeric and nominal features. It is used for classification and not regression.  KNN is an unsupervised learner. It is a lazy learner. It is computationally intensive because it needs to go through all the data to give the result.  Linear regression does not store any data. As soon as an equation is formed to fit the data, the data is thrown away. It takes only numeric features."
 },
 {
   "index": 216,
   "question": "Q1: In creating a decision tree, when choosing a feature to split on, what is the most correct use of correlation?",
   "A": "Choose the feature with correlation value closest to 1",
   "B": "Choose the feature with correlation value closest to 0",
   "C": "Choose the feature with correlation value closest to -1",
   "D": "Choose the feature with the absolute of correlation value closest to and including 1",
   "answer": "D",
   "why": "Correct answer is D because correlation can go both positive and negative ways"
 },
 {
   "index": 217,
   "question": "Which of the following model is 'not' considered to be overfitted?",
   "A": "An excessively complex model having too many parameters relative to number of observations",
   "B": "A model that learns to generalize from the trend rather than memorizing training data",
   "C": "A model that is trained by maximizing its performance on training data rather than on unseen data",
   "D": "A model that has high out-sample error and low in-sample error.",
   "answer": "B",
   "why": "B. is correct because this model tries to generalize to other unseen data sets by learning the trends from training data and thus not prone to overfitting.  A. is wrong because with too many parameters relative to the observations, it overreacts to minor fluctuations in the training data and thus lowering the predictive performance.  C. is wrong because the efficacy of the model is determined not by its performance on the training data but by its ability to perform well on unseen data  D. is wrong because its general scenario of overfitting with high out-sample error while low in-sample error"
 },
 {
   "index": 218,
   "question": "Which of the following is an example of a problem best solved using parametric modeling?",
   "A": "Identifying images of trees given a large dataset where the images are either trees or butterflies",
   "B": "Estimating the distance a canon ball shot from a canon will travel given the angle of takeoff",
   "C": "Classifying the species of flower samples given petal length and color",
   "D": "Determining the number of bees attracted to a food source given the richness of the food source",
   "answer": "B",
   "why": "The distance travelled by a cannon ball is an example of a parametric modeling problem because you can start with a mathematical equation (an estimate of the underlying behavior of the system) to express how it will behave. The other choices are better solved by non-parametric or instance based modeling"
 },
 {
   "index": 219,
   "question": "Which of the following methods are BEST suited for building instance based models?",
   "A": "Linear regression",
   "B": "k-NN",
   "C": "k-NN and kernel regression",
   "D": "Parametric regression",
   "answer": "C",
   "why": "k-NN and kernel regressions are non-parametric methods that can be used for building an instance based model. Kernel regression takes into account the weightage of the nearest datapoints, whereas k-NN gives equal weightage to the nearest datapoints."
 },
 {
   "index": 220,
   "question": "Being given a very large dataset, ensuring that the models do not trivially overfit   (k and leaf size both superior to 1 for kNN and Decision Trees), select the best possible order when in comes to memory space for saving the model",
   "A": "Linear Regression is better than kNN which is better than Decision Trees",
   "B": "Linear Regression is better than Decision Trees which is better than kNN",
   "C": "Linear Regression is the best and for kNN and Decision Trees it depends on the number of attributes (columns) in the data",
   "D": "None of the above",
   "answer": "B",
   "why": "Linear Regression is the best in terms of space for saving the model. For Decision trees and kNN: if the leaf size is strictly greater than 1, the size of the model stored is at most equal to a number which is striclty inferior to the number of rows in the data. The space used is then lower for decision trees, than kNN which needs to store the whole dataset"
 },
 {
   "index": 221,
   "question": "For which value of k from the following options does the KNN model have the least in sample RMS error?",
   "A": "1",
   "B": "2",
   "C": "3",
   "D": "4",
   "answer": "A",
   "why": "Lesser the value of k, more closely the model fits the data, resulting in lesser in-sample RMS error (In case of k=1 the error will be least = 0)"
 },
 {
   "index": 222,
   "question": "The main difference between supervised and unsupervised learning is...",
   "A": "Supervised learning maps the expected output while unsupervised learning only knows the variables.",
   "B": "Unsupervised learning maps the expected output while supervised learning only knows the variables.",
   "C": "Unsupervised learning is fast and accurate.",
   "D": "Supervised learning clusters input data based on statistical properties only.",
   "answer": "B",
   "why": "Correct answer is b) because supervised learning maps inputs to outputs."
 },
 {
   "index": 223,
   "question": "What kind of error does RSME emphasize?   ",
   "A": " It emphasizes the larger errors a bit more",
   "B": "It emphasizes the smaller errors a bit more",
   "C": "It emphasizes the most common errors  a bit more",
   "D": "It doesn't emphasize any kind of error.",
   "answer": "A",
   "why": "The square of a number like 64 is 8 versus the square of a number like 2 is ~1.7 so the larger errors are emphasized a bit more.  "
 },
 {
   "index": 224,
   "question": "Why boosting is less commonly used with decision trees than bagging?",
   "A": "Boosting algorithm is poorly suited for iterative processing.",
   "B": "Boosting algorithm reduces the variability of training data that will inevitably lead to overfitting.",
   "C": "Boosting algorithm uses a randomly chosen training set rather than picking a small more targeted subset which can reduce a chance of detecting a structure in the data.",
   "D": "Boosting  is an averaging algorithm used to reduce the variance of individual trees.",
   "answer": "B",
   "why": "Boosting algorithm is only focused on results that were incorrectly predicted in the previous iteration. By doing so, it reduces the size of data with each subsequent step. This limited set of samples will inevitably lead to overfitting."
 },
 {
   "index": 225,
   "question": "What best explains the risky nature of selling short?",
   "A": "The investor's losses are potentially unbounded if the underlying stock goes up.",
   "B": "The investor must pay interest on any shares that are borrowed.",
   "C": "Short selling does not go through an exchange's order books, but rather is handled at the broker.",
   "D": "Companies tend to have negative reactions to their stocks being shorted.",
   "answer": "A",
   "why": "As short selling involves \"borrowed\" shares of stock that the investor must return later, the investor is responsible for covering any increase in price that the stock has undergone during the time period. The stock's price can increase without bound, and as a result, the investor can be responsible to pay back a theoretically unlimited amount (whereas, in comparison, a \"long\" position on a stock can lose only 100% of its value if the stock price goes to zero)."
 },
 {
   "index": 226,
   "question": "Which of the following is correct about overfitting?",
   "A": "Overfitting can't occur on nonlinear and nonparametric models.",
   "B": "Overfitting has a very high forecasting accuracy in any cases.",
   "C": "Overfitting refers to a model that can neither model the training data not generalize to new data.",
   "D": "An overfitted model yield a poor forecasting accuracy on unseen data.",
   "answer": "D",
   "why": "The answer is (d). Overfitting basically makes an effort to fit into the training data perfectly that the results are not generalized at all."
 },
 {
   "index": 227,
   "question": "When should we use the 4th Root of the mean of the 4th powers (i.e. RMSE but instead of squaring the errors are raised to their 4th powers and instead of square rooting, the numbers are rooted by a power of 4) instead of RMSE changing nothing else?",
   "A": "It doesn't matter, they can be used interchangeably.",
   "B": "When we want to penalize predictors with even a small fraction of outliers heavily.",
   "C": "When we want only to penalize predictors with a large fraction of outliers heavily but small fractions of outliers is acceptable.",
   "D": "One cannot say with the information given.",
   "answer": "B",
   "why": "Correct Answer is B: because the 4th power of a larger error will be much greater than the square of the same error, hence we will penalize outliers significantly more that we were in RMSE"
 },
 {
   "index": 228,
   "question": "Which of the following statements are True?    I. Parametric models are more space efficient than instance based models, but their training time is relatively slower.  II. Querying time of Parametric models is faster when compared to Instance Based Models.  III. Parametric models are more space efficient than instance based models and their training time is relatively faster.  IV. Querying time of Parametric models is slower when compared to Instance Based Models.",
   "A": "I, II",
   "B": "II, III",
   "C": "III, IV",
   "D": "I, IV",
   "answer": "A",
   "why": "Parametric models do not store any data, parameters need to be recomputed when more data is gathered and it uses only the solution equation to query. While instance based models store all data points, and consults the data to query.  "
 },
 {
   "index": 229,
   "question": "Of the three learning algorithms, K-nearest neighbor, decision trees, and linear regression, which is the most resource intensive operation to query test data and why?",
   "A": "a)\tLinear Regression: because as the size of the testing set to be classified grows computational resources increase exponentially in the query stage.",
   "B": "b)\tDecision Trees: The query step requires many Boolean operations to be carried out on the learned tree which are slow in most environments.",
   "C": "c)\tDecision Trees: To query the data both the training and testing data must be used which can slow query times in large data sets. ",
   "D": "d)\tK- nearest neighbors: Because in order to query a new test observation the entire training set must be held in memory, leading to high resource demands. ",
   "answer": "D",
   "why": "KNN  In order to classify any new data point using KNN, the entire data set must be used meaning the training data must be held in memory, this is not true for decision tree or regression learners and results in the cost of query for KNN being the highest of the three, especially as the training data set becomes very large."
 },
 {
   "index": 230,
   "question": "Which among the following is NOT an example of Regression vs Classification",
   "A": "Size of a Tumor vs probability of cancer",
   "B": "Size of Tumor vs type of cancer",
   "C": "size of tumor vs whether a tumor is cancerous or not",
   "D": "size of tumor vs how many chemotherapy sessions are required",
   "answer": "A",
   "why": "size of tumor is a numerical measure, the probability of cancer is also a numerical measure, hence both of them are regression case"
 },
 {
   "index": 231,
   "question": "Which is a better split way for single tree?",
   "A": "Choose random feature",
   "B": "Choose feature by Correlation",
   "C": "Choose feature by order",
   "D": "Choose feature by mean of array",
   "answer": "B",
   "why": "Correlation shows better feature related to the result."
 },
 {
   "index": 232,
   "question": "Which of the following choices does not belong to supervised regression learning?",
   "A": "a) Linear regression",
   "B": "b) Decision forests",
   "C": "c) Clustering",
   "D": "d) K nearest neighbor (KNN)",
   "answer": "C",
   "why": "Correct answer is c) because supervised learning needs both predictors X and response Y, but clustering only has predictors X and it does not has response Y. "
 },
 {
   "index": 233,
   "question": "Which of the following three statements regarding overfitting is/are true:    1) For a KNN learner, decreasing K is likely to increase overfitting.    2) For a parametric regression learner, increasing d (the degree of the equation) is likely to decrease overfitting.    3) For a bootstrap aggregating learner, implementing adaptive boost has no impact on overfitting when compared with using simple bagging.",
   "A": "(1) only",
   "B": "(1) and (2) only",
   "C": "(2) and (3) only",
   "D": "(1) and (3) only",
   "answer": "A",
   "why": "(1) is a true statement; as K is decreased, there are fewer and fewer \"nearest neighbors\" to consider for a given input at query time, and taking the mean of fewer neighbors increases chances of overfitting.    (2) is a false statement; increasing d means that the model equation curve will end up having more \"twists and turns\" in order to fit the data more closely, thus increasing chances of overfitting.    (3) is a false statement; implementing ada boost may result in a greater likelihood of overfitting over simple bagging provided that m (the number of bags) grows large enough. This is one of the possible drawbacks of ada boost, which tries to make adjustments in the creation of each successive bag based on the sets of data that have particularly poor results in the previous bags."
 },
 {
   "index": 234,
   "question": "Which of the following best distinguishes unsupervised learning from reinforcement learning?",
   "A": "Reinforcement learning requires a reward signal while  unsupervised learning does not.",
   "B": "Reinforcement learning attempts to exploit hidden structure in unlabeled data while unsupervised learning exploits hidden structure in labeled data.",
   "C": "Reinforcement learning techniques do not usually require normalization of input features while unsupervised learning techniques do.",
   "D": "Reinforcement learning models are often generative while unsupervised learning models are not.",
   "answer": "A",
   "why": "A reward signal or a training label is what distinguishes reinforcement learning and supervised learning from unsupervised learning. (B) is clearly wrong. (C) Normalization is not the distinguishing difference between these classes of algorithms. (D) Both are often generative but don't have to be."
 },
 {
   "index": 235,
   "question": "Which of the following is key characteristic of supervised learning?",
   "A": "Training data includes only input but not desired results.",
   "B": "Training data includes both input and desired results.",
   "C": "Training data includes only desired results but not input.",
   "D": "Both A and B are correct.",
   "answer": "B",
   "why": "Supervised learning required both input data and results to generalize to a model."
 },
 {
   "index": 236,
   "question": "Bagging is typically when you want to reduce the variance while retaining the bias. What is the correct approach when using this method ? ",
   "A": "Sample the input data (WITH replacement). And, for each of to those sets, apply the SAME predictor(ex: svm, linear)to get a trained model for each of the training set. ",
   "B": "Sample the input data (WITHOUT replacement). And, for each of to those sets, apply the SAME predictor(ex: svm, linear)to get a trained model for each of the training set. ",
   "C": "Sample the input data (WITH replacement). And, for each of to those sets, apply DIFFERENT predictors(ex: svm, linear)to get a trained model for each of the training set. ",
   "D": "Sample the input data (WITHOUT replacement). And, for each of to those sets, apply DIFFERENT predictors(ex: svm, linear)to get a trained model for each of the training set.",
   "answer": "A",
   "why": "Bagging (Bootstrap aggregating), you sample the data with replacement. For each of those set, use the same predictor to train the model then averaging-out or vote to get the result. "
 },
 {
   "index": 237,
   "question": "Which of the following is NOT correct about the difference between Cutler's Random Tree algorithm and Quinlan's ID3 algorithm?",
   "A": "In Random Tree algorithm, you choose the route randomly at each node instead of comparing the value to the split value so that it prevents overfitting and reduces variance.",
   "B": "In ID3 algorithm, you choose a feature based on the amount of information gain as opposed to choosing it randomly.",
   "C": "ID3 tree algorithm is harder to compute because it has to recursively select a best feature at each step.",
   "D": "Random Tree algorithm's performance is greatly boosted by using techniques such as bagging.",
   "answer": "A",
   "why": "Random Tree algorithm does not randomly select the route while it's traversing. It rather selects a feature randomly."
 },
 {
   "index": 238,
   "question": "Which one of these learning methods is the fastest at query time?",
   "A": "KNN",
   "B": "Decision tree",
   "C": "linear regression",
   "D": "K-means",
   "answer": "C",
   "why": "linear regression is the fastest at query time because once you calculate the parameters, you just need to solve the equation of X-s and find the answer. But in case of K nearest neighbor, you need to consult with all the data points and calculate the distance to all of them and take the mean of the ones that are closest so its going to be really slow. For decision tree the query time is of log2(n) so it depends on how many data points you have."
 },
 {
   "index": 239,
   "question": "In which case is it better to use a regression learner than a classification learner?",
   "A": "Determining the air speed velocity of an unladen swallow based on factors such as body mass and atmospheric conditions",
   "B": "Determining an undergraduate student's standing such as freshman, sophomore, junior, or senior",
   "C": "Determining the risk of a potential insuree on a scale from 1 to 10",
   "D": "Determining if it will rain today",
   "answer": "A",
   "why": "Regression returns continuous values while classification gives discrete values. Option A has the potential to be any real number. The other options have discrete labels that apply to them such as a single number from the set 1 to 10 or yes or no."
 },
 {
   "index": 240,
   "question": "Your boss gives you a dataset of daily returns for a given stock, and daily factor value data (n number of arbitrary factors) corresponding to that stock. He asks that you build a predictive model using the factor values at time t corresponding to the returns at time t+1. Your boss intends to query this model at future days with that days’ factor values to get some numerical prediction for the stock’s next day returns. Most generally, what kind of learning problem can your task be described as:",
   "A": "Classification",
   "B": "Parametric",
   "C": "Regression",
   "D": "Non-Parametric",
   "answer": "C",
   "why": "C, Regression. Given that your model will be used for ‘numerical prediction’, we can determine that this is a regression problem. Parametric/Non-Parametric simply describe different types of models you can use to solve a learning problem, but they don’t describe what type of learning problem it is (although the uninformed test taker might be inclined to answer ‘parametric’). Classification problems are those where you are trying to classify an object into a type – clearly not what your boss is asking you to do.  "
 },
 {
   "index": 241,
   "question": "Which of the following statement is correct regrading the properties of KNN, linear regression and decision tree in general ?",
   "A": "Linear regression is generally used to handle classification problems.",
   "B": "Decisions tree has the lowest cost of learning (training).",
   "C": "KNN has the highest cost of query.",
   "D": "Linear regression requires the least effort to handle missing data.",
   "answer": "C",
   "why": "A is wrong --> linear regression generally handles regression problem  B is wrong --> KNN has the lowest cost of learning, in fact KNN does not require an explicit learning step.  C is correct --> Need pair-wise computation to get distance from all entries in dataset.  D is wrong --> Decision tree requires the least effort to handle missing data."
 },
 {
   "index": 242,
   "question": "Overfitting happens when a model learns the nuances of the training data too well. The reason for this is:",
   "A": "A lack of generalization.",
   "B": "Too much training data.",
   "C": "Invalid training data.",
   "D": "Incorrect algorithm for the data.",
   "answer": "A",
   "why": "It is important for a ML algorithm to be able to generalize. We are not selecting the perfect answer, but the best answer from the data provided and what we've seen before."
 },
 {
   "index": 243,
   "question": "You were asked to develop a demo learner app as quickly as possible. Soon you realize the given dataset is not so big, that makes you naturally concerned about overfitting. Given this short notice you will prefer",
   "A": "KNN because it can handle smaller data sets without overfitting at no training cost",
   "B": "Linear Regression because it generalizes smaller datasets well and easy to train",
   "C": "Decision trees because it can handle smaller data sets without overfitting better than KNN",
   "D": "Either B or C",
   "answer": "B",
   "why": "Linear Regression because it generalizes smaller datasets well and easy to train than decision trees. Both KNN and Decision trees suffers from overfitting problem for smaller datasets."
 },
 {
   "index": 244,
   "question": "How is a random tree built using A. Cutler's approach different than a standard decision tree using JR Quinlan's approach?",
   "A": "Building a random tree entails randomizing the association between each input test point and the label associated with it in the training set whereas for a standard tree, each input point and its label remains intact.",
   "B": "It is much more difficult to build a random tree that learns quickly than to build a deterministic tree that learns quickly using JR Quinlan's approach.",
   "C": "When determining where to split, in the random tree approach of Cutler two factors are randomly selected whereas in Quinlan's approach a method  using a measure of information gain such as entropy is used to determine the best factors to split on.",
   "D": "In a decision tree built by Quinlan's approach factors can be repeated but in the random tree generated by Cutler's method a factor can only appear once.",
   "answer": "C",
   "why": "A.  Randomization occurs in selecting factors and split values, not in randomly scrambling the input data, so this is false.  B.  It is much easier to build a random tree because meaures of information gain do not have to be studied and implemented.  C.  Correct answer.  Random trees built by Cutler's method are fast learners due selecting the factors and their split values randomly, whereas the standard tree relies on measures of information gain which are more complex to build and analyze and run more slowly.  D.  Factors can appear multiple times in either algorithm."
 },
 {
   "index": 245,
   "question": "Which of the following best describes the definition of “overfitting”?",
   "A": "Error is decreasing for both in-sample (training) data and out-of-sample (test) data.",
   "B": "Error is decreasing for in-sample (training) data and increasing for out-of-sample (test) data.",
   "C": "Error is increasing for in-sample (training) data and decreasing for out-of-sample (test) data.",
   "D": "Overfitting can only occur on kNN Learners and is not an issue for other machine learning algorithms.",
   "answer": "B",
   "why": "Overfitting is when a machine learning algorithm’s model is too specific to its training data.  So, it’s the area where error is continuing to decrease on that training data but error is increasing for unseen test data, as the learner’s model does not represent that data well."
 },
 {
   "index": 246,
   "question": "How is a random tree built using A. Cutler's approach different than a standard decision tree using JR Quinlan's approach?",
   "A": "Building a random tree entails randomizing the association between each input test point and the label associated with it in the training set whereas for a standard tree each input point and its label remains intact. ",
   "B": "It is much more difficult to build a random tree that learns quickly than to build a deterministic tree that learns quickly using JR Quinlan's approach.",
   "C": "When determining where to split, in the random tree approach of Cutler two factors are randomly selected whereas in Quinlan's approach a method  using a measure of information gain such as entropy is used to determine the best factors to split on.",
   "D": "In a decision tree built by Quinlan's approach factors can be repeated but in the random tree generated by Cutler's method a factor can only appear once.",
   "answer": "C",
   "why": "Random decision trees using A. Cutler's method are very fast learners because of the random factor selection as opposed to determining a \"best\" factor to split on using entropy or correlation used when building a tree using Quinlan's approach."
 },
 {
   "index": 247,
   "question": "You are given a task to build 2 different (regression) decision trees based on the same data set. Method A: A decision tree that is built in shortest possible time, Method B: A decision tree that is most balanced. Which of the following ‘pair’ of statements for the above mentioned methods is most likely to be true regarding the criteria for choosing the (i) ‘feature to split on’ and (ii) ‘split value’ for that feature while building the respective decision trees?",
   "A": "Method A: (i) choose randomly, (ii) choose randomly; Method B: (i) choose randomly, (ii) choose the median value",
   "B": "Method A: (i) choose randomly, (ii) choose randomly; Method B: (i) based on the highest correlation, (ii) choose the median value",
   "C": "Method A: (i) based on the highest correlation, (ii) choose the median value; Method B: (i) choose randomly, (ii) choose the median",
   "D": "Method A: (i) based on the highest correlation, (ii) choose the median value; Method B: (i) choose randomly, (ii) choose randomly",
   "answer": "B",
   "why": "Correct Answer: B; Selecting the feature randomly and then choosing the split value randomly will build the decision tree fastest time (based on Adele Cutler’s approach) and splitting the feature based on correlation followed by choosing the split value based on median might take longer to build but will build a tree that will be very close to being balanced (based on Ross Quinlan’s approach)."
 },
 {
   "index": 248,
   "question": "Aziz has been tweaking his decision tree algorithm to improve performance. In his most recent change, he decided to test each input parameter (X_i) in sequence at each successive level of the tree (i.e. parameter 0 at the root, parameter 1 at level 1, parameter 2 at level 2 and so on, looping back as needed). Previously Aziz had chosen which parameter to test at each branch by selecting the one with the highest correlation to Y values for the remaining training data. When he updates his decision tree in this way Aziz is developing… ",
   "A": "a smaller decision tree memory footprint",
   "B": "more predictive accuracy for queries",
   "C": "a faster decision tree query time",
   "D": "a faster decision tree building time",
   "answer": "D",
   "why": "Determining correlation has been completely eliminated for all parameters at every branch of the tree. Since correlation must be recalculated over all records within a branch/subtree, eliminating it in this way greatly speeds up the tree construction time. The minor downside is that it could result in an unbalanced tree."
 },
 {
   "index": 249,
   "question": "Which of the following statements comparing parameterized models to K-nearest neighbors (as an example of instance-based models) is FALSE?",
   "A": "For very large data sets, KNN will usually have a much faster learning rate than an instance-based model.",
   "B": "Running a query for KNN is typically much faster than running one on a parametized model.",
   "C": "KNN implementations will often take up very large amounts of storage compared to parameterized models.",
   "D": "The cost of updating a KNN dataset with new instances is very low compared to adding new data to a parameterized model.",
   "answer": "B",
   "why": "B is the correct answer because all of the heavy processing for KNN happens at query time. Learning is basically just loading up the data (which rules out A and D, because both are true). C is true because optimal KNN requires remembering all of the data (though there are ways of giving up some accuracy to reduce storage) while a parameterized model can work fine just by remembering the values the optimizer came up with for the equation. It doesn't have to remember all of the data used to generate the equation (ruling out C)."
 },
 {
   "index": 250,
   "question": "What is the output of this python code?    array = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])  array = array[:3, :3]  array = array[1:, 1:]  print(array)",
   "A": "[[ 6,  7,  8]   [10, 11, 12]   [14, 15, 16]]",
   "B": "[[ 11 ]]",
   "C": "[[ 6,  7]   [10, 11]]",
   "D": "[[ 0, 0, 0, 4]   [ 0, 0, 0, 0]   [ 0, 0, 0, 0]   [13, 0, 0, 0]]",
   "answer": "C",
   "why": "-> np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])    [[ 1,  2,  3,  4]   [ 5,  6,  7,  8]   [ 9, 10, 11, 12]   [13, 14, 15, 16]]    -> array[:3, :3]    [[ 1,  2,  3]   [ 5,  6,  7]   [ 9, 10, 11]]    -> array[1:, 1:]    [[ 6,  7]   [10, 11]]"
 },
 {
   "index": 251,
   "question": "Which of the following three models, kNN, decision trees, and linear regression, can be guaranteed to achieve perfect in-sample prediction accuracy?  ",
   "A": "kNN, if we set k (the number of nearest neighbors to use), to equal the number of training observations.  ",
   "B": "Decision Trees, if we set the leaf size (the maximum number of observations to be use at a leaf) to equal the number of training observations  ",
   "C": "Linear Regression, if we use a second order model to capture any curvature of the data  ",
   "D": "Both Knn and Decision Trees if we set k =1 and leaf_size = 1   ",
   "answer": "D",
   "why": "For Knn with k=1, the closest observation to every in-sample query point will be the point itself, so it will output the same value. For decision trees with leaf_size =1, the tree will be expanded until it perfectly fits the in-sample data and every in-sample query point will be represented in the tree. Choice A is wrong because k=n (n= training observation) will always output the mean, however if the student is not familiar with the algorithm this answer may be appealing.  Choice B is wrong since increasing the leaf size usually increases in-sample error. Choice C is wrong because a polynomial linear regression will only fit the data perfectly if there are three observations. It is possible for a linear regression to achieve perfect in-sample accuracy, but only if we include a degree n-1 polynomial so students who do not understand linear regressions may choose choice C.   "
 },
 {
   "index": 252,
   "question": "Which of the following are true for ID3 algorithm of building a decision tree?",
   "A": "ID3 algorithm solves overfitting problem by bottom-up technique usually known as \"pruning\"",
   "B": "It can handle numeric attributes and missing values",
   "C": "Uses information gain as splitting criteria",
   "D": "All of the above",
   "answer": "C",
   "why": "Pruning, handling of numeric attributes and missing values were introduced in C4.5 algorithm which was an extension of the ID3 algorithm. ID3 algorithm uses information gain for splitting criteria."
 },
 {
   "index": 253,
   "question": "Why is roll forward cross validation used instead of standard cross validation in financial applications?",
   "A": "Roll forward cross validation is a better measure of the quality of predictions than cross validation, especially for financial applications.",
   "B": "Roll forward cross validation takes lesser time to compute than cross validation.",
   "C": "Standard cross validation might produce unrealistically good results for quantitative data. ",
   "D": "Roll forward cross validation uses more trials than standard cross validation, thereby improving the measurement of prediction quality.",
   "answer": "C",
   "why": "Standard cross validation allows \"peeking into the future\", because random selection of train and test sets might pick data points from the future for the training set."
 },
 {
   "index": 254,
   "question": "Which of the following would NOT be an advantage of KNN in comparison to Linear Regression?",
   "A": "Weaker assumptions about data",
   "B": "Total Memory requirements",
   "C": "Ability to adapt to new data",
   "D": "Not having to worry about choosing just the right features of the training data",
   "answer": "B",
   "why": "KNN, an instance based model also known as 'Memory Based Learning' requires more memory than parameterized models like Linear Regression. The extra memory requirement is used to store problem instances. It keeps training data and uses it to compare to new problem instances. "
 },
 {
   "index": 255,
   "question": "If two variables are highly correlated, what does that tell you about the two variables?",
   "A": " A. that their R^2 value must be >0.7  ",
   "B": " B. that low values on one variable will lead to low values on the other  ",
   "C": " C. that there are no other variables responsible for the relationship   ",
   "D": " D. that changes in one variable are accompanied by predictable changes in the other ",
   "answer": "D",
   "why": "D is the correct answer.  A is wrong, R^2 can be negative and highly correlated  B is wrong, same reason as A, it can be a negative correlation  C is wrong, assumption is incorrect, it is possible to have correlation with other variable"
 },
 {
   "index": 256,
   "question": "Which of the following is a correct set of supervised and unsupervised learning problems?    1. Predicting the origin of wines using chemical analysis.  2. Differentiating among three species of flowers using physical characteristics.  3. Determining the species of mushroom using physical characteristics.  4. Anticipating the quality of wines using several features.  5. Arranging the same type of fruit at one place among four kinds of fruits using physical characteristics.  ",
   "A": "Supervised: 1, 3, 5  Unsupervised: 2, 4  ",
   "B": "Supervised: 1, 4, 5  Unsupervised: 2, 3  ",
   "C": "Supervised: 1, 3, 4  Unsupervised: 2, 5  ",
   "D": "Supervised: 1, 2, 4  Unsupervised: 3, 5  ",
   "answer": "C",
   "why": "1. Supervised learning problem. Chemical analysis data are used to predict the label. (origin of wines)  2. Unsupervised learning problem. Clustering has to be used to differentiate among three species of flowers.  3. Supervised learning problem. Physical characteristics are used to determine the label. (species of mushroom)  4. Supervised learning problem. Several features are used to anticipate the label. (quality of wines)  5. Unsupervised learning problem. Clustering has to be used to arrange the same type of fruit at one place.  "
 },
 {
   "index": 257,
   "question": "Which of the following is a correct set of supervised and unsupervised learning problems?    1. Predicting the origin of wines using chemical analysis.  2. Differentiating among three species of flowers using physical characteristics.  3. Determining the species of mushroom using physical characteristics.  4. Anticipating the quality of wines using several features.  5. Arranging the same type of fruit at one place among four kinds of fruits using physical characteristics.",
   "A": "Supervised: 1, 3, 5  Unsupervised: 2, 4",
   "B": "Supervised: 1, 4, 5  Unsupervised: 2, 3",
   "C": "Supervised: 1, 3, 4  Unsupervised: 2, 5  ",
   "D": "Supervised: 1, 2, 4  Unsupervised: 3, 5  ",
   "answer": "C",
   "why": "1. Supervised learning problem. Chemical analysis data are used to predict the label. (origin of wines)  2. Unsupervised learning problem. Clustering has to be used to differentiate among three species of flowers.  3. Supervised learning problem. Physical characteristics are used to determine the label. (species of mushroom)  4. Supervised learning problem. Several features are used to anticipate the label. (quality of wines)  5. Unsupervised learning problem. Clustering has to be used to arrange the same time of fruit at one place."
 },
 {
   "index": 258,
   "question": "What is the key difference in the way a random tree is implemented that leads to faster training times over a classic decision tree?  ",
   "A": "The use of the numpy library is what makes random trees perform faster.",
   "B": "A random tree selects random features to split the data, while a classic decision tree must determine the feature that is most closely correlated to the labels to use as the factor to split on.",
   "C": "Random trees only select 60% of the data at random for training while classic decision trees use all of the data for training.",
   "D": "Random trees have a much smaller leaf size making them perform faster than a classic decision tree.",
   "answer": "B",
   "why": "Selecting a random feature to split the data is the key difference between random trees and classic decision trees. By not having to measure correlation to the labels for thousands of data points and just selecting a random feature, training times are sped up considerably."
 },
 {
   "index": 259,
   "question": "Which of these statements are true?  I. K-NN takes less time to train, but more time to query than decision trees  II. Decision trees take less time to train, but more time to query than K-NN  III. Decision trees are more prone to overfit than K-NN  IV. K-NN is more prone to overfit than decision trees",
   "A": "I & III",
   "B": "II & III",
   "C": "I & IV",
   "D": "II & IV",
   "answer": "A",
   "why": "This is based on the properties of the three types of learners:    Statement I is true: K-NN is the quickest to build and the most expensive to query among the three types of learners, and decision trees fall in the middle on both criteria.    Statement II. is false because Statement I is true.    Statement III. is true because decision trees have a higher tendency to specialize on the training data, especially with smaller leaf sizes, whereas K-NN is susceptible to overfitting only for very small values of K.    Statement IV. is false because Statement III is true.    Therefore, A. I & III is the only right answer."
 },
 {
   "index": 260,
   "question": "Which of the following is a correct set of supervised and unsupervised learning problems?    1. Predicting the origin of wines using chemical analysis datas.  2. Differentiating among three species of flowers using physical characteristics.  3. Determining the species of mushroom using physical characteristics.  4. Anticipating the quality of wines using several features.  5. Arranging the same type of fruit at one place among four kinds of fruits using physical characteristics.",
   "A": "Supervised: 1, 3, 5  Unsupervised: 2, 4",
   "B": "Supervised: 1, 3, 5  Unsupervised: 2, 4",
   "C": "Supervised: 1, 3, 4  Unsupervised: 2, 5",
   "D": "Supervised: 1, 2, 4  Unsupervised: 3, 5",
   "answer": "C",
   "why": "1 - Supervised learning problem, chemical analysis datas are used to predict the label. (origin of wines)  2. Unsupervised learning problem, clustering has to be used to differentiate among three species of flowers.  3. Supervised learning problem, physical characteristics are used to determine the label. (species of mushroom)  4. Supervised learning problem, several features are used to anticipate the label. (quality of wines)  5. Unsupervised learning problem, clustering has to be used to group the same time of fruit at one place."
 },
 {
   "index": 261,
   "question": "When using Numpy.corrcoef() to measure the quality of predictions, it returns values in the range of -1 to 1. Which of the following interpretations of the range of values is correct about the correlation? ",
   "A": "0 is good correlation, numbers approaching 1 and -1 are not good",
   "B": "-1 is good correlation, 1 is not-good, 0 is inconclusive",
   "C": "1 is good correlation, -1 is not good, 0 is no or poor correlation",
   "D": "Numbers approaching 1 and -1 are good correlation, 0 is not good",
   "answer": "C",
   "why": "The correlation value describes the closeness of the data when plotting Ytest vs. Ypredict as well as the slope of the line. Positive numbers reflect the correct slope of the line."
 },
 {
   "index": 262,
   "question": "What is overfitting?",
   "A": "Overfitting is when a model trusts training data too less so that the error of the model on testing data is always over 0.5%(RMSE).  ",
   "B": "Overfitting is when a model trusts training data too much so that the model also captures noise.",
   "C": "Overfitting is when a model trusts training data too much so that the model always works very well on testing data.",
   "D": "Overfitting is when a model trusts training data too less so that the model captures no noise.",
   "answer": "B",
   "why": "Overfitting is when a model trusts training data too much so that the model also captures noise. And this leads to poor performance on testing set."
 },
 {
   "index": 263,
   "question": "Overfitting is made worse by",
   "A": "Fitting too many bags into your Random Tree",
   "B": "Having training data which fits too perfectly",
   "C": "Increasing the leaf size of a Random Tree",
   "D": "Using an forest of trees",
   "answer": "B",
   "why": "A is nonsense, but might make someone pause if they didn't know the material. B is the correct answer, because making your training data fit too perfectly causes testing data to have a higher error rate. C and D, of course, help generalize your predictions, decreasing the overfit error."
 },
 {
   "index": 264,
   "question": "What would be the optimal algorithm for a solution that requires fast training time.     a.\tkNN  b.\tDecision Tree  c.\tLinear Regression   d.\tNeural network  ",
   "A": "kNN  ",
   "B": "Decision Tree  ",
   "C": "Linear Regression   ",
   "D": "Neural network  ",
   "answer": "C",
   "why": "A linear regression algorithm would be the best choice based on its minimal training costs.    "
 },
 {
   "index": 265,
   "question": "In AdaBoost, what kind of learners are usually combined for producing the boosted output?",
   "A": "Strong learners",
   "B": "Weak Learners",
   "C": "Both strong and weak learners have equal weightage",
   "D": "None of the above",
   "answer": "B",
   "why": "Weak learners - The output of the weak learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier."
 },
 {
   "index": 266,
   "question": "Which one of the following is a regression problem?",
   "A": "An unknown animal has four legs, a long neck, 10 feet high and only eats grass. What animal is it?",
   "B": "I am considering to purchase a laptop computer with X gigs of disc space, with a Y inch screen, from Z manufacture. How much should it pay?",
   "C": "I have a fruit that is round shaped, red color and starts with an alphabet a. What fruit is it?",
   "D": "This country celebrates Christmas in summer season, speaks English, has multiple time zones. What is this country?",
   "answer": "B",
   "why": "B is the correct answer because it is a question for approximating a numeric value, whereas other choices ask for identifying a class like the name of a fruit."
 },
 {
   "index": 267,
   "question": "Boosting is a variation of bagging in that it:",
   "A": "Focuses on areas where bagging is not performing well by taking data points where the previous model has not performed well in and weights those data points more heavily in the subsequent models",
   "B": "Focuses on areas where bagging is not performing well by only including models where its in-sample error is below a certain threshold and each subsequent model's error is better than the previous",
   "C": "Focuses on areas where bagging is not performing well by generating models where the data is sampled without replacement",
   "D": "Boosting is implemented the same as any bagging algorithm where all models are generated at once.",
   "answer": "A",
   "why": "Boosting is a variation of bagging in that it is a step-wise ensemble method focused on areas where the models have not performed well and weights those areas in each subsequent model. All other answer incorrectly describe boosting algorithm"
 },
 {
   "index": 268,
   "question": "When we are doing the very simple bagging for a certain model, we would like to to \"bag\" the training dataset into smaller ones. For one data point A, where can we find it?",
   "A": "Only in one certain bag for 1 time",
   "B": "In one bag but several times",
   "C": "In different bags",
   "D": "All of above",
   "answer": "D",
   "why": "When we are doing bagging, the randomly choosing of data should be with replacement, so that one data can be bagged into different bags, or same bag many times. This is to avoid some potential bias on the training dataset."
 },
 {
   "index": 269,
   "question": "In Machine Learning, why is overfitting considered bad?",
   "A": "A model that overfits \"memorizes\" the relationship rather then \"learning\" the relationship.",
   "B": "A model that overfits learns the \"noise\" in the dataset.",
   "C": "A model that overfits violates Occam's razor principle.",
   "D": "All of the above.",
   "answer": "D",
   "why": "Overfit models exhibit all of the above symptoms:  - They do not generalize but memorize the relationship  - They cannot effectively filter out noise from the relationship  - They violate Occam's razor principle"
 },
 {
   "index": 270,
   "question": "Between KNN, Linear Regression, and Decision Trees, select which machine learning method you would use for each for the following scenarios: 1) to minimize query time, 2) is the easiest method to incrementally add new training data to an existing model, 3) to minimize the space required to store the model, 4) to minimize training time.",
   "A": "1) Decision Trees, 2) KNN, 3) Decision Trees, 4) KNN",
   "B": "1) Linear Regression, 2) KNN, 3) Linear Regression, 4) KNN",
   "C": "1) Linear Regression, 2) Decision Trees, 3) Linear Regression, 4) Linear Regression",
   "D": "1) Decision Trees, 2) Linear Regression, 3) Decision Trees, 4) Linear Regression",
   "answer": "B",
   "why": "1) Linear Regression is the fastest querying method as it is just an O(1) multiplication and addition of values. Decision trees require O(log(n)) search, and KNN requires comparing distance functions of many points  2) KNN is the easiest learner to add data to the model, as the data point is just added to a database or index structure. There are incremental algorithms for regression and decision trees, but they are not as simple as KNN  3) Linear Regression minimizes model space as it only needs to store coefficients of the model. Decision trees can be very large and KNN needs to store many data points  4) KNN can be considered to have the minimum training time because it only needs to store data points during the training phase. It doesn't need to select features like decision trees or fit a line like linear regression"
 },
 {
   "index": 271,
   "question": "Which of the those is a key factor in Parametric Models?",
   "A": "Previous Predictions of instances of the problem.",
   "B": "Mean (Expectation) of distribution of data points.",
   "C": "Learn the coefficients for the function from the training data.",
   "D": "A lot of data and no prior knowledge.",
   "answer": "C",
   "why": "The parametric models tries to form a function then learn the coefficients for this function using the training data. choices A and D are correct for instance based models, While B is just a random choice out of subject."
 },
 {
   "index": 272,
   "question": "Which of the following statements best describe Boosting?  ",
   "A": "It is an ensemble method by using weighted sampling and increasing weights of examples that were wrongly classified in previous learner.",
   "B": "It is an ensemble method by randomly selecting data with replacement.  ",
   "C": "t trains multiple models on equally weighted samples and average their predictions",
   "D": "It avoids overfitting by increasing iterations of learners.  ",
   "answer": "A",
   "why": "B is wrong in terms of randomly  C is wrong in terms of equal weight  D is wrong since overfitting might occur as iteration increases.  "
 },
 {
   "index": 273,
   "question": "Which of the following statements is true",
   "A": "Parameterized models are biased whereas instance based models are free from all biases",
   "B": "Instance-based models need more storage space than parameterized models",
   "C": "Parameterized models are slower to run than instance-based models",
   "D": "Instance-based models are slower to train than parameterized models",
   "answer": "B",
   "why": "Instance based models are lazy-learners and hence store all of the training data, which requires more storage space than parameterized models which only store the parameters of the model and throw away the training data after the model has been built."
 },
 {
   "index": 274,
   "question": "Which of the following statements is FALSE about Boosting?",
   "A": "It is an ensemble method  ",
   "B": "It improves the accuracy of the base model  ",
   "C": "It runs multiple times on testing dataset",
   "D": "It re-weights training dataset",
   "answer": "C",
   "why": "It runs multiple times on training dataset, not testing dataset."
 },
 {
   "index": 275,
   "question": "Which of the following is not a possible cause for overfitting?",
   "A": "Too much noise is presented in the sample.",
   "B": "Limited number of input variables are given in the sample.",
   "C": "Limited number of input data are presented in the sample for learning.",
   "D": "Pruning process is ignored when applying decision tree algorithm.",
   "answer": "B",
   "why": "\"B. Limited number of input variables are given in the sample.\" is not a cause for overfitting. Just the opposite, too many variables without proper selection will always lead to overfitting. A,C and D are all common causes for overfitting. "
 },
 {
   "index": 276,
   "question": "While building a decision tree, which of the following approaches in selecting the best feature to split on is the fastest?",
   "A": "Entropy",
   "B": "Correlation",
   "C": "Gini Index",
   "D": "Random ",
   "answer": "D",
   "why": "A,B,C & D all can be used to determine the best feature to split on and hence all seem plausible. However, A,B & C require low to high mathematical computation on the entire input data-set (at least O(N)) whereas D requires simple random number generation between a given range."
 },
 {
   "index": 277,
   "question": "Which of the following techniques can be used to reduce overfitting in decision trees?",
   "A": "Post-pruning",
   "B": "Brining",
   "C": "Growing",
   "D": "Randomizing ",
   "answer": "A",
   "why": "As per Mitchell (p.77),  post pruning the decision tree are therefore important to avoid overfitting in decision tree learning."
 },
 {
   "index": 278,
   "question": "Which of the following is an example of an unsupervised learning problem?",
   "A": "Grading wine quality on a scale of 1-6 based on a number of features",
   "B": "Grouping stocks together based on similarity to each other",
   "C": "Determining the classification of an animal based on the answers to 20 questions about that animal",
   "D": "Predicting the price of a stock based on historial data on that stock",
   "answer": "B",
   "why": "A is a supervised learning problem where features are used to predict a label.  B is the only one that is not trying to find a specific target variable. It is clustering stocks together based on similarity (K Means) and not classifying them with specific labels in mind.  C is a supervised learning problem where the features are the answers to the questions which are used to determine the type of animal.  D is a supervised learning regression problem where the features are used to predict a continuous value."
 },
 {
   "index": 279,
   "question": "Regression is a better method when the output is a ______ variable, whereas Classification is the better method when output is a _______ variable. ",
   "A": "continuous,  continuous",
   "B": "continuous, categorical",
   "C": "categorical, categorical ",
   "D": "categorical, continuous",
   "answer": "B",
   "why": "When trying to determine whether to use a regression method, or a classification method an important consideration is the required output. Classification methods are used when we would like to find similarities in the data and make predictions based on those similarities therefore it produces categorical output. Regression on the other hand uses the data to estimate parameters to predict a relationship between the data and the response, therefore the output is a continuous variable. "
 },
 {
   "index": 280,
   "question": "Which of the following would be a good reason for choosing a parametric-based learner?",
   "A": "New evidence needs to be easily integrated into the model",
   "B": "The problem space is complex and not well understood",
   "C": "Demanding query performance requirements",
   "D": "Model will have unlimited storage space available",
   "answer": "C",
   "why": "The correct answer is (c).  Parametric learners are typically slower to train, but have very fast query response times. Answers a), b), and d) all describe reasons for choosing an instance-based learner."
 },
 {
   "index": 281,
   "question": "The primary difference between K nearest neighbor (KNN) and Kernel regression is:",
   "A": "KNN data points are weighted based on distances between points, whereas Kernel regression data points are equally weighted.",
   "B": "Kernel regression data points are weighted based on distances between points, whereas KNN data points are equally weighted.",
   "C": "KNN and Kernel regression data points are both weighted in the same way, data points considered are equally weighted.",
   "D": "KNN and Kernel regression data points are both weighted in the same way, data points considered are weighted based on distances between points.",
   "answer": "B",
   "why": "Correct answer is B) because KNN weighs points equally and Kernel regression uses the distances between considered points to assign weighted values."
 },
 {
   "index": 282,
   "question": "Lets take an example of a problem where given data about the size of houses and historic prices in the real estate market, you need to predict their future prices (assuming all other factors are constant). Which Machine learning method would be best suitable to solve this ?",
   "A": "Supervised Machine Learning with classification",
   "B": "Supervised Regression Learning",
   "C": "Unsupervised Machine Learning",
   "D": "Partial Supervised Learning",
   "answer": "B",
   "why": "You can test your model using past data so this is definately an example of Supervised Learning and there is no data classification involved in this example. "
 },
 {
   "index": 283,
   "question": "Which of the following statements is true about overfitting?",
   "A": "Overfitting occurs in regions where the in-sample error is increasing and the out-of-sample error is decreasing.",
   "B": "A random tree learner with leaf size 1 is more likely to be overfit than one with leaf size 50.",
   "C": "A k-nearest neighbor model with a k value of 50 is more likely to be overfit than one with a k value of 1.",
   "D": "A polynomial model of degree 1 is more likely to be overfit than one of degree 50.",
   "answer": "B",
   "why": "A single random tree learner with leaf size 1 will fit the training data exactly, which makes it likely to be overfit.    A, C, and D are all false:  A. Overfitting occurs in regions where the in-sample error is decreasing and the out-of-sample error is increasing.  C. A k-nearest neighbor model with a k value of 1 is more likely to be overfit than one with a k value of 50.  D. A polynomial model of degree 50 is more likely to be overfit than one of degree 1."
 },
 {
   "index": 284,
   "question": "Which of the following learners is not susceptible to overfitting?",
   "A": "Linear Regression",
   "B": "Random Tree",
   "C": "Ensemble learner using boosting",
   "D": "A bag learner composed of random tree learners",
   "answer": "D",
   "why": "Linear regression models are susceptible to overfitting as the degree of polynomials increases. Random tree learners overfit when the leaf size of the learner is small. Ensemble learners using boosting tend to overfit as the number of bags increases. According to Leo Breiman, random forests (bag learners composed of random tree learners) do not overfit."
 },
 {
   "index": 285,
   "question": "In Ada boosting, what should we do with the points that are not well predicted in the first bag?",
   "A": "Weight more of these data, making them more likely to be picked in the next bag.",
   "B": "Weight less of these data, making them less likely to be picked in the next bag.",
   "C": "Do nothing.",
   "D": "Delete these data. ",
   "answer": "A",
   "why": "We should weight the data according to the error, let the model to be more representative."
 },
 {
   "index": 286,
   "question": "Which ensemble method strives to improve the learner by focusing on areas where the system is not performing well?",
   "A": "Bagging",
   "B": "Ada Boost",
   "C": "Overfitting",
   "D": "Underfitting",
   "answer": "B",
   "why": "Ada Boost is an improvement upon the Bagging ensemble learner. While training and testing with Bagging, some data points will have significant errors. Those data points with errors are more likely to be picked for the next bag when using Ada Boost, then a new model is built and tested. This is how Ada Boost focuses on areas where the system is not performing well."
 },
 {
   "index": 287,
   "question": "Which best characterizes a balanced decision tree?",
   "A": "Slower to query than a random tree",
   "B": "Slower to build than a random tree",
   "C": "Will not overfit training data",
   "D": "Requires more attributes than building a random tree",
   "answer": "B",
   "why": "A perfect decision tree requires calculations to determine the best attributes to split on. Thus, it will take longer to build than a random tree."
 },
 {
   "index": 288,
   "question": "Which of the following is NOT true about overfitting?",
   "A": "Overfitting is more likely to happen when the training examples are rare.",
   "B": "Overfitting should be suspected when the test error is zero.",
   "C": "Decreasing the model complexity would help to ease overfitting.",
   "D": "It is possible that test error can be smaller than training error.",
   "answer": "B",
   "why": "Overfitting should be suspected when the TRAINING error is zero. Because of the increase in complexity, the flexible models in training set cause overfitting and maybe a zero training error. The other options are correct."
 },
 {
   "index": 289,
   "question": "Which decision tree algorithm has the least overfitting?",
   "A": "CART",
   "B": "Random Forest",
   "C": "Boosting",
   "D": "All have same performance in overfitting",
   "answer": "B",
   "why": "Random forest over decision/regression tree mostly lies in the boostrap samplings in both examples and features during training and ensemble average in the end, so overfitting has been eliminated. Boosting are more sensitive than Random Forests to outliers and parameter choices so the overfitting occurs."
 },
 {
   "index": 290,
   "question": "John is working on building a decision tree for a dataset. He can do it in 3 ways.  (a) Build random trees  (b)Build decision tree where Gini index is used for splitting.  (c) Build Baglearner with number of bags= 20 which uses randomly built decision trees.  What would you suggest him if he is most concerned about time taken for learning and wants to speed up the procees",
   "A": " option (a) of building  random decision trees",
   "B": "option (b) of building decision tree using Gini index",
   "C": "option (c) of building bag learner of with number of bags as 20",
   "D": "It cannot be said since all of them take equal time  for the learning step",
   "answer": "A",
   "why": "The Random trees are the fastest way of building trees since we randomly select feature and its value to split on. So, option A is correct  Option b is incorrect as calculating Gini index at each splitting stage takes additional time and is slower as compared to Random Trees.  Option C is incorrect since it involves reforming the dataset for each tree and making 20 decision trees which is much slower compared to making 1 decision tree.  Option D is incorrect since they don't take same amount of time for learning as explained above."
 },
 {
   "index": 291,
   "question": "Which of the statement regarding root square mean error (RMSE) is true?   (n = number of instances and assume n > 1 with some random variables) ",
   "A": "In k nearest neighbors with k=n, RMSE error will be zero for training set.  ",
   "B": "In k nearest neighbors with k=n, RMSE error will be zero for test set.   ",
   "C": "In k nearest neighbors with k=1, RMSE error will be zero for training set.  ",
   "D": "In k nearest neighbors with k=1, RMSE error will be zero for test set.  ",
   "answer": "C",
   "why": "When k=1, RMSE error will be zero for training set as it matches one to one and can query the exact match. When k=n, RMSE error will be the same with RMSE of average for training set. Test sets performance depends on the distribution of test sets. "
 },
 {
   "index": 292,
   "question": "which method is not included in supervised learning?",
   "A": "k nearest neighbor",
   "B": "linear regression",
   "C": "clustering",
   "D": "decision forests",
   "answer": "C",
   "why": "Clustering is a method of unsupervised learning. We can derive the structure by clustering the data based on relationships among the variables in the data without feedback based on the prediction results."
 },
 {
   "index": 293,
   "question": "A single learner running KNN, another running a decision tree model and another running linear regression tree all begin training identical copies of a data set at the same time. Assuming a reasonably large data set, which learner will finish first? Choose the best answer.",
   "A": "KNN",
   "B": "Decision Trees",
   "C": "Linear regression",
   "D": "They will all finish at the same time",
   "answer": "A",
   "why": "KNN has the least learning time cost out of the listed models."
 },
 {
   "index": 294,
   "question": "Which one of the models below is a parametric model?",
   "A": "A K nearest neighbor model with K = 6",
   "B": "A decision tree with 'max tree depth' = 10 and 'leaf_size' = 10",
   "C": "A linear regression model with slope = 4 and intercept = 10.2",
   "D": "A neural network network with 'learning rate'  = 0.1 and 'number of neurons in hidden layer'  = 8",
   "answer": "C",
   "why": "Decision tree, KNN and neural networks are instance based models. On the other hand, a linear regression model is a parameterized model "
 },
 {
   "index": 295,
   "question": "Which of the following is correct about parametric and instance-based models:",
   "A": "Parametric: Linear Regression, Logistic Regression  Instance Based: KNN",
   "B": "Parametric: Linear Regression, KNN  Instance Based: Logistic Regression ",
   "C": "Parametric: KNN, Logistic Regression  Instance Based: Linear Regression",
   "D": "None of the above",
   "answer": "A",
   "why": "Correct classification of parametric and instance-based models.    Parametric models: Logistic Regression, Linear Discriminant Analysis, Perceptron, Naive Bayes, Simple Neural Networks    Instance-Based models: k-Nearest Neighbour (kNN), Learning Vector Quantization (LVQ), Self-Organizing Map (SOM), Locally Weighted Learning (LWL)"
 },
 {
   "index": 296,
   "question": "In general, which of the following models has the slowest query time?",
   "A": "Decision Tree",
   "B": "Once the model has been trained, all queries take the same time.",
   "C": "K-Nearest-Neighbor",
   "D": "Linear Regression",
   "answer": "C",
   "why": "In general, K-Nearest-Neighbor is the slowest to query because all the data is used in calculating the prediction. There may be an edge case (only one data point) where it could match the other models, but in general it will be slowest to query."
 },
 {
   "index": 297,
   "question": "Which of the following is correct for boosting?",
   "A": "Boosting is to use a new algorithm to train data",
   "B": "Boosting can cut overfitting",
   "C": "Boosting can cut errors",
   "D": "Boosting and bagging are totally different",
   "answer": "C",
   "why": "Boosting is focusing on where the training is not performing well, so it can help cut errors"
 },
 {
   "index": 298,
   "question": "What is the most likely reason for overfitting when training a random tree learner?",
   "A": "The number of bags used is too small.",
   "B": "The maximum allowed leaf size is too small.",
   "C": "The training set is too small.",
   "D": "The test set is too small. ",
   "answer": "B",
   "why": "Random trees with small maximum leaf sizes will grow very large in an attempt to perfectly classify every training instance. This will result in a lower training error, but the algorithm will have a harder time generalizing to unseen data. "
 },
 {
   "index": 299,
   "question": "A benefit of creating a decision tree by splitting data with random splits instead of splits based on correlation is:",
   "A": "Random splits prevent overfitting because the random choice of a factor prevents the tree from relying too much on a particular portion of test data.",
   "B": "Random splits are less costly in terms of compute time because correlation calculations are more expensive.",
   "C": "Random splits mirror biological processes and therefore are better for learning.",
   "D": "There are no benefits to splitting data with a random split instead of splits based on correlation.",
   "answer": "B",
   "why": "In the JR Quinlan decision tree algorithm, the correlation has to be calculated for each column. This is an expensive operation. To speed up the algorithm, A Cutler used random splits. Random splits don’t prevent overfitting."
 },
 {
   "index": 300,
   "question": "In terms of the cost of query, which of the following algorithms is considered the be the worst or slowest for a decision tree learner?",
   "A": "Decision Tree",
   "B": "KNN",
   "C": "Linear Regression",
   "D": "They are all equal for terms of cost of query",
   "answer": "B",
   "why": "The system would have to compute the distance from the query to all the individual data points, sort them, and find the nearest K data point"
 },
 {
   "index": 301,
   "question": "A machine learning problem dataset has only two unique values to predict, 0 and 1, which clearly can be applied to a classification algorithm. What is the greatest challenge from using a regression machine learning algorithm on this problem?",
   "A": "A regression algorithm needs more than 2 unique values as the target value to train from",
   "B": "A regression algorithm may result in predictions less than 0 or greater than 1, which may not have any interpretable value",
   "C": "A regression algorithm needs a continuous real number to train on, which integer values 0 and 1 do not fulfill",
   "D": "There is absolutely no reason to apply a regression algorithm, because you can already apply classification algorithms.",
   "answer": "B",
   "why": "Correct answer is B) because A, C, and D are all false and if you require a probability propensity of 0 versus 1, predicted values outside of [0, 1], which many regression algorithms would result in, wouldn't make any sense."
 },
 {
   "index": 302,
   "question": "Which statement below about overfitting is correct?",
   "A": "Overfitting occurs in every model",
   "B": "Overfitting occurs when model fits well with training data but generalizes poorly on testing model.",
   "C": "When overfitting occurs, both testing and training error are relatively low.",
   "D": "It is more likely to overfit the training data when a larger k is used in implementing kNN model.",
   "answer": "B",
   "why": "A) not correct, because in a weak model or model which is not highly complex, there is no overfitting, because both training and testing errors can be high.  B) This is correct. The definition of overfitting.  C) Not correct. When overfitting occurs, training error is low but testing error is high..  D) Not correct. When k is smaller, the overfitting is more likely to occur. When k equals 1, the model fits perfectly on training data, but may perform poorly on testing data."
 },
 {
   "index": 303,
   "question": "How are sampled the instances to fill the third bag D3 in a boosting algorithm ?",
   "A": "Instances are randomly and uniformly sampled from the training set.",
   "B": "Instances are weighted according to the prediction error of the model train on D2 and randomly selected from D2.",
   "C": "Instances are weighted according to the prediction error of the joint model train on D1 and D2 and randomly selected from the training set.",
   "D": "Instances are weighted according to the prediction error of the joint model train on D1 and D2 and randomly selected from D2.",
   "answer": "C",
   "why": "A boosting algorithm focus its training on instances with a significant prediction error of the joint model of all previous bags. To do so instances from the training set wrongly predicted are weighted to be more likely to get selected when creating a new bag."
 },
 {
   "index": 304,
   "question": "Which of the following is not true about RMSE?",
   "A": "RMSE can be interpreted as the standard deviation of the error in observed values vs. predicted values. ",
   "B": "RMSE is used to measure the error in a classification problem, not regression problems. ",
   "C": "A lower RMSE indicates a better fit of a model. ",
   "D": "RMSE gives a disportionate weight to larger errors. ",
   "answer": "B",
   "why": "RMSE is used for regression problems and not classification problems because the RMSE value gives a continuous variable as an answer as opposed to a discrete variable that would be used for a classification problem. "
 },
 {
   "index": 305,
   "question": "Rank the following in decreasing order of query cost: KNN, Decision Trees and Linear Regression.  ",
   "A": "KNN, Decision Trees, Linear Regression\t",
   "B": "Decision Trees, KNN, Linear Regression",
   "C": "Linear Regression, KNN, Decision Trees",
   "D": "KNN, Linear Regression, Decision Trees",
   "answer": "A",
   "why": "Since Linear Regression is parameterized, it only requires a simple computation at query time, hence low query cost. Decision tree requires traversal of the entire tree and hence needs more time. Whereas KNN has a highest query cost as K- nearest neighbours need to be located at query time."
 },
 {
   "index": 306,
   "question": "Which of the following best help to reduce overfitting?",
   "A": "Decrease the amount of training data samples",
   "B": "Use an ensemble learning technique such as bagging",
   "C": "Using a very complicated model that fits our data exactly",
   "D": "all of the above",
   "answer": "B",
   "why": "Decreasing the amount of training data is the opposite of an ensemble learner with bagging.  We want to increase the amount of varying training data samples so using bagging allows us to create more data samples from our training set.  Answer (c) is incorrect because we do not want our model to fit the data exactly as this leads to overfitting and an increase in noise.  Answer (d) is incorrect because (a) and (c) are incorrect."
 },
 {
   "index": 307,
   "question": "When building a decision tree, one of the most important parts of the algorithm is to pick the feature to split on. What of the following is correct regarding decision tree and how to pick the feature to split on in decision tree?",
   "A": "In general, using the correlation between each feature and the predictY is the best way to choose the feature to split on.",
   "B": "The decision tree is considered as parametric machine learning algorithm.",
   "C": "An ideal feature would split the feature space into regions of roughly uniform information.",
   "D": "Apply Bootstrap Aggregation(Bagging) with other machine learning not only reduces error rate but decrease the training time.",
   "answer": "C",
   "why": "(A) is wrong since correlation value of each field does not guarantee the largest information gain.  (B) Decision Tree is non-parametric machine learning algorithm  (C) True  (D) Apply bagging would lengthen the training time."
 },
 {
   "index": 308,
   "question": "Concerning the cost of learning(training) and cost of query with regard to kNN, decision trees, and linear regression learners, which of the following statements is true?",
   "A": "kNN learner is fastest to train, but slowest to query.",
   "B": "Decision tree learner is slowest to train, but fastest to query. ",
   "C": "Linear regression learner is both fastest to train, and fastest to query.",
   "D": "All learners have the same cost of learning(training) and cost of query.",
   "answer": "A",
   "why": "For cost of learning: kNN<Linear Regresion<Decision Tree, and for cost of query: Linear Regression<Decision Tree<kNN"
 },
 {
   "index": 309,
   "question": "What is a reason ensemble learners are beneficial over single learners?",
   "A": "Less overfitting",
   "B": "Lower error",
   "C": "Input from multiple types of learners",
   "D": "All of the above",
   "answer": "D",
   "why": "Using an ensemble learner is beneficial because it combines the usage of multiple types of learners. Every type has their own natural bias, and using an ensemble will give you their strengths (C) while reducing their weaknesses (B). And because it's a group of them, there is less overfitting "
 },
 {
   "index": 310,
   "question": "Given a training set of 6000 samples and testing set of 4000 samples, which of the following is false regarding Bagging?",
   "A": "Data subsets (bags) are collected randomly ",
   "B": "Each bag should contain 4000 samples of data.",
   "C": "Each bag should contain 6000 samples of data.",
   "D": "Bagging improves performance.",
   "answer": "B",
   "why": "Each bag should contain the same number of samples as the training data, which in this case is 6000."
 },
 {
   "index": 311,
   "question": "In Boosting, it's preferred to use ___ learners. For mis-classified data points, the algorithm will apply ___ weights to them.",
   "A": "strong, lower",
   "B": "weak, lower",
   "C": "strong, higher",
   "D": "weak, higher",
   "answer": "D",
   "why": "create strong learners from weak learners.  put more weights to errors."
 },
 {
   "index": 312,
   "question": "Which one of the following statement is NOT true?",
   "A": "For decision tree, we don't have to normalize the data.",
   "B": "The classification accuracy of KNN is worse with larger values of K.",
   "C": "Decision tree cannot handle missing data.",
   "D": "Linear regression performs well with small number of observations.",
   "answer": "C",
   "why": "Decision tree can easily handle missing data."
 },
 {
   "index": 313,
   "question": "What is the range of possible values for the output of a linear correlation calculation?",
   "A": "All numbers >= 0",
   "B": "-2 to 2",
   "C": "All numbers <= 0",
   "D": "-1 to 1",
   "answer": "D",
   "why": "D is correct because by the mathematical definition of linear correlation, the only possible resultant values are in the range -1 to 1, inclusive."
 },
 {
   "index": 314,
   "question": "Which of the following is not true about arbitrage as a hedge fund investing strategy?",
   "A": "Identifies and exploits instances where the market price of an asset has (temporarily) deviated from its true price, or its intrinsic value.",
   "B": "In a reasonably efficient market, opportunities for arbitrage will be fleeting because investors will quickly bid up the price of undervalued assets, and bid down the price of overpriced assets.",
   "C": "Profit margins are large and can be sustained over a long period of time.",
   "D": "Value investing is another type of arbitrage that entails taking long positions in assets that the investor considers underpriced, in the expectation that price will eventually be bid up to the near-true value.",
   "answer": "C",
   "why": "In Arbitrage strategy, the profit margins are small and opportunities are rapidly competed away by other arbitrageurs, hence hedge funds use programmed or high-frequency trading systems to act on opportunities very quickly.    "
 },
 {
   "index": 315,
   "question": "Which of the following is NOT an approach to pick a feature to split on while building a decision tree?",
   "A": "Entropy of Information Gain",
   "B": "Correlation of feature's data",
   "C": "Number of times the feature has been used to split with before",
   "D": "Picking a random feature",
   "answer": "C",
   "why": "As learned in the class, any feature can be used multiple times to split on."
 },
 {
   "index": 316,
   "question": "You are tasked with building a model to predict y-values. The x-values have only 1 degree of freedom. You are given historical data with x-values between 0 and 100 to work with. However, your client wishes to predict y-values for x-values between 150 and 200. Which type of model should you build for the client?",
   "A": "K-nearest neighbor",
   "B": "Decision Tree",
   "C": "Recursive Descent",
   "D": "Linear Regression",
   "answer": "D",
   "why": "Linear regression is the correct answer because it is a parametric model that can be used to extrapolate values beyond the range of inputs whereas an instance based model cannot."
 },
 {
   "index": 317,
   "question": "Select the python function for finding correlation 'c' of 'A' and 'B'. (using numpy)",
   "A": "c=numpy.corr(A,B)",
   "B": "c=numpy.corrcoef(A,B)",
   "C": "cr=numpy.corrcoef(A,B)  c=cr[1,1]",
   "D": "cr=numpy.corrcoef(B,A)  c=cr[1,0]",
   "answer": "D",
   "why": "The cr=bumpy.corrcoef(B,A) creates a matrix of 2*2 where the correlation between A and B is given by cr[1,0] or cr[0,1]"
 },
 {
   "index": 318,
   "question": "What is a reason one might decide to use linear regression instead of kNN for a particular regression problem?",
   "A": "As you increase \"k\", kNN is more likely to overfit the data.",
   "B": "kNN is unable to extrapolate a trend in the values toward the edges of the sample when making a prediction, but linear regression can.",
   "C": "Linear regression uses less computing time to train than kNN.",
   "D": "All of the above",
   "answer": "B",
   "why": "A is false because as you decrease \"k\", kNN is more likely to overfit the data. C is false because kNN essentially requires no training time, whereas linear regression has to find parameters. D is false because A and C are false. B is true because the nearest neighbors in the data set will remain the same no matter how far away you move beyond the edge of the data (the extrapolation is a straight line)."
 },
 {
   "index": 319,
   "question": "Which of the following learning problems is a classification problem?",
   "A": "Determining an optimized portfolio based on daily prices of various stocks",
   "B": "Grading wine quality on a scale from 1 to 6 based on various factors",
   "C": "Guessing an animal based on responses of various yes/no questions",
   "D": "Predicting commodity prices based on daily temperature and precipitation data ",
   "answer": "C",
   "why": "A classification problem has discrete answers.  Of all the questions, only guessing an animal will result in a discrete answer.  All other possible answers have continuous answers."
 },
 {
   "index": 320,
   "question": "Which of the following is true about measuring the quality of predictions (training RMSE and training correlation.",
   "A": "Training data (in-sample) prediction measures indicate the performance of a model better than test data (out-of-sample) prediction measures because they quantify how well a model generalizes.",
   "B": "We should always use model parameters (e.g. degrees of freedom of a polynomial) that result in the smallest in-sample RMSE.",
   "C": "In most cases correlation decreases as RMSE increases. ",
   "D": "The correlation used for measuring the quality of predictions is the measure of the relationship between predicted value Y and the values of our independent variable X (feature set). ",
   "answer": "C",
   "why": "As RMSE increases, which indicates poorer performance correlation decreases. Although, there might be cases where the correlation may not decrease like in a biased model."
 },
 {
   "index": 321,
   "question": "Which of the following statement are false?",
   "A": "Predicting how fast an object will be traveling when it hits the ground dropped from height x is a problem well suited for a parameterized model.",
   "B": "K-nearest neighbor is a type of parameterized model.",
   "C": "Instance-based models typically take more storage space than parameterized models.",
   "D": "Predicting whether or not somebody will like a certain tv show is a problem well suited for an instance-based model.",
   "answer": "B",
   "why": "K-nearest neighbor is actually a type of instance-based model. The other answer choices are true."
 },
 {
   "index": 322,
   "question": "Which of the following is NOT an example of a classification problem?",
   "A": "Type of fruit based determined by sugar content",
   "B": "Flower type based on measurements of the length and width of both the sepal and petal",
   "C": "Credit worthiness of potential borrower",
   "D": "Estimated FICO credit score",
   "answer": "D",
   "why": "FICO score is a continual value, and thus is a regression problem."
 },
 {
   "index": 323,
   "question": "What is the main difference between the Bagging and Boosting algorithms?",
   "A": "Bagging averages the results from the M bags while Boosting only averages the results of the bags that produced the lowest error when tested against the training set.",
   "B": "Bagging produces bags that are randomly drawn while boosting produces bags that draws data proportional to the error associated with it in the last bag. ",
   "C": "Bagging randomly chooses from the M bags while Boosting averages the results.",
   "D": "Bagging picks the result from the bag that produces the lowest error while Boosting averages the results from M bags. ",
   "answer": "B",
   "why": "Boosting's main difference from bagging is that it uses the model from the last bag to determine which data points it performs poorly on and should be emphasized in the new bag. It does this by calculating the error for each data point and randomly chooses the data for the new bag proportional to the error associated with the data point."
 },
 {
   "index": 324,
   "question": "Which of the following would always give a perfect performance (zero RMS error / 100% correlation) for in-sample (i.e. training) data after learning that same training data?",
   "A": "Random Tree Learner (homework version), leaf size = 1;  Linear regression (y=ax+b);",
   "B": "kNN, k=1;  Random Tree Learner (homework version), leaf size = 1;",
   "C": "Linear regression (polynomial the size of the training set);  Random Forest (homework version, with repetitions), leaf_size = 1;",
   "D": "All of the above.",
   "answer": "B",
   "why": "Going over all possibilities:  Linear regression (polynomial the size of the training set): This can work for some algorithms. The model can have enough information to create a polynomial that goes through all training points.  Linear regression (y=ax+b): Nope. Degree 1 polynomial doesn't have enough information.  Random Forest (homework version, with sampling repetitions for bagging), leaf_size = 1: Nope. Not all the training data is used for training, so learner will almost never have enough information and the question asks for \"always\".    kNN, k=1: Yes. Serious overfitting. The prediction will just grab the training point stored in training and return it as is.  Random Tree (homework version), leaf size = 1: Yes. Serious overfitting as well. Each training point has its own leaf in the tree for perfect training set predictions.    Hence, the correct answer is D, \"All of the above\"."
 },
 {
   "index": 325,
   "question": "What is the impact of increasing the number of bags used in Random Tree learner on out of sample RMSE when the leaf size is fixed.  Assume that the same learn data is used for all bags.",
   "A": "Increase",
   "B": "Decrease",
   "C": "Doesn't change much.",
   "D": "It depends",
   "answer": "C",
   "why": "Hence the same learn data is used for all bags, with large test data set there will be no much of a change for our of sample RMSE."
 },
 {
   "index": 326,
   "question": "If we don’t care about the speed, for modeling stock prices with a decision tree, which of the following feature selection method will have the best performance? ",
   "A": "select randomly",
   "B": "select based on Gini impurity (which measures how often a sample is incorrectly labeled)",
   "C": "select based on the correlation between a feature and the target (dataY)",
   "D": "all of above methods will have similar performances",
   "answer": "C",
   "why": "A: false. “We don’t care about the speed”, which is the main advantage of random feature selection  B: false. Stock prices are continuous, we need to use a regression tree  C: best choice in here.  D: false.   "
 },
 {
   "index": 327,
   "question": "Which of the following is NOT true about boosting?",
   "A": "Boosting is an ensemble learner ",
   "B": "Boosting relies on running weak learners in sequence",
   "C": "Boosting increases performance of a weak learner",
   "D": "Boosting solves overfitment issues",
   "answer": "D",
   "why": "Boosting is an ensemble learner which runs weak learners in sequence to improve performance.  However, it does not help with overfitment (and can actually make it worse)"
 },
 {
   "index": 328,
   "question": "Which of the below statements is true about the construction of a random tree?",
   "A": "Not having to normalize the data is one of the benefits of building a random tree.",
   "B": "Splitting on features with the most information gain is the linchpin of the random tree. ",
   "C": "Randomly using samples from both the training and testing sets to build the model works well for random trees, as you can measure the performance of both sets of data concurrently.",
   "D": "When building a random tree, its best to refrain from checking the validity of the split as this adds processing time.",
   "answer": "A",
   "why": "A) is correct and is a key benefit over other algorithms (like KNN)  B) is not correct as we are specifically avoiding splitting on information gain to save algorithm time  C) is not correct as you never want to build the model using the test data, as one needs a dataset that is not represented in the model to test against for out of sample data.  D) is not correct as this could lead to one side of the tree having no leaves."
 },
 {
   "index": 329,
   "question": "Supervised learning differs from unsupervised clustering in that supervised learning requires",
   "A": "a.\tat least one input attribute.",
   "B": "b.\tinput attributes to be categorical.",
   "C": "c.\touput attributes to be categorical.",
   "D": "d.\tat least one output attribute.",
   "answer": "D",
   "why": "Definition of Supervised learning "
 },
 {
   "index": 330,
   "question": "What causes Overfitting?",
   "A": "An Increasing Test Error and Decreasing Training Error",
   "B": "Creating a model to match your training data too closely",
   "C": "Training on your test data",
   "D": "Running KNN with a k of 5",
   "answer": "B",
   "why": "A is how you can tell you have overfit your data. Be is what causes overfitting. The closer you get to modeling your training data exactly, the lower the error on your training set will be and the higher the error when running against your test data (or real data). "
 },
 {
   "index": 331,
   "question": "Which of the following options does NOT improve the performance of decision tree construction? ",
   "A": "Given a relatively large leaf_size value.",
   "B": "Instead of looking for the metric with the best correlation, randomly choose one of the metrics. ",
   "C": "Make the tree as balanced as possible. ",
   "D": "Instead of using the median as the split value, choose the average of two random values. ",
   "answer": "C",
   "why": "For A, it reduces the number of subtrees. For B and D, finding the best correlated metric and median can be time consuming. For C, a well balanced tree requires the split value to evenly divide the tree, which asks for the median of all the metric values. So it takes a lot of time. "
 },
 {
   "index": 332,
   "question": "You are given a set of  data with a known target variable.  Your task is to build a model to predict the target variable.  The target variable is a continuous numerical value.  What type of learning method would be best suited for this task?",
   "A": "Supervised Learning Regression",
   "B": "Supervised Learning Classification",
   "C": "Reinforcement Learning",
   "D": "Unsupervised Learning",
   "answer": "A",
   "why": "target variable is in data set, therefore a Supervised Learning Problem,  target variable is continuous numerical value, regression is best suited for this task."
 },
 {
   "index": 333,
   "question": "You are attempting to add ML capabilities to an embedded system with limited resources. You have a training data set of 10,000 elements and will be able to update remotely. Which type of learner would best fit this system? ",
   "A": "k-Nearest Neighbor",
   "B": "Parametric Regression",
   "C": "Both A and B can be used.",
   "D": "Neither A or B can be used.",
   "answer": "B",
   "why": "The limited resources of the embedded system makes the Parametric Regression method best. Updates to the parameters can be done via update and should take no more memory than when initially designed."
 },
 {
   "index": 334,
   "question": "Which of the following pairs best fill the blank for regression & classification, respectively, in the situation:   \"Forecasting the change in _______ of a stock's closing price from one day to the next.\"",
   "A": "Magnitude & Direction",
   "B": "Direction & Magnitude",
   "C": "Direction & Direction",
   "D": "Magnitude & Magnitude",
   "answer": "A",
   "why": "Magnitude is a continuous number (regression), and direction is a discrete category (classification)."
 },
 {
   "index": 335,
   "question": "Which of the following involves unsupervised learning algorithm?",
   "A": "Estimating stock prices after 5 days based on existing stock prices, volumes and other data.",
   "B": "Predicting the buy/sell recommendations of stocks from an analyst based on stock market data, accounting information, stock news and his previous analysis report.",
   "C": "Guessing the trends (rise/drop) of next day's closing prices of stocks based on today's stock and transaction records.",
   "D": "Separating stocks into two groups with similar characteristics based on stock prices, indicators and other data.",
   "answer": "D",
   "why": "Supervised learning makes use of data to predict the outcome/label, while unsupervised learning tries to find the hidden pattern in the data.    Choice A is a supervised learning, because the algorithm tries to predict the stock price (continuous) based on input data.  Choice B is a supervised learning, because the algorithm tries to predict the recommendations (categorical) based on input data.  Choice C is a supervised learning, because the algorithm tries to predict the the movement of stock price (rise or drop) based on input data (actually it is very similar to Choice A).  Choice D is a unsupervised learning, because the algorithm does not try to predict anything, but instead tries to find the hidden pattern in the data by clustering the stocks into two groups."
 },
 {
   "index": 336,
   "question": "Why is feature selection important when building a decision tree?",
   "A": "We want to find the feature which most closely splits the decision space in half",
   "B": "Features can only be used once, so need to make sure the current split is the best time to use it",
   "C": "If a feature is unused, the decision tree will be sub-optimal",
   "D": "We want to use enough features to ensure a \"statistically significant\" tree",
   "answer": "A",
   "why": "The thing we're optimizing for when selecting a feature during building a decision tree is to split the decision space in half (or get as close as we can to it). This ensures a balanced and shallow (as possible) tree.  B - False, we can split on a feature more than once  C - We don't need to use all features  D - False"
 },
 {
   "index": 337,
   "question": "Which following is correct in terms of adaboost(boosting)?",
   "A": "Overfitting is less likely to occur in adaboost than simple bagging",
   "B": "Adaboost simply draw a random subset of training samples with replacement from the training set",
   "C": "At each iteration of the training process, a weight is assigned to each sample in the training set ",
   "D": "Incorrectly predicted training examples at the previous step would have decreased weights in the next step",
   "answer": "C",
   "why": "because     a): overfitting is more likely to occur in adaboost than simple bagging  b): It is bagging(not boosting) just draw a random subset of training samples with replacement from the training set  d): Incorrectly predicted training examples at the previous step would have increased weights."
 },
 {
   "index": 338,
   "question": "As part of the decision tree overfitting-prevention technique known as 'rule post-pruning', how many rules will be generated for a given tree before the pruning has begun?",
   "A": "The total number of nodes in the tree, including decision and leaf nodes",
   "B": "The height of the tree",
   "C": "The number of decision nodes (excluding leaf nodes)",
   "D": "The number of leaf nodes",
   "answer": "D",
   "why": "Answer d):  According to Chapter 3 in Mitchell, one rule is created for each path from the root node to a leaf node. Since there is only one path from a given leaf to the root, there are as many rules as there are leaf nodes."
 },
 {
   "index": 339,
   "question": "Which of the following is an advantage of a parametric learner?",
   "A": "The model can be queried quickly because it has access to all of the training data.",
   "B": "The model is memory efficient because it only needs to store the parameters and not the training data.",
   "C": "The model can be trained quickly because it requires data sets with a small number of features.",
   "D": "The model can be trained quickly because parameters can be calculated on the fly.",
   "answer": "B",
   "why": "The answer is B. A parametric model takes the training data and generates a mathematical equation that can be used to determine the result of a future data point. Because of this only the parameters need to be stored and the training data can be discarded.     A is incorrect because the model does not have access to all of the training data and if it did, exploring that data to determine the correct result would not be as fast as the parameterized model.    C is incorrect because there is no limit to the number of features    D is incorrect because calculating the parameters on the fly both requires access to the training data and is expensive."
 },
 {
   "index": 340,
   "question": "In general, compared with bagging, which answer about boosting is true?",
   "A": "Boosting has higher bias and higher variance.",
   "B": "Boosting has lower bias and higher variance.",
   "C": "Boosting has higher bias and lower variance.",
   "D": "Depending on the dataset, the bias and variance relation between bagging and boosting will change.",
   "answer": "C",
   "why": "Boosting uses weak performance samples repeatedly. It will increase the bias and decrease the variance from the data selection point of view."
 },
 {
   "index": 341,
   "question": "Which algorithm for building decision trees tends to produce trees the fastest?",
   "A": "CART – Classification and Regression Trees",
   "B": "Random Trees",
   "C": "ID3",
   "D": "C4.5",
   "answer": "B",
   "why": "The correct answer is b) because does not base the feature to split on or the split value on an optimality criteria (such as information gain or correlation) like the other algorithms do."
 },
 {
   "index": 342,
   "question": "Given an small dataset with an extremly large numbers of predictors, which model is supposed to perform better: an instance-based model or an parametric model?",
   "A": "Instance-based model since the dataset is very complex",
   "B": "Parameterized model since there's too little data to fit in a high dimensional space. ",
   "C": "Instance-based model since the dataset is very simple",
   "D": "Parametrized model because there's too much data to fit in a low dimensional space",
   "answer": "B",
   "why": "Since instance based models use previously seen instances to make predictions it's difficult to fit little data in a high dimensional space. That is because the little data will be spread across all dimensions. The datapoints are therefore sparsely distributed and each point has little predictive power. Parametrized models on the other hand will fit a given model using the datapoints trying to optimize an objective function. Therefore even though there might be many datapoints, they still make a reasonable prediction. "
 },
 {
   "index": 343,
   "question": "If you are building a decision tree model, which is most likely to be true as the leaf size increases?",
   "A": "RMSE and In-Sample correlation both increase",
   "B": "RMSE and In-Sample correlation both decrease",
   "C": "RMSE increases and In-Sample correlation decreases",
   "D": "RMSE decreases and In-Sample correlation increases",
   "answer": "C",
   "why": "A greater leaf size will cause the model to not fit the training (in-sample) data perfectly, and will make it's correlation go down as the leaf size increases (in general) because individual cases in the tree are not handled as particularly--and are decided by an average or voting method across the size of the leaf.  This will also cause the RMSE to increase since these more one-off cases will not be handled as particularly, but instead face the average or vote of other elements going into a leaf."
 },
 {
   "index": 344,
   "question": "For which of the following applications would an instance-based model have the greatest benefits over a parameterized model?",
   "A": "Query speed & Extrapolation",
   "B": "Speed of training model & Ease of Adding new data",
   "C": "Low space requirements for model & Ease of Adding new data",
   "D": "Biased model & Speed of training model",
   "answer": "B",
   "why": "Instance based models such as KNN do not take very long to train a model and adding new data just requires the addition of new data points to the existing model. Querying speed is an advantage of parameterized models, so answer A is incorrect. Instance-based models have much larger space requirements than parameterized models so Answer C is incorrect and they are unbiased so answer D is not a good choice either."
 },
 {
   "index": 345,
   "question": "Which of the following statements is true about regression/classification?",
   "A": "a) In regression, the target variable is categorical and unordered.",
   "B": "b) In classification, the target variable is continuous or ordered numerical values. ",
   "C": "c) Suppose you are working on stock market prediction, and you would like to predict the price of a particular stock tomorrow (measured in dollars). This is a regression problem.",
   "D": "d) Regression involves identifying group membership.",
   "answer": "C",
   "why": "Correct answer is c) because regression gives a numerical predication which is the stock price in this case, whereas the classification is to classify an object."
 },
 {
   "index": 346,
   "question": "ID3 and its extension C4.5 are two different methods used to build a decision tree. Which of this statement is True only  for ID3 ?",
   "A": "Cannot handle continuous features but can easily handle  missing values",
   "B": "Only one attribute at a time is tested for making a decision",
   "C": "Data may most likely be over-fitted or over-classified, if a large sample is tested",
   "D": "Goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes",
   "answer": "B",
   "why": "Only one attribute at a time is tested for making a decision.  The algorithm used entropy and information gain to search the best attribute to split the data on. There are various splitting criteria based on impurity of a node. The main aim of splitting criteria is to reduce the impurity of a node."
 },
 {
   "index": 347,
   "question": "What does a high RMSE tell us about the quality of the model in terms of its predictions and correlation of data?",
   "A": "It indicates the model has a low prediction accuracy and a high correlation between the predicted values and the actual values.",
   "B": "It indicates the model has a high prediction accuracy and a high correlation between the predicted values and the actual values.",
   "C": "It indicates the model has a low prediction accuracy and a low correlation between predicted values and the actual values.",
   "D": "It indicates the model has high prediction accuracy and a low correlation between the predicted values and the actual values.",
   "answer": "C",
   "why": "Root Mean Square Error (RMSE) indicates the absolute fit of the model to the data. Higher values of RMSE indicate a worse fit (and thus a lower prediction accuracy). This is because the formula takes the squared difference between the predicted and the actual values and thus, a high RMSE indicates a large difference between predicted and actual. Correlation and RMSE have an inverse relationship so a high RMSE yields a low correlation."
 },
 {
   "index": 348,
   "question": "When building a decision tree, which method for determining the feature to split on is most likely to result in a shallower tree?",
   "A": "Selecting the feature with the highest correlation",
   "B": "Selecting a feature at random",
   "C": "Selecting the feature with the lowest correlation",
   "D": "The method used does not impact the tree depth",
   "answer": "A",
   "why": "Correct answer is A) since splitting on features with high correlation will tend to more quickly and cleanly separate training examples, ultimately requiring fewer splits to separate the records."
 },
 {
   "index": 349,
   "question": "Which of the following is not true about bagging?",
   "A": "Training and testing samples are created with replacement from the dataset.",
   "B": "Each bag must be trained using the same method.",
   "C": "Gives a better performance than just one model by offsetting the bias from any model.",
   "D": "The results of individual models can be combined by averaging(for regression) or voting(for classification).",
   "answer": "B",
   "why": "The purpose of bagging is to leverage the good performance of different models for different datasets. It does not mandate all the bags to use the same training model."
 },
 {
   "index": 350,
   "question": "What is the approximate percentage of unsampled data in a data set of size K if random sampling with replacement is used to generate a sample of size K during bagging. ",
   "A": "0.2",
   "B": "0.4",
   "C": "0.6",
   "D": "0.8",
   "answer": "B",
   "why": "The percentage of sampled unique data in from a sample of size K is approximated at 1 -1/e or 0.63. It has been mentioned several times at 0.6 in the class and piazza posts. The % of un-sampled data  would be 1 - 0.6 = 0.4"
 },
 {
   "index": 351,
   "question": "Consider a Bag Learner with the following parameters: learner=RTLearner, bag_size=1. Which of the following choices regarding overfitting is correct?",
   "A": "There is little or no overfitting since we are utilizing a Bag Learner.",
   "B": "Overfitting generally occurs when leaf_size increases away from 1; the exact point where overfitting begins can vary.",
   "C": "Overfitting generally occurs when leaf_size decreases towards 1; the exact point where overfitting begins can vary.",
   "D": "Overfitting is always continuously present for any leaf_size.",
   "answer": "C",
   "why": "The student should realize that a bag_size=1 is simply the RTLearner. Answer explanations are:  A) Incorrect. There will be overfitting for some datasets  B) Incorrect. For datasets with overfitting it will occur as leaf size decreases (not increases away from 1)  C) Correct. When a dataset is chosen that exhibits overfitting it will happen when we move towards a leaf size of 1.  D) Incorrect. Overfitting is never continuously present"
 },
 {
   "index": 352,
   "question": "Suppose you want to train a machine learning model to predict whether a stock price will increase or drop in coming week. Which machine learning model can't be applied here?",
   "A": "Logistic regression",
   "B": "Linear regression",
   "C": "Decision Tree",
   "D": "KNN",
   "answer": "B",
   "why": "this is a classification problem, all other methods except linear regression can be used for classification. Logistic regression is the option to confuse students who didn't understand the models."
 },
 {
   "index": 353,
   "question": "Which is NOT a correct way to reduce overfitting?",
   "A": "Bagging.",
   "B": "Use the testing data to train the model.",
   "C": "Cross validation.",
   "D": "Collect more data to train the model.",
   "answer": "B",
   "why": "Getting higher testing accuracy by training the model with testing data is useless because the model will overfit to the testing data."
 },
 {
   "index": 354,
   "question": "Assume a dataset with all discrete features. In Random Tree, an attribute is selected at random to split the data. In ID3 and C4.5, we calculate Entropy and Information Gain to determine the best attribute to split the data on. Let's suppose at each step, we instead choose an attribute with least Information Gain and also suppose we can reuse/repeat features for the nodes. What is the effect on the resulting tree when compared to a tree built using ID3 and C4.5 ? ",
   "A": "The tree will be unbalanced and will cause overfitting ",
   "B": "The size of tree is smaller (in terms of the number of split nodes and leaf_nodes) and will cause underfitting",
   "C": "The tree is larger in depth and will take more time during querying",
   "D": "The performance (prediction accuracy) is significantly lower",
   "answer": "C",
   "why": "IG enables ordering of nodes in a decision tree with most important nodes at the top. This enable to branches to be as short as possible. If attributes are selected with least IG, the tree will grow much larger in depth and size (provided the features can be repeated) "
 },
 {
   "index": 355,
   "question": "What is not true about bagging?",
   "A": "Bagging stands for bootstrap aggregating.",
   "B": "Bagging reduces variance.",
   "C": "Generating multiple training data from the original training data set without replacement.",
   "D": "Bagging helps to avoid overfitting.",
   "answer": "C",
   "why": "Bagging samples new multiple training data set from the original training data set uniformly with replacement."
 },
 {
   "index": 356,
   "question": "What of the following is more likely to happen if you end up with n out of M bags containing the exact same selection of elements? (2 <= n < M).",
   "A": "You will have different models yielding the same results which guarantees a better fit for test data.",
   "B": "Nothing, having repeated bags can never affect the performance of bagging.",
   "C": "You could obtain a better less overfitted result if you could recreate them until they are different.",
   "D": "This can never happen because bag selection is done through independent subsets of the training data",
   "answer": "C",
   "why": "Having equal bags will lead to having equal submodels, which in turn will bias the overall result producing a model \"overfitted\" towards the train data of those models. This invalidates A and B. C is wrong because bagging allows repetition (lecture  03-04). If you CAN recreate bags (no worries about time penalties mentioned) then diversity in your models will lead to a less overfitted ensemble model."
 },
 {
   "index": 357,
   "question": "We are ______ likely to overfit as K increase in KNN and ______ likely to overfit as D increases in a parametric model. ",
   "A": "more, more",
   "B": "more, less",
   "C": "less, more",
   "D": "less, less",
   "answer": "C",
   "why": "In KNN you take the average of the K nearest neighbor, so a higher K leads to an average that is less biased towards a single point. With parametric models, as we increase the polynomials we are allowing the equation to become quite specific to the data itself. "
 },
 {
   "index": 358,
   "question": "Which among the following is NOT a metric to determine the best feature to split on? ",
   "A": "Gini Index ",
   "B": "Variance Reduction ",
   "C": "Entropy ",
   "D": "Gaussian Naive Bayes Estimation",
   "answer": "D",
   "why": "Gaussian Naive Bayes is used as a parameter estimation factor for Naive Bayes classifiers. It is used as a technique to handle continues values during parameter estimation for Naive Bayes Classifiers."
 },
 {
   "index": 359,
   "question": "Given m training points and n testing points, where n is much greater than m, rank the total query costs from least to greatest of kNN, balanced decision tree (\"DT\"), and linear regression (\"LR\") models.",
   "A": "LR < DT < kNN",
   "B": "DT < LR < kNN",
   "C": "kNN < LR < DT",
   "D": "DT < kNN < LR",
   "answer": "A",
   "why": "The fact that there are many more testing points than training points has no bearing on the answer. kNN queries in O(m), DT in O(log(m)), LR in O(1). Total query time would then be O(n*m), O(n*log(m)), and O(n), respectively. This gives us the ranking of LR < DT < kNN."
 },
 {
   "index": 360,
   "question": "Which of the following statement about the comparison between supervised learning and unsupervised learning is correct? ",
   "A": "Supervised learning has the problem of overfitting when training algorithm trying to match the value (y) in the training data, unsupervised learning doesn't have overfitting problem because there is no target value (y) in training set",
   "B": "For time complexity, generally supervised learning is better than unsupervised learning",
   "C": "For space complexity, generally supervised learning is better than unsupervised learning",
   "D": "Supervised learning tend to have bias since usually we will predefine the function model, unsupervised learning doesn't have bias",
   "answer": "C",
   "why": "A: Wrong. Unsupervised learning also have problem of overfitting, for example, KNN algorithm we make k=1  B: Wrong. The query time for supervised learning is faster than unsupervised learning, but the training time usually is longer since it needs to generate the predict functions.  C: Correct. Generally, supervised learning is better than unsupervised learning, since supervised learning only need to store the model (function), unsupervised learning need to store all the training data  D. Wrong. Unsupervised learning also has bias, for example KNN, we assume that point near each other should belong to the same group, this is also a kind of bias"
 },
 {
   "index": 361,
   "question": "The root-mean-square error is a measure of ?  ",
   "A": "sample size",
   "B": "moving average periods",
   "C": "exponential smoothing",
   "D": "forecast accuracy",
   "answer": "D",
   "why": "The answer is D) . It is a measure of forecast accuracy of the predictive model."
 },
 {
   "index": 362,
   "question": "Which of the following statements about properties of k-NN versus, decision trees, and linear regression is true?",
   "A": "Decision tree uses a black box model and can generate over-fitting problems in training set.",
   "B": "The two-class k-NN algorithm is guaranteed to yield an error rate no worse than the Bayes error rate.",
   "C": "Linear regression will over-fit your data when you have highly correlated input variables.",
   "D": "Decision tree doesn’t perform well if its assumptions are somewhat violated by the true model from which the data were generated.",
   "answer": "C",
   "why": "Linear regression doesn't eliminate the correlation between your input variables. Some times introducing to many variables may over-fit your training set. And that will cause the fault of the model."
 },
 {
   "index": 363,
   "question": "Please select the correct statement ",
   "A": "Linear Regression requires more space than KNN for saving model ",
   "B": "KNN requires less compute time than Linear Regression to query the model",
   "C": "In KNN, as K decreases, model more likely to overfit the data ",
   "D": "In Linear Regression, higher order polynomials model less likely to overfit the data ",
   "answer": "C",
   "why": "Correct Answer is C because in KNN as K decreases, model more likely to overfit data. Think about K=1 which will overfit the data (more specific model) whereas K=N (where N > 1) will be more general model. A, C and D are incorrect statement. "
 },
 {
   "index": 364,
   "question": "A kNN model is using k=1.  How would we expect its RMSE score to be for In-Sample and Out-Sample?",
   "A": "High for In-Sample and high for Out-Sample querying.",
   "B": "High for In-Sample and low for Out-Sample querying.",
   "C": "Low for In-Sample and high for Out-Sample querying.",
   "D": "Low for In-Sample and low for Out-Sample querying.",
   "answer": "C",
   "why": "The answer is C because kNN using k=1 which indicates that the model is striving really hard to model the dataset exactly.  Therefore, it is overfit.  Because it is overfit, we expect RMSE to be low for In-Sample and high for Out-Sample querying or testing.  RMSE is an error metric.  The higher the RMSE, the higher the error."
 },
 {
   "index": 365,
   "question": "When compared to trees produced with random features and random splits (PERT), trees that are produced with the GINI criterion (CART), in general:",
   "A": "Have a fewer number of nodes ",
   "B": "Have a greater number of nodes",
   "C": "Have approximately the same number of nodes",
   "D": "There is no way to know which algorithm, in general, produces trees with more nodes",
   "answer": "A",
   "why": "From the chart on page 3 of Cutler's paper on PERT and Culter's remarks in the paper, the CART trees had significantly fewer nodes for perfectly-fitting trees, even though PERT was trained much more quickly.  "
 },
 {
   "index": 366,
   "question": "Which one is a SUPERVISED event?",
   "A": "Hilary Clinton will be the next president because Prof. Tucker Balch did a survey during the class among his smart students, and result showed that most of his students believed Hilary won the first debate.",
   "B": "You drive your car on a local road without speed limit sign at speed 35 mph. You think it is okay because two of your friends received tickets with speed 40 mph and you have never received ticket on this road with speed 35 mph.",
   "C": "The time of finishing MC3-P1will not be long because your friend Mr.Genius and Mr.Smart only spent less than 10 minutes.",
   "D": "The probability of meeting this problem in midterm is 0.0001 based on the information from Prof. Tucker's website.",
   "answer": "B",
   "why": "The answer B provides feedbacks of former similar events that can be used for predicting and assessing current event while other choices don't mention the feedback of a similar event in the history."
 },
 {
   "index": 367,
   "question": "Unlike linear regression and decission tree learners, KNN learners are:",
   "A": "slow to train, but fast to query",
   "B": "slow to query, but more accurate",
   "C": "fast to train, but slow to query",
   "D": "unsuited for classification problems",
   "answer": "C",
   "why": "The KNN algorithm does all of its computation at query time.  This makes it extremely fast to train since it only has to save the training set.  Querying is slow because the distance function must be calculated for every point each time a query is made."
 },
 {
   "index": 368,
   "question": "We want to select a classifier solving (x, y) coordinates to the corresponding quadrant. Which of the following algorithm would perform the worst in terms of accuracy?",
   "A": "k-Nearest Neighbor",
   "B": "Decision Trees",
   "C": "Linear Regression",
   "D": "They all perform equally well",
   "answer": "C",
   "why": "Linear regression works well only when there exists parallel decision boundaries in the feature space, while decision trees can have decision boundaries orthogonal to the feature axes. k-Nearest Neighbor would also work well using Euclidean distance as the similarity function."
 },
 {
   "index": 369,
   "question": "In regards of K-NN model, Linear Regression and Decision Trees. which of the following is true?",
   "A": "One of advantages of KNN model is that it does not have to normalize data and can easily handle missing data",
   "B": "Decision Trees requires the lowest cost of query.",
   "C": "KNN requires the lowest cost of learning",
   "D": "Linear Regression requires the lowest cost of learning",
   "answer": "C",
   "why": "It does not need training/learning for KNN model, while it needs training for Linear Regression and Decision Trees."
 },
 {
   "index": 370,
   "question": "Which of the following models is more likely to overfit?",
   "A": "A kNN algorithm where k is 10",
   "B": "A kNN algorithm where k is 2",
   "C": "A bagging with kNN where k is 2 and bag size is 20",
   "D": "A bagging with kNN where k is 10 and bag size is 20",
   "answer": "B",
   "why": "When k decreases; kNN is going to overfit. Bagging can reduce overfitting; therefore the correct answer is not C and D. When we compare k=2 with k=10, k=2 is more likely to overfit. Hence, The correct answer is B. "
 },
 {
   "index": 371,
   "question": "Which of the following is a classification problem?  (i) Predicting housing price at a particular area based on size of the house  (ii) Determining whether or not a credit card transaction was a fraud  (iii) Recognizing communities within large groups of people in social networks",
   "A": "All three",
   "B": "(ii) and (iii)",
   "C": "only (i)",
   "D": "only (ii)",
   "answer": "D",
   "why": "(i) is a regression problem, since it involves the prediction of a continuous value (housing price)  (ii) is a classification problem, since the prediction is a class label (fraud or not)  (iii) is a form of unsupervised learning (clustering). The output is a set of unlabelled clusters, each representing a community of similar people."
 },
 {
   "index": 372,
   "question": "Is random tree learning supervised or unsupervised, and why?",
   "A": "Supervised, because the algorithm requires labeled training data.",
   "B": "Supervised, because the user can configure a minimum leaf size.",
   "C": "Unsupervised, because the algorithm computes the splitting values from the training data alone.",
   "D": "Unsupervised, because the algorithm is randomized.",
   "answer": "A",
   "why": "The correct answer is A because supervised is defined as inferring from labeled training data.    The alternative plausible answer is C because random trees do not really make use of the labels during tree construction."
 },
 {
   "index": 373,
   "question": "Select the statement below that best reflects when overfitting occurs in a parametrized polynomial model and a KNN model: ",
   "A": "Increasing the degrees of freedom will cause both the parametrized polynomial and KNN models to overfit.",
   "B": "Increasing K will cause both a parameterized polynomial and KNN model to overfit.",
   "C": "Fewer degrees of freedom will cause a parameterized polynomial model to overfit while increasing K will cause a KNN model to overfit.",
   "D": "Larger degrees of freedom will cause a parameterized polynomial model to overfit while decreasing K will cause a KNN model to overfit.",
   "answer": "D",
   "why": "More degrees of freedom tailors a parameterized polynomial to the specific data it is being trained too while a KNN model overfits the training data when fewer data points are averaged when calculating the nearest neighbor.  "
 },
 {
   "index": 374,
   "question": "Which one of the following statements is NOT true.",
   "A": "The more decisions there are made in a tree the less accurate any expected outcomes are likely to be.",
   "B": "The training phase of kNN is faster than for a decision tree.",
   "C": "Linear regression works good when relationships between variables are both linear and non-linear.",
   "D": "kNN has higher computational cost of querying than linear regression.",
   "answer": "C",
   "why": "The statement C is NOT correct because linear regression only looks at linear relationships between variables. It assumes there is a straight-line relationship between them. That can be incorrect for some data sets. For example relationship between income and age is curved not linear."
 },
 {
   "index": 375,
   "question": "1.A k-NN model will underfit the data for value of k nearing the total number of training samples.    2.The solutions for a linear regression problem obtained through normal equations and least squares iterative gradient descent approach will be identical.    For the above two statements, please determine whether they are true or false and select the correct combination.",
   "A": "1.True; 2. False",
   "B": "1.True; 2. True",
   "C": "1.False; 2. False",
   "D": "1.False; 2. True",
   "answer": "B",
   "why": "1 True. k-NN model will underfit the data. As k -> N, all samples have an influence on the class label. Eventually, when k is too large it will always predict majority class    2. True. Cost function is convex, so there is only one global minimum."
 },
 {
   "index": 376,
   "question": "Which of the following approach is considered to be unsupervised learning algorithm?  ",
   "A": "a) Decision Tree Learning  ",
   "B": "b) Linear Regression  ",
   "C": "c) K-means  ",
   "D": "d) K-nearest neighbors  ",
   "answer": "C",
   "why": "Correct answer is c) because K-means is “clustering” algorithm which uses unlabeled datasets. All other algorithms use labeled datasets and considered to be supervised learning algorithms."
 },
 {
   "index": 377,
   "question": "You’ve built a new machine learning algorithm for trading that can be tuned for accuracy by adjusting a parameter you call “A” for accuracy. When you use the algorithm to train predictive models using historic stock data you find that models created with a large “A” values are very accurate (error rates are low) whereas models created with a small “A” values are not very accurate (error rates are high). When you test these same models using real-time stock data, however, you find just the opposite. What do you think might cause this problem?",
   "A": "High speed trading requires high performance algorithms. When you connect to real-time stock data it is likely that the program starts dropping data and you can no longer trust it.",
   "B": "Increasing the “A” variable causes the model to overfit the training data which results in higher error rates when processing data the model has never seen before.",
   "C": "Algorithms always perform better during training, so this behavior is expected.",
   "D": "Historic data looks backward in time whereas real-time data goes forward in time so tuning the model has the opposite effect in each scenario.",
   "answer": "B",
   "why": "Overfitting is a common problem in machine learning. The algorithm builds a model that matches the training data very closely but that model does not generalize well when faced with data it has never seen before, so it performs poorly against the ‘test’ data. In this case the training data is the historic stock data and the test data is the real-time stock data. "
 },
 {
   "index": 378,
   "question": "What does a low RMSE on training data and high RMSE on test data imply?",
   "A": "The learner is overfitted to the training data-set",
   "B": "The learner in underfitted to the training data-set",
   "C": "No over or underfitting of the learner",
   "D": "The learner is overfitted to the test data-set",
   "answer": "A",
   "why": "A low error on training data, but high error on test data implies that the learner is overfitted to the training data causing poor results on test data."
 },
 {
   "index": 379,
   "question": "Which learning algorithm has the following characteristics:  query time = log n_samples   training time = n_samples*n_features*log_n_samples?",
   "A": "KNN",
   "B": "Linear Regression",
   "C": "Decision Tree",
   "D": "None of the above",
   "answer": "C",
   "why": "Decision tree splits the data according to log n_samples during query. Training time is linearly proportional to n_samples*n_features*log_n_samples."
 },
 {
   "index": 380,
   "question": "About boosting, which of the following is TRUE?",
   "A": "The famous machine learning model \"random forest\" uses boosting.",
   "B": "Compared to bagging, it is easier to parallelize the training of boosting models.",
   "C": "Boosting model typically consists of multiple weak learners and combines them as a strong one.",
   "D": "Boosting models can avoid overfitting in any cases.",
   "answer": "C",
   "why": "A: False. Random forest uses bagging, rather than boosting.    B: False. Boosting models are harder to be paralleled since the training period is sequential.    C: True. By definition of boosting, it combines a set of weak learners to be a single strong learner.    D: False. Boosting models can still overfit with many boosting steps."
 },
 {
   "index": 381,
   "question": "Which of the following is NOT true of bagging?",
   "A": "Bagging decreases variance by takes the average of multiple instances of the model trained using different subsets of the data.",
   "B": "After the advent of boosting to improve predictive ability, bagging has become redundant as an ensemble method.",
   "C": "When we use bagging, we do not need to be as concerned about individual models overfitting.",
   "D": "If each model instance trains on unique subsamples (without replacement) of the data, bagging will be much less effective.",
   "answer": "B",
   "why": "While boosting does increase predictive power, bagging is still useful for algorithms with high variance like decision trees. Each method has their purpose."
 },
 {
   "index": 382,
   "question": "What distinguishes supervised from unsupervised learning?",
   "A": "Supervised learning requires input data and expected outcomes for the input data, while unsupervised learning is not passed expected outcomes.",
   "B": "Supervised learning uses unlabeled data for training while unsupervised learning uses labeled data.",
   "C": "Supervised learning uses k-nearest neighbor while unsupervised learning uses decision forests.",
   "D": "Supervised learning can achieve 0 RMSE with 1 for correlation while it is not possible to achieve the same results using unsupervised learning.",
   "answer": "A",
   "why": "Supervised learning accepts input data along with the expected outcome.  For example, MC3-Project 1 is an example of supervised learning since we are provided with wine quality data where the data provided are various factors used to rate the wine and the quality that was selected by an individual.  Unsupervised learning would attempt to rate the wine without knowing the quality assigned to it by a human."
 },
 {
   "index": 383,
   "question": "Which of the following statements are true as they relate to overfitting?",
   "A": "A hypothesis overfits the training examples if no other hypothesis performs better over the entire dataset compared to the training data.  ",
   "B": "A hypothesis overfits the training examples if another hypothesis performs better over the entire dataset while underperforming on the training data.",
   "C": "A hypothesis overfits the training examples if it performs better on the training dataset than it does over the entire dataset.",
   "D": "A hypothesis overfits the test examples if it performs better on the training dataset than other hypotheses while underperforming on the training data.",
   "answer": "B",
   "why": "When a hypothesis exists that works on the entire dataset while not matching the sample data as well, that shows that the original hypothesis is overfitted to the sample data. Ref pg 67 Machine Learning by Tom M. Mitchell"
 },
 {
   "index": 384,
   "question": "You've been asked to implement a rain prediction decision tree algorithm for a new line of sensors to be deployed across the globe at many diverse locations and weather conditions.  You have access to one year's worth of data collected from one of the sensors located in New Mexico, where the climate is rather dry throughout most of the year.  It includes the following features: average daily temperature, average daily pressure, and includes the total amount of rain observed for each day (in inches).  After training your model on a randomly selected set of 60% of the data and using the remaining 40% of the data for testing the trained model, you obtain the following results:    In sample error: 0.55  In sample correlation: 0.5  Out of sample error: 0.4  Out of sample correlation: 0.65    Based on the information collected and given the fact the sensor does not suffer from any noise artifacts (i.e., assume perfect measurements), does the model suffer from overfitting?",
   "A": "No.  In sample performance is slightly less than out of sample performance so we're not overfitting.",
   "B": "Yes.  Out of sample performance is higher so we've overfit the model.",
   "C": "Yes.  Our data collection is for only 1 region in the globe and we need to generalize across many diverse locations.",
   "D": "We can't conclude definitively without more details of the model and test data from other locations.",
   "answer": "D",
   "why": "This model may very well generalize across other locations but it may not.  It also may be overfitting to New Mexico weather and/or the model may be too complex.  Both our training and test data is insufficient to make this conclusion, given the goal/requirement to deploy this model across many diverse locations.  "
 },
 {
   "index": 385,
   "question": "Which of the following statement is TRUE?",
   "A": "KNN is sensitive to noise.",
   "B": "Decision tree is sensitive to noise.",
   "C": "Decision tree is a linear classifier.",
   "D": "KNN is a linear classifier.",
   "answer": "B",
   "why": "When noise presents in the dataset, decision tree tends to over fit the data which reduce the performance of the learner. Therefore the decision tree is sensitive to noise."
 },
 {
   "index": 386,
   "question": "RMSE stands for which of the following:",
   "A": "Root Mean Sum Error",
   "B": "Ratio Mean Square Error",
   "C": "Root Measure Sum Error",
   "D": "Root Mean Square Error",
   "answer": "D",
   "why": "The correct definition of RMSE is Root Mean Square Error"
 },
 {
   "index": 387,
   "question": "Which is true when comparing kNN to decision trees?",
   "A": "Decision trees and kNN have similar computational complexity.",
   "B": "knn can classify new examples faster than a decision tree",
   "C": "Decision trees can classify new examples faster than a kNN",
   "D": "It's not possible to compare kNN to decision trees.",
   "answer": "C",
   "why": "As the number of samples in a data set increases, the computational complexity of a kNN learner grows. "
 },
 {
   "index": 388,
   "question": "Consider the below decision tree algorithms. Which algorithm uses “information gain” as splitting criteria?",
   "A": "ID3",
   "B": "C4.5",
   "C": "CART",
   "D": "All of the above",
   "answer": "A",
   "why": "ID3 uses information gain as splitting criteria"
 },
 {
   "index": 389,
   "question": "What is the goal of Supervised Learning?",
   "A": "Using a set of training data to create a model that can be used to predict other data in the future.",
   "B": "Using a provided model to know whether new data fits the model well.",
   "C": "Using a set of training data to create clusters of data that you can fit future data to these clusters.",
   "D": "To randomly assign an answer to unknown datasets.",
   "answer": "A",
   "why": "A is the correct definition of Supervised Learning, B is a definition of Reinforcement Learning, C is a definition of Unsupervised Learning, and D is not really machine learning at all, just an arbitrary answer."
 },
 {
   "index": 390,
   "question": "Which one of the following statements is TRUE:",
   "A": "Training cost of KNN is greater training cost of linear regression, given the same data size.",
   "B": "Linear does not need to be trained, you just need to find the optimal slope. Hence linear regression has no training cost",
   "C": "One of the advantages of KNN is its constant training time. However, KNN is slower than linear regression when it comes to query time.",
   "D": "The training step for KNN requires going over all the data samples once, hence training KNN costs linear time.",
   "answer": "C",
   "why": "Query cost for linear regression is very low, since you just need to apply the formula. While the learning cost for KNN is very low, querying KNN is where the model needs to go through each of data samples and group them based on the number k neighbour selected. "
 },
 {
   "index": 391,
   "question": "Pruning decisions trees is one way to avoid overfitting. Which of the following describes a valid way to prune a decision tree?",
   "A": "Prior to building the decision tree, smooth the data to create a more continuous data set.",
   "B": "Prior to building the decision tree, remove data rows where the dependent variable is more than two standard deviations away from the mean.",
   "C": "Build a fully-grown decision tree and then combine leaves together to make new leaves.",
   "D": "Build a decision tree but stop splitting values based on a random value.",
   "answer": "C",
   "why": "C describes a correct process for pruning, wherein a fully grown tree is pruned bottom-up in order to avoid overfitting to training data. The other options describe other processes that may or may not help overfitting."
 },
 {
   "index": 392,
   "question": "What kind of learners do we use in Ada boosting?",
   "A": "Strong learners because you would have to use lesser number of them since they will give correct outputs faster.",
   "B": "Weak learners because they will be trained faster.",
   "C": "Weak learners because they will be robust to overfitting.",
   "D": "Strong learners because then the iterations required to reach the error threshold is lower.",
   "answer": "C",
   "why": "As the number of weak learners increase, the bias converges exponentially fast while the variance increases by geometrically diminishing magnitudes which means it overfits much slower than other methods"
 },
 {
   "index": 393,
   "question": "Which of the following learner has the fastest training speed given the same data set, under the condition that overfitting won't occur?",
   "A": "KNN with K=1.",
   "B": "Linear Regression.",
   "C": "Decision Tree with leaf_size=1.",
   "D": "Decision Tree with leaf_size=20.",
   "answer": "B",
   "why": "KNN should have the fastest training speed because it is instance-based learning, which only needs to store all the data during the training phase. But with K=1, it is almost certain to overfit. So does decision tree with leaf_size=1. Then A and C are not correct.  As for B and D, they'll generally not overfit, but decision tree needs to split the data recursively and select the median of each split, which requires more training time than linear regression. So B is correct.  "
 },
 {
   "index": 394,
   "question": "Which of the following is not a supervised learning alrogithm",
   "A": "Linear regression",
   "B": "Decision tree",
   "C": "k-means clustering",
   "D": "k-Nearest Neighbors",
   "answer": "C",
   "why": "Linear regression, decision tree and KNN are all supervised learning algorithms we talked about in the class, which leaves k-means the only answer. Actually, k-means is an unsupervised clustering algorithm. Even though we didn't talk about k-means in class, but it should not be problem since the other three are clearly supervised for anyone who is following along in the class."
 },
 {
   "index": 395,
   "question": "Comparing to the normal decision tree, what is the key feature of the random tree algorithm?",
   "A": "Random tree builds slower",
   "B": "Random tree uses deterministic feature to split the data",
   "C": "Random tree has a output which is easier to query",
   "D": "None of the above",
   "answer": "D",
   "why": "Random tree builds faster, and it randomly select feature to split on. There is no differences between the two algorithm's output"
 },
 {
   "index": 396,
   "question": "A machine learning algorithm fits a line to a scatter plot of data points. The line has a slope of -2 and a correlation coefficient of -0.9. Choose the answer that best describes the correlation of the data.  ",
   "A": "The data has no correlation because the slope of the line is negative.",
   "B": "The data has a strong inverse correlation because the slope of the line is near -1.",
   "C": "The data has a strong inverse correlation because the correlation coefficient of the line is near -1.",
   "D": "The data has a positive correlation because the negative slope and the negative correlation coefficient cancel each other out.",
   "answer": "C",
   "why": "The correlation coefficient, not the slope of the line, determines the correlation of the data points. Correlation coefficients run from -1 to 1, with 1 being the strongest positive correlation and -1 the strongest inverse correlation, so C is the correct answer."
 },
 {
   "index": 397,
   "question": "How does AdaBoost handle training data with high error when selecting bags?",
   "A": "Removes high error points from the training set before selecting bags.",
   "B": "Gives lower weights (less likely to be included) to high error points from the training set when selecting bags.",
   "C": "Gives higher weights (more likely to be included) to the high error points from the training set when selecting bags.",
   "D": "Adds all high error points to the next bag automatically.",
   "answer": "C",
   "why": "AdaBoost uses training data to test the model and determine error metrics for each point in the training data. When choosing the next bag AdaBoost weights each instance in the training data according to the error. Higher error values for an instance in the training data mean the instance is more likely to be selected for the next bag. "
 },
 {
   "index": 398,
   "question": "Under what condition would you choose to use KNN rather than Decision Tree?",
   "A": "When there is plenty of processing power, but limited space.",
   "B": "When the \"In sample\" data fairly represents the \"Out of sample\" data.",
   "C": "When trying to approximate discrete-valued target functions.",
   "D": "When compute time to train is the most important requirement.",
   "answer": "D",
   "why": "D is correct since KNN needs almost no training time. The only requirement is to store the data so it can be used later during query time. A is wrong since both need all the data stored. B is wrong because fair representative data benefits both methods. C is wrong since Decision Trees are better for approximating discrete-valued target functions."
 },
 {
   "index": 399,
   "question": "Which of the following tasks would be best accomplished using regression learning method?",
   "A": "Identifying the race group that a subject belongs to ",
   "B": "Predicting stock price of Apple",
   "C": "Ranking states based on the test scores of public school students",
   "D": "Both A and C",
   "answer": "B",
   "why": "Regression method enables us to predict the value of a continuous variable. In this question, stock price is most likely to be in the form of real number, therefore a continuous variable. The other tasks are mostly seeking discrete values. "
 },
 {
   "index": 400,
   "question": "Which of following statements about boosting is NOT correct?   ",
   "A": "AdaBoost is short for Adaptive Boosting. ",
   "B": "Boosting refers to a family of algorithms that are able to convert weak learners to strong learners  ",
   "C": "In boosting, weights are assigned to training examples. Weights of incorrectly predicted examples are increased after each round. ",
   "D": "AdaBoost is not susceptible to outliers. ",
   "answer": "D",
   "why": "A, B, and C are correct statements.   The statement D is false.  Adaboost is susceptible to outliers. AdaBoost tends to treat outliers as \"hard\" cases and put tremendous weights on outliers.  "
 },
 {
   "index": 401,
   "question": "Suppose you are building a machine learning algorithm with training data and then diligently applying test data to check accuracy.  At what point do you fear the algorithm may be suffering from overfitting?",
   "A": "Training data error is increasing and test data error is decreasing",
   "B": "Training data error is decreasing and test data error is decreasing",
   "C": "Training data error is increasing and test data error is increasing",
   "D": "Training data error is decreasing and test data error is increasing",
   "answer": "D",
   "why": "The algorithm can be built in a manner that is too precise, i.e. the degrees of freedom are too high.  When this occurs the algorithm fits the training data extremely well but any new data, such as the test set, are not accurately represented.  The solutions are given in manner that can easily make a test taker second guess what the expected action is as an algorithm is developed."
 },
 {
   "index": 402,
   "question": "Let N be the number of samples in our training set. Which of the following  statement is true about bagging?",
   "A": "Each bag must always contain N samples.",
   "B": "Each bag usually contains N' samples, where N' > N.",
   "C": "The expectation of  the number of distinct samples in each bag is usually less than 0.65*N.",
   "D": "Each bag must always contain 0.6*N samples.",
   "answer": "C",
   "why": "Each bag usually contains N' samples where N' <= N, therefore statements A, B, and D are incorrect. Random selection of N samples from the training set of size N with replacement will give on average  (1-1/e)*N = 0.632*N distinct samples; if N'<N, this number will be even less. Therefore expectation of the number of distinct samples is usually less than 0.65*N."
 },
 {
   "index": 403,
   "question": "When applying bagging to a dataset of size n, which most accurately describes the number of times an instance will be included in the bags:",
   "A": "Exactly once across all bags, since the original dataset is split evenly among the bags.",
   "B": "At most once per bag, since each bag samples without replacement from the dataset.",
   "C": "From 0 to n, the chance being equal across all bags, since each bag samples with replacement.",
   "D": "It will vary from bag to bag, according to how difficult it is for the learners to learn the particular instance.",
   "answer": "C",
   "why": "The correct answer is C. Bags sample with replacement from the dataset and sampling procedure doesn't vary between bags. A is false, since the dataset is not split evenly. B is false, since the bags sample with replacement, not without. D is false, since it refers to a different related technique: boosting."
 },
 {
   "index": 404,
   "question": "Suppose that I am interested in buying a house built in year x_1 with x_2 bathrooms, x_3 total square feet, and x_4 floors.    What kind of method should I use to determine the approximate price I should be paying for this house?",
   "A": "Classification",
   "B": "Regression",
   "C": "Clustering",
   "D": "None of the above",
   "answer": "B",
   "why": "House prices are a continuous variable, and it is obviously not a clustering problem, so it is regression."
 },
 {
   "index": 405,
   "question": "Which of the following regression tools are parametric?   - Linear Regression   - k Nearest Neighbour   - Decision Trees",
   "A": "k Nearest Neighbour",
   "B": "Linear Regression",
   "C": "k Nearest Neighbour & Linear Regression",
   "D": "k Nearest Neighbour & Decision Trees",
   "answer": "B",
   "why": "Linear Regression involves representing a model using a set of parameters, such that it can represented as a polynomial or a line that fits the data.     k Nearest Neighbour and Decisions Trees instead involve recording a set of training data in some data structure, that is then queried against to predict a value. "
 },
 {
   "index": 406,
   "question": "List the learning algorithms in terms of the time to train the model, from fastest to slowest",
   "A": "KNN, decision tree, parametric (linear regression)",
   "B": "KNN, parametric (linear regression), decision tree",
   "C": "Decision tree, KNN, parametric (linear regression)",
   "D": "Parametric (linear regression), KNN, decision tree",
   "answer": "B",
   "why": "- KNN has the fastest training time, as we just need to add the new data points to our model  - Linear regression takes longer than KNN, as it needs to compute a line or curve that best fits the data. However, this is still faster than decision tree, since for each split in the tree, we'll need to do computation for the best feature to split on as well as the split value, and then proceed to recursively build the rest of the tree from there for all of the remaining data."
 },
 {
   "index": 407,
   "question": "Which of the following parametric learning has the lowest training cost?",
   "A": "kNN, where k = 1",
   "B": "Linear regression ",
   "C": "Decision tree",
   "D": "kNN, where k = 10",
   "answer": "B",
   "why": "kNN undoubtedly has the lowest training cost, but linear regression is the only parametric learning among all four choices."
 },
 {
   "index": 408,
   "question": "Consider the two problems:   1) We want to estimate how many students attending Georgia tech  will be attracted to a particular beer as its bitterness(hoppiness)  increases.   2) We want to estimate the total Kinetic Energy of moving automobiles based on their total mass.   Which type of model would best solve each of the above problems?  {KE = .5*m*(v^2) }",
   "A": "Parametric model for both 1 and 2",
   "B": "Instance based model for both 1 and 2  ",
   "C": "Parametric model for problem 1, Instance based for problem 2  ",
   "D": "Parametric model for problem 2, Instance based for problem 1  ",
   "answer": "D",
   "why": "For problem 1,  we can start with a equation for Kinetic energy. We can then use parametric learner to find the correct parameters.    However, in case of students’ preference for beer taste in problem 2, we do not have an underlying formula to begin with."
 },
 {
   "index": 409,
   "question": "Comparing the Bagging with Random Tree algorithm, which one of the following statements is not correct?",
   "A": " Bagging stands for Bootstrap Aggregation, it's a better way to aggregate different combination of the training data and combine multiple training results to make the prediction model more accurate.",
   "B": "Bagging algorithm is a more accurate algorithm which can work independently and get better prediction result without any other algorithms support internally.",
   "C": "When the bag number increases, more training data will have opportunity to be put into the \"bag\" for training the prediction model.",
   "D": "Comparing with single Random Tree algorithm, the overfit will be less possible to happen for Bagging algorithm.",
   "answer": "B",
   "why": "A. Corrrect, \"train differnt subsets of the training data\" is the core of bagging algorithm.  B. Not Correct, Bagging is more like a framework to repeat other algorithms like Random Tree or Linear Regresson and eventually combine different training results together. Bagging must work together with these algorithms.  C. Correct. \"More bags\" means randomly get more subsets of the training data.  D. Correct. Because \"Bagging\" will make more \"sample data\" be collected to help train the prediction model."
 },
 {
   "index": 410,
   "question": "which of the following is not true about RMSE.",
   "A": "RMSE is sensitive to occasional large error",
   "B": "RMSE is a concave function which is widely used as optimization technique.",
   "C": "Finding mean square error ensures that errors get accumulate rather than canceling each other.",
   "D": "RMSE is not useful if we are interested in finding error rate of a classifier.",
   "answer": "B",
   "why": "RMSE is a convex function which is widely used as optimization technique."
 },
 {
   "index": 411,
   "question": "When fitting a K-NN model to data, when would we expect to see overfitting  and why?",
   "A": "When K is small because this will fit the training data well but may not fit the test data.",
   "B": "When the data is not linearly separable because then it will be hard to fit the test data.",
   "C": "When K is large because this will lead to large sections being labled the same and therefore overfitting.",
   "D": "When the data has low amounts of noise because then the test data set will be hard to fit.",
   "answer": "A",
   "why": "The correct answer is A which was explained in the lectures. This is because the  model fits the training data very closely and it may actually be fitting noise  which would lead to errors when classifying the test data set. Answer B is wrong  because K-NN is actually quite good at fitting data that is not linearly separable.  Answer C is wrong, a large K is actually what protects against overfitting.  Answer D is wrong because a low amount of noise actually makes it easy to create  a model without overfitting."
 },
 {
   "index": 412,
   "question": "Which of these correctly describes bagging?",
   "A": "Separating training data into bags based on their target value",
   "B": "Given N data points, picking N data points randomly for each bag, each time with replacement",
   "C": "Given N data points, picking m data points at random each time, where m=N/(number of bags)",
   "D": "Given N data points, dividing the data into B bags, such that each bag contains N/B distinct data points",
   "answer": "B",
   "why": "In bagging, if we have N data points for training, we pick one data point to put in each bag N times, i.e., we pick N data points for each bag with replacement. This usually ends up in each bag containing 60% unique data points. Then we can have B such bags and create a model for each bag, and let these models vote to give the final answer. "
 },
 {
   "index": 413,
   "question": "Which of the following learning problem is regression",
   "A": "Given the occurence of some key words in an email, predict wheter the email is spam or not spam",
   "B": "Given the location, number of rooms, built year, total area of a house, and lot size, predict which price range will this house better fit in: 100,000 to 200,000, 200,000 to 300,000, 300,000 to 400,000",
   "C": "Given the gross income and the ranks of director and actors, predict its rating on rottentomatoes.com, from 1%, 2%, ..., to 100%",
   "D": "Given the location, number of rooms, built year, total area of a house, and lot size, predict the price in term of dollars",
   "answer": "D",
   "why": "The price of the house is a continuous value, while the two types of the email are catagorical, the range of price is discrete value, and so is the rating of movie on rottentomatoes (100 discrete values, a lot but still discrete)"
 },
 {
   "index": 414,
   "question": "You cross scatter plot two vectors of data with different units and see a gaussian blob of points spread across the graph.  Which of the following is the most correct description of the data.",
   "A": "RMSE is large",
   "B": "Correlation coefficient is small",
   "C": "All of the above",
   "D": "None of the above",
   "answer": "B",
   "why": "RMSE cannot be calculated because the vectors have different units.  Correlation coefficient is a unit-less comparison that is normalized by the standard deviation of each vector.  Correlation coefficient is small because the change in one variable cannot explain the change in the other."
 },
 {
   "index": 415,
   "question": "Which of the following statements if TRUE in regards to overfitting of linear regression models?    Note: D = number of dimensions, N = number of test inputs",
   "A": "For linear regression models, overfitting is more likely to occur as D INCREASES",
   "B": "For linear regression models, overfitting is more likely to occur as D DECREASES",
   "C": "For linear regression models, overfitting does not occur",
   "D": "For linear regression models, overfitting occurs, but is equally as likely for any value of D",
   "answer": "A",
   "why": "Overfitting is more likely as D increases because the resulting polynomial will produce a line that more closely matches the test data, which most likely contains either some random error or some noise.  "
 },
 {
   "index": 416,
   "question": "Which of the following is wrong about parameterized models and instance-based models?  ",
   "A": "Parameterized models may have infinite number of parameters",
   "B": "Parameterized models make assumptions about noise",
   "C": "Instance-based models do not tell much about how the data is structured",
   "D": "Instance-based models are computationally expensive ",
   "answer": "A",
   "why": "Correct answer is A because parameterized models are a  family of distributions that can be described using a finite number of parameters. "
 },
 {
   "index": 417,
   "question": "Which of the following is the most accurate answer regarding a K Nearest Neighbor (KNN) learning algorithm?",
   "A": "KNN learners do not require much computer space for saving a model. ",
   "B": "KNN learners are not susceptible to overfitting. ",
   "C": "KNN learners are fast to query than Linear Regression learners. ",
   "D": "KNN learners are faster to train than Linear Regression learners. ",
   "answer": "D",
   "why": "KNN Learners are fast to train and slow to query.  Essentially all the data is stored as a model so it takes a lot of space.  When K is a smaller number like 1, overfitting is very likely."
 },
 {
   "index": 418,
   "question": "Which of the following is not an example of supervised learning?",
   "A": "Run a regression of housing price on housing size to estimate the price of a new house.",
   "B": "Given a set of DNA, identify groups of DNA with similar gene segments.",
   "C": "Predict whether a customer is likely to purchase certain goods according to a database of customer profiles and their history of shopping activities.",
   "D": "Decide whether the breast cancer is malignant or not based on tumor size.",
   "answer": "B",
   "why": "Supervised learning is the type of learning for which both sample input and output are provided to train the machine and get the desired output, while in unsupervised learning, no output data sets are provided and the goal is to find similarities in the training data and cluster data into different classes. Answers a, c and d are applications that use historical data to train the machine and make prediction on output. However, b does not have output data set; the goal of the machine is to find the similarities among genes and put them into clusters. Therefore, b is not an example of supervised learning."
 },
 {
   "index": 419,
   "question": "What is the main difference between the decision tree algorithm and the random tree algorithm?",
   "A": "Nothing, they are essentially the same algorithm.",
   "B": "The decision tree algorithm deterministically selects the best feature to split on.",
   "C": "The random tree algorithm uses the median of a feature as the split value.",
   "D": "The random tree algorithm uses recursion.",
   "answer": "B",
   "why": "The main difference between the two algorithms is that the decision tree algorithm deterministically selects the best feature to split on and the random tree algorithm randomly selects a feature to split on.  An additional difference is that the decision tree algorithm uses the median value of the feature as the split value and the random tree algorithm uses the mean of two randomly selected data points of the feature."
 },
 {
   "index": 420,
   "question": "When constructing the base learner for a PERT implementation what combination of steps are randomized?",
   "A": "Feature selection and number of bags.",
   "B": "Number of bags and number of leaf nodes.",
   "C": "Data points to split on and number of times to try and resolve a tie.",
   "D": "Feature selection and the split(data points).",
   "answer": "D",
   "why": "This question was derived from the Adele Cutler paper 'PERT - Perfect Random Tree Ensembles'.  Number of bags may be randomly selected, but that is not part of building the base learner.  Number of times to resolve a tie was stated as 10 in the paper."
 },
 {
   "index": 421,
   "question": "What value of K in a K nearest neighbor model will overfit the data?",
   "A": "2",
   "B": "1",
   "C": "3",
   "D": "N",
   "answer": "B",
   "why": "When K =1 it will fit every point in the training data and therefore cause overfitting to occur."
 },
 {
   "index": 422,
   "question": "Determine which one is NOT correct about the comparison between a random tree and a decision tree created from information (JR Quinlan).",
   "A": "The computational cost is lower to build a random decision tree.",
   "B": "Predicting results from a random tree are still interpretable even though the split value is randomly chosed to create the tree. ",
   "C": "Overfitting can occur to both trees when the leaf size is small.",
   "D": "We can not built a perfect random tree because feature and split value are randomly chosed to create the tree. ",
   "answer": "D",
   "why": "We always can build a perfect random tree, which fits the training data perfectly."
 },
 {
   "index": 423,
   "question": "Which learning method is the fastest query-wise?",
   "A": "Linear regression",
   "B": "kNN",
   "C": "Decision Trees",
   "D": "Neural Networks",
   "answer": "A",
   "why": "Linear regression is fastest because calculating the prediction is as simple as plugging in all of your independent variable values and computing the result (to get y, the dependent variable).    kNN can be quite slow because you have to calculate the distance from your query to all of the other data elements (potentially thousands), sort distances, take top k values, and calculate mean of those top k values.    Decision trees are fairly fast at query time but they are slower than linear regression.  In general, query time is O(\\log(n_{samples})). A tree's path is variable depending on the input, unlike linear regression that is one plug-and-play equation.    Neural networks are also slower. This black box learning method is computationally intensive, particularly if the network topology is complex.  "
 },
 {
   "index": 424,
   "question": "Given a set of training data which contains both features and labels, which of the following Machine Learning algorithms would be good choices for predicting the labels for a set of test data which contains only features?  (1) Decision Trees  (2) K Nearest Neighbors  (3) Linear Regression  (4) Random Forests",
   "A": "(1) and (3)",
   "B": "(1), (2), (3) and (4)",
   "C": "(1), (2) and (3)",
   "D": "(1), (2) and (4)",
   "answer": "B",
   "why": "Each of the four algorithms is suitable for solving a supervised learning problem. Each of these was discussed as a valid supervised learning approach in lectures and in the context of the MC3_P1 project."
 },
 {
   "index": 425,
   "question": "In bagging, we have a dataset that's divided into a training set (with n instances) and a testing set. Then, we make (m) bags and in each bag, we randomly select (n') instances from the training set with replacement (some instances may be repeated). Which of the following sentences is true about this method?",
   "A": "At the end of learning, we will have (n') models.",
   "B": "The resulting ensemble of learners will most likely give a better prediction than a single model.",
   "C": "When querying, we take the output of a random model of the models we made.",
   "D": "The number of unique samples in each bag is always 60% of the training data (n).",
   "answer": "B",
   "why": "Choice A is wrong because we will have (m) models.  Choice C is wrong because we take the mean of the outputs of the models if it's regression or we take majority vote if it's classification.  Choice D is wrong because we can never be sure about the number of unique samples in the bags due to the randomness of the selection.    Choice B is correct because in practice we know that an ensemble of learners can give better predictive performance than a single model."
 },
 {
   "index": 426,
   "question": "When utilizing Bagging with Regression Trees, which of the following is usually true?",
   "A": "Overfitting is reduced by returning the median prediction from an ensemble of many trees.",
   "B": "Overfitting is reduced by returning the most common prediction from an ensemble of many trees.",
   "C": "Bagging has no impact on the overfitting of Regression Trees.",
   "D": "Overfitting is reduced by returning the mean of the predictions from an ensemble of trees.",
   "answer": "D",
   "why": "Since we are doing regression and not classification, we must take the mean of the predictions of the trees which will smooth the individual results and reduce overfitting."
 },
 {
   "index": 427,
   "question": "You work for a company that specializes in field equipment for coleopterists (scientists who study beetles). Your boss approaches you about a new product the company is developing. The order \"Coleoptera\" constitutes almost a quarter of all known animal species, more than any other order, and your company wants to produce a hand-held beetle identifier that will use measurements of various parts of a specific beetle (input by hand from a scientist in the field) and output a predicted species. He asks you to build the prediction model for this device and gives you a data set of 20,231 examples, each one containing 12 continuous numeric attributes representing measurements of various features on the scarab (wing length, horn length, abdomen diameter, leg segments, etc.).    Your company wants the device to work in such a way that that as new specimens of known species are encountered in the field, this data can be input by the scientists into the device. If the scientist has definitively identified a bug to be of a particular species, she will plug in the scarab’s 12 measurements in the device along with the classification. This data must then be available to the scientist right away for making new predictions.    Given this requirement, which is the most appropriate choice of algorithm for this project?",
   "A": "Decision Tree without bagging",
   "B": "Decision Tree with bagging",
   "C": "Linear regression",
   "D": "k-Nearest Neighbors",
   "answer": "D",
   "why": "d is the correct answer because with k-NN you can incorporate new data into your model \"on the fly\" without having to rebuild the model. You simply add the new instances with their classifications and they are instantly available for new classifications. "
 },
 {
   "index": 428,
   "question": "Which of the following best describes Bagging?",
   "A": "It is a method that utilizes the false classifications from previous iterations to improve accuracy",
   "B": "It is a method that repeatedly uses the exact same data for every subsequent iteration",
   "C": "It is a method that randomly creates subsets of training data and uses it for different iterations",
   "D": "It is a method that randomly creates subsets of the entire data set (training and testing) and uses it for different iterations",
   "answer": "C",
   "why": "A is boosting, B is incorrect because it randomly creates subsets of data D is incorrect because we do not include test data in the subset data generation"
 },
 {
   "index": 429,
   "question": "Suppose you have a regression model with 5 parameters, and a KNN model with k=5. What could you do to reduce the overfitting in both the parametric model as well as the non-parametric, instance-based model?",
   "A": "Decrease the number of parameters to something less than 5, and pick a new k less than 5.",
   "B": "Decrease the number of parameters to something less than 5, and pick a new k greater than 5.",
   "C": "Increase the number of parameters to something greater than 5, and pick a new k less than 5.",
   "D": "Increase the number of parameters to something greater than 5, and pick a new k greater than 5.",
   "answer": "B",
   "why": "Decreasing the number of parameters in a regression model reduces overfitting, while increasing the k value used in KNN reduces overfitting, so the answer is (B)."
 },
 {
   "index": 430,
   "question": "Select a learner with least cost of learning and a learner with least cost of query.",
   "A": "Decision Trees; KNN",
   "B": "Linear Regression; Decision Trees",
   "C": "KNN; Linear Regression",
   "D": "KNN; Decision Trees",
   "answer": "C",
   "why": "KNN has the least cost of learning, and linear regression has the least cost of query"
 },
 {
   "index": 431,
   "question": "Of the following examples, which would you address using an unsupervised learning algorithm:",
   "A": "Given email labeled as spam/not spam, learn a spam filter.",
   "B": "Given a set of news articles found on the web, group them into sets of articles about the same stories.",
   "C": "Given a database of rank of red wine, learn to rank new incomming red wine.",
   "D": "Given a dataset of patients diagnoised as either having diabetes or not, learn to classify new patients as having diabetes or not.",
   "answer": "B",
   "why": "With A,C and D choices, the input data has already been labeled which indicates supervised learning."
 },
 {
   "index": 432,
   "question": "Which of the following is false?",
   "A": "Knn is an instance based method that holds onto all the data and uses it when queried",
   "B": "Linear regression can only fit a first order polynomial (straight line) to the data",
   "C": "Regression trees have a leaf size parameter that controls the number of observations in leaf nodes  ",
   "D": "Knn’s training runtime complexity is 1 because it does not fit parameter to the data",
   "answer": "B",
   "why": "You can fit higher order polynomials to the data using linear regression by squaring the features and including them as squared features"
 },
 {
   "index": 433,
   "question": "There are two vectors X, Y in R^n. a, b in R. Which statement below is correct?  ",
   "A": "RMSE(X, Y) = (sum(sqrt(X – Y)) / n)^2",
   "B": "RMSE(X, Y) = RMSE(aX + b, Y)",
   "C": "corrcoef(X, Y) = (E[XY] – E[X]E[Y]) / ((E[X^2] – E[X]^2) (E[Y^2] – E[Y]^2))",
   "D": "corrcoef(X, Y) = corrcoef(aX + b, Y)",
   "answer": "D",
   "why": "(a) RMSE(X, Y) = sqrt(sum((X – Y)^2) / n)  (b) RMSE(X, Y) != RMSE(aX + b, Y)  (c) corrcoef(X, Y) = (E[XY] – E[X]E[Y]) / ((E[X2] – E[X]2)^0.5*(E[Y2] – E[Y]2)^0.5)  "
 },
 {
   "index": 434,
   "question": "In terms of the expressiveness of the model, which one is most restrictive: kNN, decision trees, or linear regression?",
   "A": "kNN. Because the prediction is made only based on the neighbors.",
   "B": "Decision trees. Because you cannot guarantee how deep the tree will be.",
   "C": "Linear regression. Because it is a parametric model.",
   "D": "None. They all have similar expressiveness.",
   "answer": "C",
   "why": "kNN and decision trees are non-parametric models, and they do not assume any characteristic of the given data. Linear regression, on the other hand, assumes that the features can linearly explain the outcome."
 },
 {
   "index": 435,
   "question": "Assume k is the number of nearest neighbors for a KNN learner, d is the degree of a polynomial model learner, and bags are the number of bags used for a bagging learner. Which machine learner is least likely to overfit?  ",
   "A": "A KNN learner (k=1)",
   "B": "A polynomial model learner (d=100)  ",
   "C": "A KNN learner with bagging (k=1, bags=100)  ",
   "D": "A polynomial model learner with boosted bagging (d=100, bags=100)",
   "answer": "C",
   "why": "Correct answer is c) because while a KNN learner with k=1 is likely to overfit, the large number of bags will reduce the likelihood of overfitting. Note that d) is incorrect since a polynomial model with a large degree is likely overfit and large numbers of bags with boosting can further contribute to overfitting.   "
 },
 {
   "index": 436,
   "question": "An instructor was curious about the association between students and class forum data.  The instructor wanted to know two things:  (1) Can you predict the grade of a student based on their post volume to the class forum?   (2) Are there any other inferences that can be made about students and class forum data?   Given data that contains past semester student information (grades, classes taken, age, etc.) and class forum data (post volume, ratings, length, instructor comments, etc.), which type of learning algorithm would be best suited for the above items?",
   "A": "(1) unsupervised, (2) supervised",
   "B": "(1) supervised, (2) unsupervised",
   "C": "(1) unsupervised, (2) unsupervised",
   "D": "(1) supervised, (2) supervised",
   "answer": "B",
   "why": "Item (1) can be addressed with labeled training data gathered from student grades and their class forum post volume.  A supervised learning algorithm is best suited for this scenario.  Item (2) would be best served using an unsupervised learning algorithm in order to cluster the data into potentially interesting groupings."
 },
 {
   "index": 437,
   "question": "As a decision tree learns more and more training data (starting from the first instance), does test error (out-of-sample error) go:",
   "A": "Down for a while then back up",
   "B": "Up for a while then back down",
   "C": "Down quickly at first, gradually converging to a small amount of error",
   "D": "Up quickly at first, gradually converging to a small amount of error",
   "answer": "A",
   "why": "Correct answer is a) because a decision tree with very few training instances will not have enough conditions to classify (or regress) new samples so test error will start high. As we add training samples, the tree becomes more generalized and test error goes down. Eventually though, as we add too many training samples, the tree starts to overfit the training data so test error starts to go back up because the tree has lost its ability to generalize.  "
 },
 {
   "index": 438,
   "question": "When looking to build a decision tree, is it better to pick the splitting feature randomly or through a calculated method such as feature correlation or Gini index?",
   "A": "It is always better to use a calculated method to determine the splitting factor because it will be a more accurate tree",
   "B": "Building the tree randomly is the best choice if it will be one tree in a Random Forrest. This is because it is much faster, and when combined with a group can be even better than a calculated tree",
   "C": "Building the tree randomly is always the better choice because it is much faster than the calculated method",
   "D": "Calculating the splitting feature is the better option if used in a group of calculated trees. The accuracy of multiple calculated trees makes up for the time it takes to create them",
   "answer": "B",
   "why": "B is the best choice for this question. Although answer A is correct in saying the tree will be more accurate, this comes at a speed cost as doing that calculation is expensive and an overly accurate tree can lead to overfitting. Answer C is correct in saying a random tree is faster, but if there is only one tree it may not be a good representation of the actual data. D is just not correct as a group of calculated trees is just extra slow and they will all be the same, therefore will not be any better as a group."
 },
 {
   "index": 439,
   "question": "The best way to reduce overfitting in bagging is to:",
   "A": "Increase the number of bags.",
   "B": "Change the type of learner being used for bagging.",
   "C": "Increase the size of the bags.",
   "D": "Decrease the number of bags.",
   "answer": "A",
   "why": "A increases the number of learners \"voting\" on the output, which makes overfitting less likely to occur.    B could help overfitting if the new learner is less prone to overfitting, but it could also make overfitting worse.    C and D can actually make overfitting worse."
 },
 {
   "index": 440,
   "question": "Which of the following is not a characteristic of AdaBoost?  ",
   "A": "adaBoost works best with weak learners  ",
   "B": "The sample data used for each model do not rely on the performance of the previous model created.   ",
   "C": "AdaBoost uses the in-sample data to create a prediction and calculate error   ",
   "D": "Each data point in the training set, has a weight associated with it which corresponds to the likelihood of being selected for use in model creation. ",
   "answer": "B",
   "why": "The sample data chosen for each model is not independent of one another. Each data point in the sample has a weight that is assigned based on the lasts model error  "
 },
 {
   "index": 441,
   "question": "Why would someone prefer a random decision tree over a normal (deterministic built) decision tree?",
   "A": "A random decision tree can be queried faster.",
   "B": "A random decision tree can be built faster.",
   "C": "A random decision tree is more accurate.",
   "D": "A random decision tree takes up less storage space.",
   "answer": "B",
   "why": "Random decision trees can be built faster because correlations and means don't have to be calculated for each split.  They however are generally a little longer (since splits aren't 100% optimal) which means they take longer to query and take more storage space.  A random decision tree and a normal decision tree have similar accuracy so it is not MORE accurate."
 },
 {
   "index": 442,
   "question": "Which of the following doesn't allow you to do supervised learning, but allows unsupervised learning?",
   "A": "The data are not labeled so you don't know the prediction (Y)",
   "B": "Classification problems",
   "C": "You cannot tell which features are most useful for training",
   "D": "The sample size is very small",
   "answer": "A",
   "why": "Supervised learning optimizes the model by minimizing the difference between prediction and ground truth, so it needs to know the output for input data. Unsupervised learning doesn't have such requirement for training."
 },
 {
   "index": 443,
   "question": "Which of the problems below is most appropriate for a classification algorithm (as opposed to regression)?",
   "A": "Scoring a test out of 100",
   "B": "Evaluating wine quality between 1 and 10",
   "C": "Identifying a tumor as malignant or benign",
   "D": "Predicting house prices",
   "answer": "C",
   "why": "Identifying a tumor as malignant or benign is a classic example of a classification problem. The other three answers are all continuous numerical values, for which classification is not commonly appropriate."
 },
 {
   "index": 444,
   "question": "If when recursively building a non-random regression tree, which methodology is most suitable for calculating the split value of a discrete feature?",
   "A": "Minimize entropy.",
   "B": "Maximize the Gini index.",
   "C": "Minimize variability.",
   "D": "Maximize the correlation coefficient.",
   "answer": "C",
   "why": "Since this is a regression tree, the target variable is continuous. Thus with a discrete variable, the most plausible choice is to split variance minimization."
 },
 {
   "index": 445,
   "question": "Which of the following algorithm(s) can be easily extensible to streaming data ( i.e online algorithms) ?",
   "A": "Neural Network",
   "B": "Parameterized model ( eg: Linear regression)",
   "C": "Instance-based model (eg: kNN)",
   "D": "All of the above can be easily extended",
   "answer": "C",
   "why": "Instance-based model are easy to extend in the online setting as it would just involve adding the incremental data to the model set. The other options are not easily extensible and are an active research topic."
 },
 {
   "index": 446,
   "question": "How is the following quote best related to the topic of overfitting?   Occam’s razor: Prefer the simplest hypothesis that fits the data",
   "A": "Models with small, simple datasets produce the best results",
   "B": "Models that generalize the data should be preferred",
   "C": "Models that are simple to program should be preferred",
   "D": "A specific hypothesis that explains all the data will work in other situations",
   "answer": "B",
   "why": "A simple hypothesis is analogous to a model which is generalized. In contrast, a model that seeks to fit all of the data may be much more complicated. The particular peril of a model that is over-complicated is that it may not adequately explain new data, because it is too specialized."
 },
 {
   "index": 447,
   "question": "Which of the following is TRUE about overfitting?",
   "A": "An overfitted model has a poor predictive performance on new, unseen data.",
   "B": "Overfitting cannot occur on nonparametric instance-based models.",
   "C": "An overfitted model is preferred because it has a low in-sample error.",
   "D": "A model may be overfitting when its in-sample error and out-of-sample error both start to decrease.",
   "answer": "A",
   "why": "Correct answer is (A) because an overfitted model tries so hard to fit to the training data perfectly (\"memorize\" training data rather than learning it) that the resulting model does not generalize at all."
 },
 {
   "index": 448,
   "question": "To avoid Overfitting in Decision trees:  (1)\tStop splitting when it is no longer statistically significant  (2)\tGrow the tree, then post-prune  ",
   "A": "Only (1) is correct",
   "B": "Only (2) is correct",
   "C": "Both (1) and (2) are correct ",
   "D": "Neither (1) nor (2) is correct",
   "answer": "C",
   "why": "Both of these are methods used to effectively not grow a tree or reduce the size of a tree that is already too large, i.e a tree which perfectly fits all the samples in the training data set and hence, is overfitting."
 },
 {
   "index": 449,
   "question": "Which of the following is most likely to result in overfitting?",
   "A": "Increasing the number of bags in an adaptive boost ensemble learner.",
   "B": "Increasing the value of k in a K-nearest neighbors model.",
   "C": "Increasing the number of bags in a bagging ensemble learner (without boosting).",
   "D": "Increasing the leaf size in a (single) random tree model.",
   "answer": "A",
   "why": "The correct answer is (a) because, for an adaptive boost ensemble learner, as the number of bags increases,   the learner tries to assign more and more specific data points to subsequent bags in an attempt to   describe all of the data points contained in the training dataset. In all other cases, overfitting is   reduced by implementing the actions suggested."
 },
 {
   "index": 450,
   "question": "Consider the following two problems:    1.  An advertisement agency would like to know which ad to present to a customer next based on previous ad-click history.      2.  A student would like to know the optimal number of hours to spend studying to maximize performance on exams.      What is the best approach (model) for solving each problem?",
   "A": "A.\tProblem 1 = Parameterized, Problem 2 = Parameterized ",
   "B": "B.\tProblem 1 = Parameterized, Problem 2 = Instance-Based ",
   "C": "C.\tProblem 1 = Instance-Based, Problem 2 = Parameterized ",
   "D": "D.\tProblem 1 = Instance-Based, Problem 2 = Instance-Based ",
   "answer": "C",
   "why": "Marketing data analysis such as Problem 1 is well-suited to an instance based model because customer preferences are not likely to be quantifiable.  They are more emotional. On the other hand, we expect studying versus grades to be a nice numerical model where we will see some optimum where additional hours of studying has a negligible effect on test performance.  So Problem 2 would make a great parametric model. "
 },
 {
   "index": 451,
   "question": "In terms of linear regression and kNN performance metrics, which one these would likely to be correct?",
   "A": "Total model space:      Linear regression : 15GB   kNN: 1GB    Compute time for training:     Linear regression : 10 seconds  kNN: 5 seconds    Compute time for querying:     Linear regression : 10 seconds  kNN: 5 seconds",
   "B": "Total model space:      Linear regression : 10GB   kNN: 10GB    Compute time for training:     Linear regression : 3 seconds  kNN: 50 seconds    Compute time for querying:     Linear regression : 60 seconds  kNN: 10 seconds",
   "C": "Total model space:      Linear regression : 32 byte  kNN: 1GB    Compute time for training:     Linear regression : 60 seconds  kNN: 0 second    Compute time for querying:     Linear regression : 1 second   kNN: 5 minutes",
   "D": "Total model space:      Linear regression : 1GB  kNN: 1GB    Compute time for training:     Linear regression : 1 second  kNN: 1 second    Compute time for querying:     Linear regression : 30 seconds  kNN: 25 second",
   "answer": "C",
   "why": "The answer is C because in terms of total model space in linear regression, if we are learning a 3rd order polynomial, we only need to save 4 numbers. Since float in python is 64bit, 4 number will be in total 64 byte. On the other hand, with kNN, we need to store all the data.     In terms of compute time for training, kNN needs no time for training itself so 0 second is a logical number.  On the other hand, in linear regression, it has to calculate all the data to find the parameters.    In terms of compute time for querying, we are only calculating polynominal function by giving the X values. However for kNN, we need to sort across all the data and this might take some time to evaluate.       So the order should be in terms of better performance metrics, total model space is linear regression, compute time for training is kNN and compute time for querying is linear regression. A,B and D is not matching with this order."
 },
 {
   "index": 452,
   "question": "If I have an overfit model, I will have high variability out-of-sample (since we have likely fit on noise). If I use a bagging procedure instead, why might I expect lower variability and lower likelihood of overfitting?",
   "A": "The bagging procedure does not protect against overfitting.",
   "B": "Because the bagging procedure averages predictions over multiple learners, the average of these predictions are more stable out-of-sample.",
   "C": "Because bagging will identify areas of the dataset where predictions are doing worse, and calibrate itself toward predicting those observations better.",
   "D": "Because like KNN, smoothing is increased as the number of observations within a group decreases.",
   "answer": "B",
   "why": "Correct answer is b) because model averaging means that prediction error an any particular single learner used in the bagging process is averaged out by the predictions of the other learners in the ensemble.     (a is untrue, d is non-sensical but plausible (also KNN will likely increase in error variability as the number of observations in each group falls), and c refers to boosting—not bagging. "
 },
 {
   "index": 453,
   "question": "Based on the four classifiers below, which one is most likely to overfit:  Classifier #\tRMSE  (Train Data)\tRMSE (Test Data)  1\t0.2\t0.3  2\t0.6\t0.5  3\t0.3\t0.3  4\t0.3\t0.7  ",
   "A": "Classifier 1",
   "B": "Classifier 2",
   "C": "Classifier 3",
   "D": "Classifier 4",
   "answer": "D",
   "why": "RMSE out sample > RMSE in sample. Higher difference in classifier 4 compared to classifier 2. "
 },
 {
   "index": 454,
   "question": "For building a decision tree, why might you choose the split feature of a node randomly instead of using entropy or Gini index? ",
   "A": "Because random selection works for both classification and regression problems, but entropy and Gini index are only suitable for classification problems",
   "B": "Because entropy and Gini index are much faster than random selection",
   "C": "Because entropy and Gini index rely on storing additional data",
   "D": "Because random selection is much faster than entropy and Gini index",
   "answer": "D",
   "why": "Random selection is chosen because it is significantly faster to choose a feature randomly than to determine the information gain of each attribute using entropy or Gini index. "
 },
 {
   "index": 455,
   "question": "Which one of the following models is NOT suitable to create AdaBoost? ",
   "A": "Linear Classifier",
   "B": "Decision stump",
   "C": "Logistic classifier",
   "D": "Support Vector Machine",
   "answer": "D",
   "why": "AdaBoost should use weak learners. SVM is a very complex model which will cause overfitting of training data. "
 },
 {
   "index": 456,
   "question": "As a result of overfitting, how does training errors and validation errors varies ?",
   "A": "Both the training errors and validation errors decreases continuously",
   "B": "Training errors decreases continuously whereas validation error first decreases then increases",
   "C": "Validation errors decreases continuously whereas training error first decreases then increases",
   "D": "Both the training errors and validation errors first decreases then increases",
   "answer": "B",
   "why": "Training errors decreases continuously whereas validation error first decreases then increases. As we overfit the data, training errors decreases as we have trained the data set at the lowest granularity. However, at the same time, while running on the test data, the errors will increase when we overfit as the data set doesn't actually emulates the fit at that granularity."
 },
 {
   "index": 457,
   "question": "Which of the following algorithm is the bagging procedure we learn from class?",
   "A": "def algorithmA(dataX, dataY, bagNumber, oneLearner):        generate a random number array representing how many samples should be in each bag        extracting sample from dataX into bags according to the random number        train those bags using oneLearner        return the mean of results",
   "B": "def algorithmB (dataX, dataY, bagNumber, sampleSize, learners)        extracting the same number of sample from dataX into bags        train those bags with different learner        return the mean of results",
   "C": "def algorithmC(dataX, dataY, bagNumber, sampleSize, oneLearner):        set initial weight to be equal        for each bag_i              extracting sampleSize number of sample from dataX into bag_i according to weight              train this bag using oneLearner              do a in-sample prediction and calculate the error              putting higher weight on points that has higher error        return the last prediction ",
   "D": "def algorithmD(dataX, dataY, bagNumber, sampleSize, Learners):        extracting the same number of sample from dataX into bags        train those bags with different learner        conduct in-sample prediction and calculating totak sum of errors        weighting learners that has smaller error with higher weight        return the weighting average of results",
   "answer": "B",
   "why": "A is wrong because we usually use the same sample size in each bag  C is the adaBoost algorithm  D is the stacking algorithm"
 },
 {
   "index": 458,
   "question": "Suppose we have a machine learning problem where X is the feature space and Y is the output space. To be considered a classification problem, which of the following must be true:",
   "A": "a)  X must be continuous",
   "B": "b)  Y must be discrete",
   "C": "c)  Y must be continuous",
   "D": "d)  X and Y must both be discrete",
   "answer": "B",
   "why": "Answer: b)  A classification problem has a finite output space.  "
 }
]
